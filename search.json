[{"title":"Spring Boot 无侵入式 实现 API 接口统一 JSON 格式返回","path":"/2023/12/25/Spring-Boot-无侵入式-实现-API-接口统一-JSON-格式返回/","content":"无侵入式 统一返回JSON格式其实本没有没打算写这篇博客的，但还是要写一下写这篇博客的起因是因为，现在呆着的这家公司居然没有统一的API返回格式?，询问主管他居然告诉我用HTTP状态码就够用了（fxxk），天哪HTTP状态码真的够用吗？ 在仔细的阅读了项目源码后发现，在API请求的是居然没有业务异常（黑人问好）。好吧 居然入坑了只能遵照项目风格了，懒得吐槽了。 因为项目已经开发了半年多了, 要是全部接口都做修改工作量还是挺大的, 只能用这种无侵入式的方案来解决. 项目源代码: https://github.com/469753862/galaxy-blogs/tree/master/code/responseResult 定义JSON格式定义返回JSON格式后端返回给前端一般情况下使用JSON格式, 定义如下 1234567&#123; &quot;code&quot;: 200, &quot;message&quot;: &quot;OK&quot;, &quot;data&quot;: &#123; &#125;&#125; code: 返回状态码 message: 返回信息的描述 data: 返回值 定义JavaBean字段定义状态码枚举类123456789101112131415161718192021@ToString@Getterpublic enum ResultStatus &#123; SUCCESS(HttpStatus.OK, 200, &quot;OK&quot;), BAD_REQUEST(HttpStatus.BAD_REQUEST, 400, &quot;Bad Request&quot;), INTERNAL_SERVER_ERROR(HttpStatus.INTERNAL_SERVER_ERROR, 500, &quot;Internal Server Error&quot;),; /** 返回的HTTP状态码, 符合http请求 */ private HttpStatus httpStatus; /** 业务异常码 */ private Integer code; /** 业务异常信息描述 */ private String message; ResultStatus(HttpStatus httpStatus, Integer code, String message) &#123; this.httpStatus = httpStatus; this.code = code; this.message = message; &#125;&#125; 状态码和信息以及http状态码就能一一对应了便于维护, 有同学有疑问了为什么要用到http状态码呀,因为我要兼容项目以前的代码, 没有其他原因, 当然其他同学不喜欢http状态码的可以吧源码中HttpStatus给删除了 定义返回体类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Getter@ToStringpublic class Result&lt;T&gt; &#123; /** 业务错误码 */ private Integer code; /** 信息描述 */ private String message; /** 返回参数 */ private T data; private Result(ResultStatus resultStatus, T data) &#123; this.code = resultStatus.getCode(); this.message = resultStatus.getMessage(); this.data = data; &#125; /** 业务成功返回业务代码和描述信息 */ public static Result&lt;Void&gt; success() &#123; return new Result&lt;Void&gt;(ResultStatus.SUCCESS, null); &#125; /** 业务成功返回业务代码,描述和返回的参数 */ public static &lt;T&gt; Result&lt;T&gt; success(T data) &#123; return new Result&lt;T&gt;(ResultStatus.SUCCESS, data); &#125; /** 业务成功返回业务代码,描述和返回的参数 */ public static &lt;T&gt; Result&lt;T&gt; success(ResultStatus resultStatus, T data) &#123; if (resultStatus == null) &#123; return success(data); &#125; return new Result&lt;T&gt;(resultStatus, data); &#125; /** 业务异常返回业务代码和描述信息 */ public static &lt;T&gt; Result&lt;T&gt; failure() &#123; return new Result&lt;T&gt;(ResultStatus.INTERNAL_SERVER_ERROR, null); &#125; /** 业务异常返回业务代码,描述和返回的参数 */ public static &lt;T&gt; Result&lt;T&gt; failure(ResultStatus resultStatus) &#123; return failure(resultStatus, null); &#125; /** 业务异常返回业务代码,描述和返回的参数 */ public static &lt;T&gt; Result&lt;T&gt; failure(ResultStatus resultStatus, T data) &#123; if (resultStatus == null) &#123; return new Result&lt;T&gt;(ResultStatus.INTERNAL_SERVER_ERROR, null); &#125; return new Result&lt;T&gt;(resultStatus, data); &#125;&#125; 因为使用构造方法进行创建对象太麻烦了, 我们使用静态方法来创建对象这样简单明了 Result实体返回测试1234567891011121314151617181920212223@RestController@RequestMapping(&quot;/hello&quot;)public class HelloController &#123; private static final HashMap&lt;String, Object&gt; INFO; static &#123; INFO = new HashMap&lt;&gt;(); INFO.put(&quot;name&quot;, &quot;galaxy&quot;); INFO.put(&quot;age&quot;, &quot;70&quot;); &#125; @GetMapping(&quot;/hello&quot;) public Map&lt;String, Object&gt; hello() &#123; return INFO; &#125; @GetMapping(&quot;/result&quot;) @ResponseBody public Result&lt;Map&lt;String, Object&gt;&gt; helloResult() &#123; return Result.success(INFO); &#125;&#125; 到这里我们已经简单的实现了统一JSON格式了, 但是我们也发现了一个问题了,想要返回统一的JSON格式需要返回Result&lt;Object&gt;才可以, 我明明返回Object可以了, 为什么要重复劳动, 有没有解决方法, 当然是有的啦, 下面我们开始优化我们的代码吧 统一返回JSON格式进阶-全局处理(@RestControllerAdvice)我师傅经常告诉我的一句话: “你就是一个小屁孩, 你遇到的问题都已经不知道有多少人遇到过了, 你会想到的问题, 已经有前辈想到过了. 你准备解决的问题, 已经有人把坑填了”。是不是很鸡汤, 是不是很励志, 让我对前辈们充满着崇拜, 事实上他对我说的是: “自己去百度”, 这五个大字, 其实这五个大字已经说明上明的B话了, 通过不断的百度和Google发现了很多的解决方案. 我们都知道使用@ResponseBody注解会把返回Object序列化成JSON字符串,就先从这个入手吧, 大致就是在序列化前把Object赋值给Result&lt;Object&gt;就可以了, 大家可以观摩org.springframework.web.servlet.mvc.method.annotation.ResponseBodyAdvice和org.springframework.web.bind.annotation.ResponseBody @ResponseBody继承类我们已经决定从@ResponseBody注解入手了就创建一个注解类继承@ResponseBody, 很干净什么都没有哈哈,@ResponseResultBody 可以标记在类和方法上这样我们就可以跟自由的进行使用了 1234567@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Documented@ResponseBodypublic @interface ResponseResultBody &#123;&#125; ResponseBodyAdvice继承类12345678910111213141516171819202122232425@RestControllerAdvicepublic class ResponseResultBodyAdvice implements ResponseBodyAdvice&lt;Object&gt; &#123; private static final Class&lt;? extends Annotation&gt; ANNOTATION_TYPE = ResponseResultBody.class; /** * 判断类或者方法是否使用了 @ResponseResultBody */ @Override public boolean supports(MethodParameter returnType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123; return AnnotatedElementUtils.hasAnnotation(returnType.getContainingClass(), ANNOTATION_TYPE) || returnType.hasMethodAnnotation(ANNOTATION_TYPE); &#125; /** * 当类或者方法使用了 @ResponseResultBody 就会调用这个方法 */ @Override public Object beforeBodyWrite(Object body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) &#123; // 防止重复包裹的问题出现 if (body instanceof Result) &#123; return body; &#125; return Result.success(body); &#125;&#125; RestControllerAdvice返回测试12345678910111213141516171819202122232425262728293031323334@RestController@RequestMapping(&quot;/helloResult&quot;)@ResponseResultBodypublic class HelloResultController &#123; private static final HashMap&lt;String, Object&gt; INFO; static &#123; INFO = new HashMap&lt;String, Object&gt;(); INFO.put(&quot;name&quot;, &quot;galaxy&quot;); INFO.put(&quot;age&quot;, &quot;70&quot;); &#125; @GetMapping(&quot;hello&quot;) public HashMap&lt;String, Object&gt; hello() &#123; return INFO; &#125; /** 测试重复包裹 */ @GetMapping(&quot;result&quot;) public Result&lt;Map&lt;String, Object&gt;&gt; helloResult() &#123; return Result.success(INFO); &#125; @GetMapping(&quot;helloError&quot;) public HashMap&lt;String, Object&gt; helloError() throws Exception &#123; throw new Exception(&quot;helloError&quot;); &#125; @GetMapping(&quot;helloMyError&quot;) public HashMap&lt;String, Object&gt; helloMyError() throws Exception &#123; throw new ResultException(); &#125;&#125; 是不是很神奇, 直接返回Object就可以统一JSON格式了, 就不用每个返回都返回Result&lt;T&gt;对象了,直接让SpringMVC帮助我们进行统一的管理, 简直完美 只想看接口哦, helloError和helloMyError是会直接抛出异常的接口,我好像没有对异常返回进行统一的处理哦 统一返回JSON格式进阶-异常处理(@ExceptionHandler))卧槽, 异常处理, 差点把这茬给忘了, 这个异常处理就有很多方法了,先看看我师傅的处理方式, 我刚拿到这个代码的时候很想吐槽, 对异常类的处理这么残暴的吗, 直接用PrintWriter直接输出结果, 果然是老师傅, 我要是有100个异常类, 不得要写100个 if else了. 赶紧改改睡吧 1234567891011121314151617181920212223242526272829@Configurationpublic class MyExceptionHandler implements HandlerExceptionResolver &#123; public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; PrintWriter out = getPrintWrite(response); if (ex instanceof XXXException) &#123; out.write(JsonUtil.formatJson(ResultEnum.PAY_ERROR.getCode(), ex.getMessage())); &#125; else &#123; out.write(JsonUtil.formatJson(ResultEnum.FAIL.getCode(), &quot;服务器异常&quot;)); &#125; if (null != out) &#123; out.close(); &#125; return mav; &#125; private PrintWriter getPrintWrite(HttpServletResponse response) &#123; PrintWriter out = null; try &#123; response.setHeader(&quot;Content-type&quot;, &quot;text/html;charset=UTF-8&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); out = response.getWriter(); &#125; catch (IOException e) &#123; log.error(&quot;PrintWriter is exception&quot;, e); &#125; return out; &#125;&#125; 上面的代码看看还是没有问题的, 别学过去哦, 异常处理@ResponseStatus(不推荐)@ResponseStatus用法如下,可用在Controller类和Controller方法上以及Exception类上但是这样的工作量还是挺大的 12345678910111213141516171819202122232425262728293031323334@RestController@RequestMapping(&quot;/error&quot;)@ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR, reason = &quot;Java的异常&quot;)public class HelloExceptionController &#123; private static final HashMap&lt;String, Object&gt; INFO; static &#123; INFO = new HashMap&lt;String, Object&gt;(); INFO.put(&quot;name&quot;, &quot;galaxy&quot;); INFO.put(&quot;age&quot;, &quot;70&quot;); &#125; @GetMapping() public HashMap&lt;String, Object&gt; helloError() throws Exception &#123; throw new Exception(&quot;helloError&quot;); &#125; @GetMapping(&quot;helloJavaError&quot;) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR, reason = &quot;Java的异常&quot;) public HashMap&lt;String, Object&gt; helloJavaError() throws Exception &#123; throw new Exception(&quot;helloError&quot;); &#125; @GetMapping(&quot;helloMyError&quot;) public HashMap&lt;String, Object&gt; helloMyError() throws Exception &#123; throw new MyException(); &#125;&#125;@ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR, reason = &quot;自己定义的异常&quot;)class MyException extends Exception &#123;&#125; 全局异常处理@ExceptionHandler(推荐)把ResponseResultBodyAdvice类进行改造一下,代码有点多了 主要参考了org.springframework.web.servlet.mvc.method.annotation.ResponseEntityExceptionHandler#handleException()方法, 有空可以看一下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Slf4j@RestControllerAdvicepublic class ResponseResultBodyAdvice implements ResponseBodyAdvice&lt;Object&gt; &#123; private static final Class&lt;? extends Annotation&gt; ANNOTATION_TYPE = ResponseResultBody.class; /** 判断类或者方法是否使用了 @ResponseResultBody */ @Override public boolean supports(MethodParameter returnType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123; return AnnotatedElementUtils.hasAnnotation(returnType.getContainingClass(), ANNOTATION_TYPE) || returnType.hasMethodAnnotation(ANNOTATION_TYPE); &#125; /** 当类或者方法使用了 @ResponseResultBody 就会调用这个方法 */ @Override public Object beforeBodyWrite(Object body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) &#123; if (body instanceof Result) &#123; return body; &#125; return Result.success(body); &#125; /** * 提供对标准Spring MVC异常的处理 * * @param ex the target exception * @param request the current request */ @ExceptionHandler(Exception.class) public final ResponseEntity&lt;Result&lt;?&gt;&gt; exceptionHandler(Exception ex, WebRequest request) &#123; log.error(&quot;ExceptionHandler: &#123;&#125;&quot;, ex.getMessage()); HttpHeaders headers = new HttpHeaders(); if (ex instanceof ResultException) &#123; return this.handleResultException((ResultException) ex, headers, request); &#125; // TODO: 2019/10/05 galaxy 这里可以自定义其他的异常拦截 return this.handleException(ex, headers, request); &#125; /** 对ResultException类返回返回结果的处理 */ protected ResponseEntity&lt;Result&lt;?&gt;&gt; handleResultException(ResultException ex, HttpHeaders headers, WebRequest request) &#123; Result&lt;?&gt; body = Result.failure(ex.getResultStatus()); HttpStatus status = ex.getResultStatus().getHttpStatus(); return this.handleExceptionInternal(ex, body, headers, status, request); &#125; /** 异常类的统一处理 */ protected ResponseEntity&lt;Result&lt;?&gt;&gt; handleException(Exception ex, HttpHeaders headers, WebRequest request) &#123; Result&lt;?&gt; body = Result.failure(); HttpStatus status = HttpStatus.INTERNAL_SERVER_ERROR; return this.handleExceptionInternal(ex, body, headers, status, request); &#125; /** * org.springframework.web.servlet.mvc.method.annotation.ResponseEntityExceptionHandler#handleExceptionInternal(java.lang.Exception, java.lang.Object, org.springframework.http.HttpHeaders, org.springframework.http.HttpStatus, org.springframework.web.context.request.WebRequest) * &lt;p&gt; * A single place to customize the response body of all exception types. * &lt;p&gt;The default implementation sets the &#123;@link WebUtils#ERROR_EXCEPTION_ATTRIBUTE&#125; * request attribute and creates a &#123;@link ResponseEntity&#125; from the given * body, headers, and status. */ protected ResponseEntity&lt;Result&lt;?&gt;&gt; handleExceptionInternal( Exception ex, Result&lt;?&gt; body, HttpHeaders headers, HttpStatus status, WebRequest request) &#123; if (HttpStatus.INTERNAL_SERVER_ERROR.equals(status)) &#123; request.setAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE, ex, WebRequest.SCOPE_REQUEST); &#125; return new ResponseEntity&lt;&gt;(body, headers, status); &#125;&#125;","tags":["框架","Spring","分布式","微服务","SpringBoot"],"categories":["框架","Spring","SpringBoot"]},{"title":"Redis 分布式锁使用不当，酿成一个重大事故","path":"/2023/12/25/Redis-分布式锁使用不当，酿成一个重大事故/","content":"基于Redis使用分布式锁在当今已经不是什么新鲜事了。 本篇文章主要是基于我们实际项目中因为redis分布式锁造成的事故分析及解决方案。我们项目中的抢购订单采用的是分布式锁来解决的，有一次，运营做了一个飞天茅台的抢购活动，库存100瓶，但是却超卖了100瓶！要知道，这个地球上飞天茅台的稀缺性啊！！！ 事故定为P0级重大事故…只能坦然接受。整个项目组被扣绩效了~~事故发生后，CTO指名点姓让我带头冲锋来处理。 好吧，冲~ 事故现场经过一番了解后，得知这个抢购活动接口以前从来没有出现过这种情况，但是这次为什么会超卖呢？ 原因在于：之前的抢购商品都不是什么稀缺性商品，而这次活动居然是飞天茅台，通过埋点数据分析，各项数据基本都是成倍增长，活动热烈程度可想而知！话不多说，直接上核心代码，机密部分做了伪代码处理。。。 12345678910111213141516171819202122232425262728public SeckillActivityRequestVO seckillHandle(SeckillActivityRequestVO request) &#123; SeckillActivityRequestVO response; String key = &quot;key:&quot; + request.getSeckillId; try &#123; Boolean lockFlag = redisTemplate.opsForValue().setIfAbsent(key, &quot;val&quot;, 10, TimeUnit.SECONDS); if (lockFlag) &#123; // HTTP请求用户服务进行用户相关的校验 // 用户活动校验 // 库存校验 Object stock = redisTemplate.opsForHash().get(key+&quot;:info&quot;, &quot;stock&quot;); assert stock != null; if (Integer.parseInt(stock.toString()) &lt;= 0) &#123; // 业务异常 &#125; else &#123; redisTemplate.opsForHash().increment(key+&quot;:info&quot;, &quot;stock&quot;, -1); // 生成订单 // 发布订单创建成功事件 // 构建响应VO &#125; &#125; &#125; finally &#123; // 释放锁 stringRedisTemplate.delete(&quot;key&quot;); // 构建响应VO &#125; return response;&#125; 以上代码，通过分布式锁过期时间有效期10s来保障业务逻辑有足够的执行时间；采用try-finally语句块保证锁一定会及时释放。业务代码内部也对库存进行了校验。看起来很安全啊~ 别急，继续分析。。。 事故原因飞天茅台抢购活动吸引了大量新用户下载注册我们的APP，其中，不乏很多羊毛党，采用专业的手段来注册新用户来薅羊毛和刷单。当然我们的用户系统提前做好了防备，接入阿里云人机验证、三要素认证以及自研的风控系统等各种十八般武艺，挡住了大量的非法用户。此处不禁点个赞~ 但也正因如此，让用户服务一直处于较高的运行负载中。 抢购活动开始的一瞬间，大量的用户校验请求打到了用户服务。导致用户服务网关出现了短暂的响应延迟，有些请求的响应时长超过了10s，但由于HTTP请求的响应超时我们设置的是30s，这就导致接口一直阻塞在用户校验那里，10s后，分布式锁已经失效了，此时有新的请求进来是可以拿到锁的，也就是说锁被覆盖了。这些阻塞的接口执行完之后，又会执行释放锁的逻辑，这就把其他线程的锁释放了，导致新的请求也可以竞争到锁~这真是一个极其恶劣的循环。这个时候只能依赖库存校验，但是偏偏库存校验不是非原子性的，采用的是get and compare 的方式，超卖的悲剧就这样发生了~~~ 事故分析仔细分析下来，可以发现，这个抢购接口在高并发场景下，是有严重的安全隐患的，主要集中在三个地方： 没有其他系统风险容错处理 由于用户服务吃紧，网关响应延迟，但没有任何应对方式，这是超卖的导火索。 看似安全的分布式锁其实一点都不安全 虽然采用了set key value [EX seconds] [PX milliseconds] [NX|XX]的方式，但是如果线程A执行的时间较长没有来得及释放，锁就过期了，此时线程B是可以获取到锁的。当线程A执行完成之后，释放锁，实际上就把线程B的锁释放掉了。这个时候，线程C又是可以获取到锁的，而此时如果线程B执行完释放锁实际上就是释放的线程C设置的锁。这是超卖的直接原因。 非原子性的库存校验 非原子性的库存校验导致在并发场景下，库存校验的结果不准确。这是超卖的根本原因。 通过以上分析，问题的根本原因在于库存校验严重依赖了分布式锁。因为在分布式锁正常set、del的情况下，库存校验是没有问题的。但是，当分布式锁不安全可靠的时候，库存校验就没有用了。 解决方案知道了原因之后，我们就可以对症下药了。 实现相对安全的分布式锁相对安全的定义：set、del是一一映射的，不会出现把其他现成的锁del的情况。从实际情况的角度来看，即使能做到set、del一一映射，也无法保障业务的绝对安全。因为锁的过期时间始终是有界的，除非不设置过期时间或者把过期时间设置的很长，但这样做也会带来其他问题。故没有意义。要想实现相对安全的分布式锁，必须依赖key的value值。在释放锁的时候，通过value值的唯一性来保证不会勿删。我们基于LUA脚本实现原子性的get and compare，如下： 12345public void safedUnLock(String key, String val) &#123; String luaScript = &quot;local in = ARGV[1] local curr=redis.call(&#x27;get&#x27;, KEYS[1]) if in==curr then redis.call(&#x27;del&#x27;, KEYS[1]) end return &#x27;OK&#x27;&quot;&quot;; RedisScript&lt;String&gt; redisScript = RedisScript.of(luaScript); redisTemplate.execute(redisScript, Collections.singletonList(key), Collections.singleton(val));&#125; 我们通过LUA脚本来实现安全地解锁。 实现安全的库存校验如果我们对于并发有比较深入的了解的话，会发现想 get and compare&#x2F; read and save 等操作，都是非原子性的。如果要实现原子性，我们也可以借助LUA脚本来实现。但就我们这个例子中，由于抢购活动一单只能下1瓶，因此可以不用基于LUA脚本实现而是基于redis本身的原子性。原因在于： 12// redis会返回操作之后的结果，这个过程是原子性的Long currStock = redisTemplate.opsForHash().increment(&quot;key&quot;, &quot;stock&quot;, -1); 发现没有，代码中的库存校验完全是“画蛇添足”。 改进之后的代码经过以上的分析之后，我们决定新建一个DistributedLocker类专门用于处理分布式锁。 123456789101112131415161718192021222324252627public SeckillActivityRequestVO seckillHandle(SeckillActivityRequestVO request) &#123; SeckillActivityRequestVO response; String key = &quot;key:&quot; + request.getSeckillId(); String val = UUID.randomUUID().toString(); try &#123; Boolean lockFlag = distributedLocker.lock(key, val, 10, TimeUnit.SECONDS); if (!lockFlag) &#123; // 业务异常 &#125; // 用户活动校验 // 库存校验，基于redis本身的原子性来保证 Long currStock = stringRedisTemplate.opsForHash().increment(key + &quot;:info&quot;, &quot;stock&quot;, -1); if (currStock &lt; 0) &#123; // 说明库存已经扣减完了。 // 业务异常。 log.error(&quot;[抢购下单] 无库存&quot;); &#125; else &#123; // 生成订单 // 发布订单创建成功事件 // 构建响应 &#125; &#125; finally &#123; distributedLocker.safedUnLock(key, val); // 构建响应 &#125; return response;&#125; 深度思考分布式锁有必要么改进之后，其实可以发现，我们借助于redis本身的原子性扣减库存，也是可以保证不会超卖的。对的。但是如果没有这一层锁的话，那么所有请求进来都会走一遍业务逻辑，由于依赖了其他系统，此时就会造成对其他系统的压力增大。这会增加的性能损耗和服务不稳定性，得不偿失。基于分布式锁可以在一定程度上拦截一些流量。 分布式锁的选型有人提出用RedLock来实现分布式锁。RedLock的可靠性更高，但其代价是牺牲一定的性能。在本场景，这点可靠性的提升远不如性能的提升带来的性价比高。如果对于可靠性极高要求的场景，则可以采用RedLock来实现。 再次思考分布式锁有必要么由于bug需要紧急修复上线，因此我们将其优化并在测试环境进行了压测之后，就立马热部署上线了。实际证明，这个优化是成功的，性能方面略微提升了一些，并在分布式锁失效的情况下，没有出现超卖的情况。然而，还有没有优化空间呢？有的！由于服务是集群部署，我们可以将库存均摊到集群中的每个服务器上，通过广播通知到集群的各个服务器。网关层基于用户ID做hash算法来决定请求到哪一台服务器。这样就可以基于应用缓存来实现库存的扣减和判断。性能又进一步提升了！ 12345678910111213141516171819202122232425// 通过消息提前初始化好，借助ConcurrentHashMap实现高效线程安全private static ConcurrentHashMap&lt;Long, Boolean&gt; SECKILL_FLAG_MAP = new ConcurrentHashMap&lt;&gt;();// 通过消息提前设置好。由于AtomicInteger本身具备原子性，因此这里可以直接使用HashMapprivate static Map&lt;Long, AtomicInteger&gt; SECKILL_STOCK_MAP = new HashMap&lt;&gt;();...public SeckillActivityRequestVO seckillHandle(SeckillActivityRequestVO request) &#123; SeckillActivityRequestVO response; Long seckillId = request.getSeckillId(); if(!SECKILL_FLAG_MAP.get(requestseckillId)) &#123; // 业务异常 &#125; // 用户活动校验 // 库存校验 if(SECKILL_STOCK_MAP.get(seckillId).decrementAndGet() &lt; 0) &#123; SECKILL_FLAG_MAP.put(seckillId, false); // 业务异常 &#125; // 生成订单 // 发布订单创建成功事件 // 构建响应 return response;&#125; 通过以上的改造，我们就完全不需要依赖redis了。性能和安全性两方面都能进一步得到提升！当然，此方案没有考虑到机器的动态扩容、缩容等复杂场景，如果还要考虑这些话，则不如直接考虑分布式锁的解决方案。 总结稀缺商品超卖绝对是重大事故。如果超卖数量多的话，甚至会给平台带来非常严重的经营影响和社会影响。经过本次事故，让我意识到对于项目中的任何一行代码都不能掉以轻心，否则在某些场景下，这些正常工作的代码就会变成致命杀手！对于一个开发者而言，则设计开发方案时，一定要将方案考虑周全。怎样才能将方案考虑周全？唯有持续不断地学习！","tags":["Redis","NoSQL","分布式锁"],"categories":["DataBase","NoSQL"]},{"title":"Java中如何更优雅的处理空值","path":"/2023/12/25/Java中如何更优雅的处理空值/","content":"导语在笔者几年的开发经验中，经常看到项目中存在到处空值判断的情况，这些判断，会让人觉得摸不着头绪，它的出现很有可能和当前的业务逻辑并没有关系。但它会让你很头疼。 有时候，更可怕的是系统因为这些空值的情况，会抛出空指针异常，导致业务系统发生问题。 此篇文章，我总结了几种关于空值的处理手法，希望对读者有帮助。 业务中的空值场景存在一个UserSearchService用来提供用户查询的功能: 12345public interface UserSearchService&#123; List&lt;User&gt; listUser(); User get(Integer id); &#125; 问题现场对于面向对象语言来讲，抽象层级特别的重要。尤其是对接口的抽象，它在设计和开发中占很大的比重，我们在开发时希望尽量面向接口编程。 对于以上描述的接口方法来看，大概可以推断出可能它包含了以下两个含义: listUser(): 查询用户列表 get(Integer id): 查询单个用户 在所有的开发中，XP推崇的TDD模式可以很好的引导我们对接口的定义，所以我们将TDD作为开发代码的”推动者”。 对于以上的接口，当我们使用TDD进行测试用例先行时，发现了潜在的问题： listUser() 如果没有数据，那它是返回空集合还是null呢？ get(Integer id) 如果没有这个对象，是抛异常还是返回null呢？ 深入listUser研究我们先来讨论 1listUser() 这个接口，我经常看到如下实现: 1234567public List&lt;User&gt; listUser()&#123; List&lt;User&gt; userList = userListRepostity.selectByExample(new UserExample()); if(CollectionUtils.isEmpty(userList))&#123;//spring util工具类 return null; &#125; return userList; &#125; 这段代码返回是null,从我多年的开发经验来讲，对于集合这样返回值，最好不要返回null，因为如果返回了null，会给调用者带来很多麻烦。你将会把这种调用风险交给调用者来控制。 如果调用者是一个谨慎的人，他会进行是否为null的条件判断。如果他并非谨慎，或者他是一个面向接口编程的狂热分子(当然，面向接口编程是正确的方向)，他会按照自己的理解去调用接口，而不进行是否为null的条件判断，如果这样的话，是非常危险的，它很有可能出现空指针异常！ 根据墨菲定律来判断: “很有可能出现的问题，在将来一定会出现!” 基于此，我们将它进行优化: 1234567public List&lt;User&gt; listUser()&#123; List&lt;User&gt; userList = userListRepostity.selectByExample(new UserExample()); if(CollectionUtils.isEmpty(userList))&#123; return Lists.newArrayList();//guava类库提供的方式 &#125; return userList; &#125; 对于接口(List listUser())，它一定会返回List，即使没有数据，它仍然会返回List（集合中没有任何元素）; 通过以上的修改，我们成功的避免了有可能发生的空指针异常，这样的写法更安全！ 深入研究get方法对于接口 1User get(Integer id) 你能看到的现象是，我给出id，它一定会给我返回User.但事实真的很有可能不是这样的。 我看到过的实现: 123public User get(Integer id)&#123; return userRepository.selectByPrimaryKey(id);//从数据库中通过id直接获取实体对象 &#125; 相信很多人也都会这样写。 通过代码的时候得知它的返回值很有可能是null! 但我们通过的接口是分辨不出来的! 这个是个非常危险的事情。尤其对于调用者来说！ 我给出的建议是，需要在接口明明时补充文档,比如对于异常的说明,使用注解@exception: 1234567891011public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体 * @exception UserNotFoundException */ User get(Integer id); &#125; 我们把接口定义加上了说明之后，调用者会看到，如果调用此接口，很有可能抛出“UserNotFoundException(找不到用户)”这样的异常。 这种方式可以在调用者调用接口的时候看到接口的定义，但是，这种方式是”弱提示”的！ 如果调用者忽略了注释，有可能就对业务系统产生了风险，这个风险有可能导致一个亿！ 除了以上这种”弱提示”的方式，还有一种方式是，返回值是有可能为空的。那要怎么办呢？ 我认为我们需要增加一个接口，用来描述这种场景. 引入jdk8的Optional,或者使用guava的Optional.看如下定义: 123456789public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体,此实体有可能是缺省值 */ Optional&lt;User&gt; getOptional(Integer id); &#125; Optional有两个含义: 存在 or 缺省。 那么通过阅读接口getOptional()，我们可以很快的了解返回值的意图，这个其实是我们想看到的，它去除了二义性。 它的实现可以写成: 123public Optional&lt;User&gt; getOptional(Integer id)&#123; return Optional.ofNullable(userRepository.selectByPrimaryKey(id)); &#125; 深入入参通过上述的所有接口的描述，你能确定入参id一定是必传的吗？我觉得答案应该是：不能确定。除非接口的文档注释上加以说明。 那如何约束入参呢? 我给大家推荐两种方式： 强制约束 文档性约束（弱提示） 1.强制约束，我们可以通过jsr 303进行严格的约束声明: 12345678910111213141516public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体 * @exception UserNotFoundException */ User get(@NotNull Integer id); /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体,此实体有可能是缺省值 */ Optional&lt;User&gt; getOptional(@NotNull Integer id); &#125; 当然，这样写，要配合AOP的操作进行验证，但让spring已经提供了很好的集成方案，在此我就不在赘述了。 2.文档性约束 在很多时候，我们会遇到遗留代码，对于遗留代码，整体性改造的可能性很小。 我们更希望通过阅读接口的实现，来进行接口的说明。 jsr 305规范，给了我们一个描述接口入参的一个方式(需要引入库 com.google.code.findbugs:jsr305): 可以使用注解: @Nullable @Nonnull @CheckForNull 进行接口说明。比如: 1234567891011121314151617public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体 * @exception UserNotFoundException */ @CheckForNull User get(@NonNull Integer id); /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体,此实体有可能是缺省值 */ Optional&lt;User&gt; getOptional(@NonNull Integer id); &#125; 小结通过 空集合返回值,Optional,jsr 303，jsr 305这几种方式，可以让我们的代码可读性更强，出错率更低！ 空集合返回值 ：如果有集合这样返回值时，除非真的有说服自己的理由，否则，一定要返回空集合，而不是null Optional: 如果你的代码是jdk8，就引入它！如果不是，则使用Guava的Optional,或者升级jdk版本！它很大程度的能增加了接口的可读性！ jsr 303: 如果新的项目正在开发，不防加上这个试试！一定有一种特别爽的感觉! jsr 305: 如果老的项目在你的手上，你可以尝试的加上这种文档型注解，有助于你后期的重构，或者新功能增加了，对于老接口的理解! 空对象模式场景我们来看一个DTO转化的场景，对象: 1234567891011@Data static class PersonDTO&#123; private String dtoName; private String dtoAge; &#125; @Data static class Person&#123; private String name; private String age; &#125; 需求是将Person对象转化成PersonDTO，然后进行返回。 当然对于实际操作来讲，返回如果Person为空，将返回null,但是PersonDTO是不能返回null的（尤其Rest接口返回的这种DTO）。 在这里，我们只关注转化操作，看如下代码: 1234567891011121314@Test public void shouldConvertDTO()&#123; PersonDTO personDTO = new PersonDTO(); Person person = new Person(); if(!Objects.isNull(person))&#123; personDTO.setDtoAge(person.getAge()); personDTO.setDtoName(person.getName()); &#125;else&#123; personDTO.setDtoAge(&quot;&quot;); personDTO.setDtoName(&quot;&quot;); &#125; &#125; 优化修改这样的数据转化，我们认识可读性非常差，每个字段的判断，如果是空就设置为空字符串(“”) 换一种思维方式进行思考，我们是拿到Person这个类的数据，然后进行赋值操作(setXXX),其实是不关系Person的具体实现是谁的。 那我们可以创建一个Person子类: 1234567891011static class NullPerson extends Person&#123; @Override public String getAge() &#123; return &quot;&quot;; &#125; @Override public String getName() &#123; return &quot;&quot;; &#125; &#125; 它作为Person的一种特例而存在，如果当Person为空的时候，则返回一些get*的默认行为. 所以代码可以修改为: 12345678910111213@Test public void shouldConvertDTO()&#123; PersonDTO personDTO = new PersonDTO(); Person person = getPerson(); personDTO.setDtoAge(person.getAge()); personDTO.setDtoName(person.getName()); &#125; private Person getPerson()&#123; return new NullPerson();//如果Person是null ,则返回空对象 &#125; 其中getPerson()方法，可以用来根据业务逻辑获取Person有可能的对象（对当前例子来讲，如果Person不存在，返回Person的的特例NUllPerson），如果修改成这样，代码的可读性就会变的很强了。 使用Optional可以进行优化空对象模式，它的弊端在于需要创建一个特例对象，但是如果特例的情况比较多，我们是不是需要创建多个特例对象呢，虽然我们也使用了面向对象的多态特性，但是，业务的复杂性如果真的让我们创建多个特例对象，我们还是要再三考虑一下这种模式，它可能会带来代码的复杂性。 对于上述代码，还可以使用Optional进行优化。 1234567891011121314@Test public void shouldConvertDTO()&#123; PersonDTO personDTO = new PersonDTO(); Optional.ofNullable(getPerson()).ifPresent(person -&gt; &#123; personDTO.setDtoAge(person.getAge()); personDTO.setDtoName(person.getName()); &#125;); &#125; private Person getPerson()&#123; return null; &#125; Optional对空值的使用，我觉得更为贴切，它只适用于”是否存在”的场景。 如果只对控制的存在判断，我建议使用Optional. Optioanl的正确使用Optional如此强大，它表达了计算机最原始的特性(0 or 1),那它如何正确的被使用呢! Optional不要作为参数如果你写了一个public方法，这个方法规定了一些输入参数，这些参数中有一些是可以传入null的，那这时候是否可以使用Optional呢？ 我给的建议是: 一定不要这样使用! 举个例子: 123public interface UserService&#123; List&lt;User&gt; listUser(Optional&lt;String&gt; username); &#125; 这个例子的方法 listUser,可能在告诉我们需要根据username查询所有数据集合，如果username是空，也要返回所有的用户集合. 当我们看到这个方法的时候，会觉得有一些歧义: “如果username是absent,是返回空集合吗？还是返回全部的用户数据集合？” Optioanl是一种分支的判断，那我们究竟是关注 Optional还是Optional.get()呢？ 我给大家的建议是，如果不想要这样的歧义，就不要使用它！ 如果你真的想表达两个含义，就給它拆分出两个接口: 1234public interface UserService&#123; List&lt;User&gt; listUser(String username); List&lt;User&gt; listUser(); &#125; 我觉得这样的语义更强，并且更能满足 软件设计原则中的 “单一职责”。 如果你觉得你的入参真的有必要可能传null,那请使用jsr 303或者jsr 305进行说明和验证! 请记住! Optional不能作为入参的参数! Optional作为返回值当个实体的返回那Optioanl可以做为返回值吗？ 其实它是非常满足是否存在这个语义的。 你如说，你要根据id获取用户信息，这个用户有可能存在或者不存在。 你可以这样使用: 123public interface UserService&#123; Optional&lt;User&gt; get(Integer id); &#125; 当调用这个方法的时候，调用者很清楚get方法返回的数据，有可能不存在，这样可以做一些更合理的判断，更好的防止空指针的错误！ 当然，如果业务方真的需要根据id必须查询出User的话，就不要这样使用了，请说明，你要抛出的异常. 只有当考虑它返回null是合理的情况下，才进行Optional的返回 集合实体的返回不是所有的返回值都可以这样用的！如果你返回的是集合： 123public interface UserService&#123; Optional&lt;List&lt;User&gt;&gt; listUser(); &#125; 这样的返回结果，会让调用者不知所措，是否我判断Optional之后，还用进行isEmpty的判断呢？ 这样带来的返回值歧义！我认为是没有必要的。 我们要约定，对于List这种集合返回值，如果集合真的是null的，请返回空集合(Lists.newArrayList); 使用Optional变量1Optional&lt;User&gt; userOpt = ... 如果有这样的变量userOpt,请记住 ： 一定不能直接使用get ，如果这样用，就丧失了Optional本身的含义 （ 比如userOp.get() ） 不要直接使用getOrThrow ,如果你有这样的需求：获取不到就抛异常。那就要考虑，是否是调用的接口设计的是否合理 getter中的使用对于一个java bean,所有的属性都有可能返回null,那是否需要改写所有的getter成为Optional类型呢？ 我给大家的建议是，不要这样滥用Optional. 即便 我java bean中的getter是符合Optional的，但是因为java bean 太多了，这样会导致你的代码有50%以上进行Optinal的判断，这样便污染了代码。(我想说，其实你的实体中的字段应该都是由业务含义的，会认真的思考过它存在的价值的，不能因为Optional的存在而滥用) 我们应该更关注于业务，而不只是空值的判断。 请不要在getter中滥用Optional. 小结可以这样总结Optional的使用： 当使用值为空的情况，并非源于错误时，可以使用Optional! Optional不要用于集合操作! 不要滥用Optional,比如在java bean的getter中!","tags":["Java"],"categories":["Java"]},{"title":"Java 性能优化：35 个小细节，提升你的 Java 代码运行效率","path":"/2023/12/25/Java-性能优化：35-个小细节，提升你的-Java-代码运行效率/","content":"前言代码优化 ，一个很重要的课题。可能有些人觉得没用，一些细小的地方有什么好修改的，改与不改对于代码的运行效率有什么影响呢？这个问题我是这么考虑的，就像大海里面的鲸鱼一样，它吃一条小虾米有用吗？没用，但是，吃的小虾米一多之后，鲸鱼就被喂饱了。 代码优化也是一样，如果项目着眼于尽快无BUG上线，那么此时可以抓大放小，代码的细节可以不精打细磨；但是如果有足够的时间开发、维护代码，这时候就必须考虑每个可以优化的细节了，一个一个细小的优化点累积起来，对于代码的运行效率绝对是有提升的。 代码优化的目标是 1.减小代码的体积 2.提高代码运行的效率 代码优化细节1、尽量指定类、方法的final修饰符 带有final修饰符的类是不可派生的。在Java核心API中，有许多应用final的例子，例如java.lang.String，整个类都是final的。为类指定final修饰符可以让类不可以被继承，为方法指定final修饰符可以让方法不可以被重写。如果指定了一个类为final，则该类所有的方法都是final的。Java编译器会寻找机会内联所有的final方法，内联对于提升Java运行效率作用重大，具体参见Java运行期优化。 此举能够使性能平均提高50% 。 2、尽量重用对象 特别是String对象的使用，出现字符串连接时应该使用StringBuilder&#x2F;StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 3、尽可能使用局部变量 调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 4、及时关闭流 Java编程过程中，进行数据库连接、I&#x2F;O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。 5、尽量减少对变量的重复计算 明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： 123for (int i = 0; i &lt; list.size(); i++) &#123; // todo ...&#125; 建议替换为： 123for (int i = 0, int length = list.size(); i &lt; length; i++) &#123; // todo ...&#125; 这样，在list.size()很大的时候，就减少了很多的消耗 6、尽量采用懒加载的策略，即在需要的时候才创建 例如： 1234String str = &quot;aaa&quot;;if (i == 1) &#123;\tlist.add(str);&#125; 建议替换为： 1234if (i == 1) &#123;\tString str = &quot;aaa&quot;;\tlist.add(str);&#125; 7、慎用异常 异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为fillInStackTrace()的本地同步方法，fillInStackTrace()方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 8、不要在循环中使用try…catch…，应该把其放在最外层 除非不得已。如果毫无理由地这么写了，只要你的领导资深一点、有强迫症一点，八成就要骂你为什么写出这种垃圾代码来了。 9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度 比如ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet等等，以StringBuilder为例： （1）StringBuilder() &#x2F;&#x2F; 默认分配16个字符的空间 （2）StringBuilder(int size) &#x2F;&#x2F; 默认分配size个字符的空间 （3）StringBuilder(String str) &#x2F;&#x2F; 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的StringBuilder）的来设定它的初始化容量，这样可以明显地提升性能。比如StringBuilder吧，length表示当前的StringBuilder能保持的字符数量。因为当StringBuilder达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要StringBuilder达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中—-这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： （1）在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间； （2）把原来的4096个字符拷贝到新的的字符数组中去。 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成new HashMap(128)、new HashMap(256)都可以。 10、当复制大量数据时，使用System.arraycopy()命令 11、乘法和除法使用移位操作 例如： 1234for (val = 0; val &lt; 100000; val += 5) &#123;\ta = val * 8;\tb = val / 2;&#125; 用移位操作可以极大地提高性能，因为在计算机底层，对位的操作是最方便、最快的，因此建议修改为： 1234for (val = 0; val &lt; 100000; val += 5) &#123;\ta = val &lt;&lt; 3;\tb = val &gt;&gt; 1;&#125; 移位操作虽然快，但是可能会使代码不太好理解，因此最好加上相应的注释。 12、循环内不要不断创建对象引用 例如： 123for (int i = 1; i &lt;= count; i++) &#123;\tObject obj = new Object();&#125; 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： 1234Object obj = null;for (int i = 0; i &lt;= count; i++) &#123; obj = new Object(); &#125; 这样的话，内存中只有一份Object对象引用，每次new Object()的时候，Object对象引用指向不同的Object罢了，但是内存中只有一份，这样就大大节省了内存空间了。 13、基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用ArrayList 14、尽量使用HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销 15、不要将数组声明为public static final 因为这毫无意义，这样只是定义了引用为static final，数组的内容还是可以随意改变的，将数组声明为public更是一个安全漏洞，这意味着这个数组可以被外部类所改变。 16、尽量在合适的场合使用单例 使用单例可以减轻加载的负担、缩短加载的时间、提高加载的效率，但并不是所有地方都适用于单例，简单来说，单例主要适用于以下三个方面： （1）控制资源的使用，通过线程同步来控制资源的并发访问 （2）控制实例的产生，以达到节约资源的目的 （3）控制数据的共享，在不建立直接关联的条件下，让多个不相关的进程或线程之间实现通信 17、尽量避免随意使用静态变量 要知道，当某个对象被定义为static的变量所引用，那么gc通常是不会回收这个对象所占有的堆内存的，如： 123public class A &#123;\tprivate static B b = new B();&#125; 此时静态变量b的生命周期与A类相同，如果A类不被卸载，那么引用B指向的B对象会常驻内存，直到程序终止 18、及时清除不再需要的会话 为了清除不再活动的会话，许多应用服务器都有默认的会话超时时间，一般为30分钟。当应用服务器需要保存更多的会话时，如果内存不足，那么操作系统会把部分数据转移到磁盘，应用服务器也可能根据MRU（最近最频繁使用）算法把部分不活跃的会话转储到磁盘，甚至可能抛出内存不足的异常。如果会话要被转储到磁盘，那么必须要先被序列化，在大规模集群中，对对象进行序列化的代价是很昂贵的。因此，当会话不再需要时，应当及时调用HttpSession的invalidate()方法清除会话。 19、实现RandomAccess接口的集合比如ArrayList，应当使用最普通的for循环而不是foreach循环来遍历 这是JDK推荐给用户的。JDK API对于RandomAccess接口的解释是：实现RandomAccess接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现RandomAccess接口的类实例，假如是随机访问的，使用普通for循环效率将高于使用foreach循环；反过来，如果是顺序访问的，则使用Iterator会效率更高。可以使用类似如下的代码作判断 12345678910if (list instanceof RandomAccess) &#123; for (int i = 0; i &lt; list.size(); i++) &#123; &#125;&#125; else &#123; Iterator&lt;?&gt; iterator = list.iterable(); while (iterator.hasNext()) &#123; iterator.next() &#125;&#125; foreach循环的底层实现原理就是迭代器Iterator，参见Java语法糖1：可变长度参数以及foreach循环原理。所以后半句”反过来，如果是顺序访问的，则使用Iterator会效率更高”的意思就是顺序访问的那些类实例，使用foreach循环去遍历。 20、使用同步代码块替代同步方法 这点在多线程模块中的synchronized锁方法块一文中已经讲得很清楚了，除非能确定一整个方法都是需要进行同步的，否则尽量使用同步代码块，避免对那些不需要进行同步的代码也进行了同步，影响了代码执行效率。 21、将常量声明为static final，并以大写命名 这样在编译期间就可以把这些内容放入常量池中，避免运行期间计算生成常量的值。另外，将常量的名字以大写命名也可以方便区分出常量与变量 22、不要创建一些不使用的对象，不要导入一些不使用的类 这毫无意义，如果代码中出现”The value of the local variable i is not used”、”The import java.util is never used”，那么请删除这些无用的内容 23、程序运行过程中避免使用反射 关于，请参见反射。反射是Java提供给用户一个很强大的功能，功能强大往往意味着效率不高。不建议在程序运行过程中使用尤其是频繁使用反射机制，特别是Method的invoke方法，如果确实有必要，一种建议性的做法是将那些需要通过反射加载的类在项目启动的时候通过反射实例化出一个对象并放入内存—-用户只关心和对端交互的时候获取最快的响应速度，并不关心对端的项目启动花多久时间。 24、使用数据库连接池和线程池 这两个池都是用于重用对象的，前者可以避免频繁地打开和关闭连接，后者可以避免频繁地创建和销毁线程 25、使用带缓冲的输入输出流进行IO操作 带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率 26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList这个，理解ArrayList和LinkedList的原理就知道了 27、不要让public方法中有太多的形参 public方法即对外提供的方法，如果给这些方法太多形参的话主要有两点坏处： 1、违反了面向对象的编程思想，Java讲求一切都是对象，太多的形参，和面向对象的编程思想并不契合 2、参数太多势必导致方法调用的出错概率增加 至于这个”太多”指的是多少个，3、4个吧。比如我们用JDBC写一个insertStudentInfo方法，有10个学生信息字段要插如Student表中，可以把这10个参数封装在一个实体类中，作为insert方法的形参。 28、字符串变量和字符串常量equals的时候将字符串常量写在前面 这是一个比较常见的小技巧了，如果有以下代码： 12345678String str = &quot;123&quot;;if (str.equals(&quot;123&quot;)) &#123; // todo...&#125;// 建议修改为：String str = &quot;123&quot;;if (&quot;123&quot;.equals(str)) &#123; // todo ...&#125; 这么做主要是可以避免空指针异常 29、请知道，在java中if (i &#x3D;&#x3D; 1)和if (1 &#x3D;&#x3D; i)是没有区别的，但从阅读习惯上讲，建议使用前者 平时有人问，”if (i &#x3D;&#x3D; 1)”和”if (1&#x3D;&#x3D; i)”有没有区别，这就要从C&#x2F;C++讲起。 在C&#x2F;C++中，”if (i &#x3D;&#x3D; 1)”判断条件成立，是以0与非0为基准的，0表示false，非0表示true，如果有这么一段代码： 123456int i = 2;if (i == 1) &#123; // todo ...&#125; else &#123;\t// todo ...&#125; C&#x2F;C++判断”i&#x3D;&#x3D;1″不成立，所以以0表示，即false。但是如果： 123456int i = 2;if (i = 1) &#123; // todo ... &#125; else &#123; // todo ... &#125; 万一程序员一个不小心，把”if (i &#x3D;&#x3D; 1)”写成”if (i &#x3D; 1)”，这样就有问题了。在if之内将i赋值为1，if判断里面的内容非0，返回的就是true了，但是明明i为2，比较的值是1，应该返回的false。这种情况在C&#x2F;C++的开发中是很可能发生的并且会导致一些难以理解的错误产生，所以，为了避免开发者在if语句中不正确的赋值操作，建议将if语句写为： 123456int i = 2;if (1 == i) &#123; // todo ... &#125; else &#123; // todo ... &#125; 这样，即使开发者不小心写成了”1 &#x3D; i”，C&#x2F;C++编译器也可以第一时间检查出来，因为我们可以对一个变量赋值i为1，但是不能对一个常量赋值1为i。 但是，在Java中，C&#x2F;C++这种”if (i &#x3D; 1)”的语法是不可能出现的，因为一旦写了这种语法，Java就会编译报错”Type mismatch: cannot convert from int to boolean”。但是，尽管Java的”if (i &#x3D;&#x3D; 1)”和”if (1 &#x3D;&#x3D; i)”在语义上没有任何区别，但是从阅读习惯上讲，建议使用前者会更好些。 30、不要对数组使用toString()方法 看一下对数组使用toString()打印出来的是什么： 1234public static void main(String[] args) &#123; int[] is = new int[]&#123;1, 2, 3&#125;;\tSystem.out.println(is.toString());&#125; 结果是： 1[I@18a992f 本意是想打印出数组内容，却有可能因为数组引用is为空而导致空指针异常。不过虽然对数组toString()没有意义，但是对集合toString()是可以打印出集合里面的内容的，因为集合的父类AbstractCollections重写了Object的toString()方法。 31、不要对超出范围的基本数据类型做向下强制转型 这绝不会得到想要的结果： 12345public static void main(String[] args) &#123; long l = 12345678901234L; int i = (int)l; System.out.println(i);&#125; 我们可能期望得到其中的某几位，但是结果却是： 11942892530 解释一下。Java中long是8个字节64位的，所以12345678901234在计算机中的表示应该是： 10000 0000 0000 0000 0000 1011 0011 1010 0111 0011 1100 1110 0010 1111 1111 0010 一个int型数据是4个字节32位的，从低位取出上面这串二进制数据的前32位是： 10111 0011 1100 1110 0010 1111 1111 0010 这串二进制表示为十进制1942892530，所以就是我们上面的控制台上输出的内容。从这个例子上还能顺便得到两个结论 1、整型默认的数据类型是int，long l &#x3D; 12345678901234L，这个数字已经超出了int的范围了，所以最后有一个L，表示这是一个long型数。顺便，浮点型的默认类型是double，所以定义float的时候要写成””float f &#x3D; 3.5f” 2、接下来再写一句”int ii &#x3D; l + i;”会报错，因为long + int是一个long，不能赋值给int 32、公用的集合类中不使用的数据一定要及时remove掉 如果一个集合类是公用的（也就是说不是方法里面的属性），那么这个集合里面的元素是不会自动释放的，因为始终有引用指向它们。所以，如果公用集合里面的某些数据不使用而不去remove掉它们，那么将会造成这个公用集合不断增大，使得系统有内存泄露的隐患。 33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+””最慢 把一个基本数据类型转为一般有三种方式，我有一个Integer型数据i，可以使用i.toString()、String.valueOf(i)、i+””三种方式，三种方式的效率如何，看一个测试： 12345678910111213141516171819202122public static void main(String[] args) &#123; int loopTime = 50000;\tInteger i = 0;\tlong startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = String.valueOf(i); &#125;\tSystem.out.println(&quot;String.valueOf()：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123;\tString str = i.toString();\t&#125;\tSystem.out.println(&quot;Integer.toString()：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = i + &quot;&quot;;\t&#125;\tSystem.out.println(&quot;i + \\&quot;\\&quot;：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;);&#125; 运行结果为： 123String.valueOf()：11ms Integer.toString()：5ms i + &quot;&quot;：25ms 所以以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： 1、String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 2、Integer.toString()方法就不说了，直接调用了 3、i + “”底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 三者对比下来，明显是2最快、1次之、3最慢 34、使用最有效率的方式去遍历Map 遍历Map的方式有很多，通常场景下我们需要的是遍历Map中的Key和Value，那么推荐使用的、效率最高的方式是： 1234567891011public static void main(String[] args) &#123;\tHashMap&lt;String, String&gt; hm = new HashMap&lt;String, String&gt;(); hm.put(&quot;111&quot;, &quot;222&quot;); Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = hm.entrySet(); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iter = entrySet.iterator(); while (iter.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = iter.next(); System.out.println(entry.getKey() + &quot;\\t&quot; + entry.getValue());\t&#125;&#125; 如果你只是想遍历一下这个Map的key值，那用”Set keySet &#x3D; hm.keySet();”会比较合适一些 35、对资源的close()建议分开操作 意思是，比如我有这么一段代码： 123456try&#123;\tXXX.close();\tYYY.close();&#125; catch (Exception e) &#123; // todo ...&#125; 建议修改为： 1234567891011try &#123; XXX.close(); &#125; catch (Exception e) &#123; // todo ... &#125;try &#123; YYY.close(); &#125; catch (Exception e) &#123; // todo ... &#125; 虽然有些麻烦，却能避免资源泄露。我想，如果没有修改过的代码，万一XXX.close()抛异常了，那么就进入了cath块中了，YYY.close()不会执行，YYY这块资源就不会回收了，一直占用着，这样的代码一多，是可能引起资源句柄泄露的。而改为上面的写法之后，就保证了无论如何XXX和YYY都会被close掉。","tags":["Java","性能优化"],"categories":["Java"]},{"title":"使用基于 SpringMVC 的透明 RPC 开发微服务","path":"/2023/12/25/使用基于-SpringMVC-的透明-RPC-开发微服务/","content":"来源：fredal.xin&#x2F;develop-with-transparent-rpc 我司目前 RPC 框架是基于 Java Rest 的方式开发的，形式上可以参考 SpringCloud Feign 的实现。Rest 风格随着微服务的架构兴起，Spring MVC 几乎成为了 Rest 开发的规范，同时对于 Spring 的使用者门槛也比较低。 REST 与 RPC 风格的开发方式RPC 框架采用类 Feign 方式的一个简单的实现例子如下： 12345@RpcClient(schemaId=&quot;hello&quot;)public interface Hello &#123; @GetMapping(&quot;/message&quot;) HelloMessage hello(@RequestParam String name);&#125; 而服务提供者直接使用 spring mvc 来暴露服务接口： 123456789101112@RestControllerpublic class HelloController &#123; @Autowired private HelloService helloService; @GetMapping(&quot;/message&quot;) public HelloMessage getMessage(@RequestParam(name=&quot;name&quot;)String name) &#123; HelloMessage hello = helloService.gen(name); return hello; &#125;&#125; 基于 REST 风格开发的方式有很多优点。一是使用门槛较低，服务端完全基于 Spring MVC，客户端 api 的书写方式也兼容了大部分 Spring 的注解，包括@RequestParam、@RequestBody 等。二是带来的解耦特性，微服务应用注重服务自治，对外则提供松耦合的 REST 接口，这种方式更灵活，可以减轻历史包袱带来的痛点，同时除了提供给类 SDK 的消费者服务外，还可提供浏览器等非 SDK 的消费者服务。 当然这种方式在实际运用中也带来了很多麻烦。首先，不一致的客户端与服务端 API 带来了出错的可能性，Controller 接口的返回值类型与 RpcClient 的返回值类型可能写的不一致从而导致反序列化失败。其次，RpcClient 的书写虽然兼容了 Spring 的注解，但对于某些开发同学仍然存在不小的门槛，例如写 url param 时@RequestParam 注解常常忘写，写 body param 时候@RequestBody 注解忘记写，用@RequestBody 注解来标注 String 参数，方法类型不指定等等（基本上和使用 Feign 的门槛一样）。 还有一点，就是比起常见的 RPC 方式，REST 方式相当于多写了一层 Controller，而不是直接将 Service 暴露成接口。DDD 实践中，将一个巨石应用拆分成各个限界上下文时，往往是对旧代码的 Service 方法进行拆分，REST 风格意味着需要多写 Controller 接入表示层，而在内部微服务应用间相互调用的场景下，暴露应用服务层甚至领域服务层给调用者可能是更简便的方法，在满足 DDD 的同时更符合 RPC 的语义。 那么我们希望能通过一种基于透明 RPC 风格的开发方式来优雅简便地开发微服务。 首先我们希望服务接口的定义能更简便，不用写多余的注解和信息： 1234@RpcClient(schemaId=&quot;hello&quot;)public interface Hello &#123; HelloMessage hello(String name);&#125; 然后我们就可以实现这个服务，并通过使用注解的方式简单的发布服务： 1234567@RpcService(schemaId=&quot;hello&quot;)public class HelloImpl implements Hello&#123; @Override HelloMessage hello(String name)&#123; return new HelloMessage(name); &#125;&#125; 这样客户端在引用 Hello 接口后可以直接使用里面的 hello()方法调用到服务端的实现类 HelloImpl 中，从而获得一个 HelloMessage 对象。相比之前的 REST 实现方式，在简洁性以及一致性上都得到了提升。 隐式的服务契约服务契约指客户端与服务端之间对于接口的描述定义。REST 风格开发方式中，我们使用 Spring MVC annotation 来声明接口的请求、返回参数。但是在透明 RPC 开发方式中，理论上我们可以不用写任何 RESTful 的 annotation 的，这时候怎么去定义服务契约呢。 其实这里运用了隐式的服务契约，可以不事先定义契约和接口，而是直接定义实现类，根据实现类去自动生成默认的契约，注册到服务中心。 默认的服务契约内容包括方法类型的选择、URL 地址以及参数注解的处理。方法类型的判断基于入参类型，如果入参类型中包含自定义类型、Object 或者集合等适合放在 Body 中的类型，则会判断为使用 POST 方法，而如果入参仅有 String 或者基本类型等，则判断使用 GET 方法。POST 方法会将所有参数作为 Body 进行传送，而 GET 方法则将参数作为 URL PARAM 进行传送。URL 地址的默认规则为/类名/方法类型+方法名，未被注解的方法都会按此 URL 注册到服务中心。 服务端的 REST 编程模型我们可以发现，两种开发风格最大的改变是服务端编程模型的改变，从 REST 风格的 SpringMVC 编程模型变成了透明 RPC 编程模型。我们应该怎样去实现这一步呢？ 我们目前的运行架构如上图，服务端的编程模型完全基于 Spring MVC，通信模型则是基于 servlet 的。我们期望服务端的编程模型可以转换为 RPC，那么势必需要我们对通信模型做一定的改造。 从 DispatcherServlet 说起那么首先，我们需要对 Spring MVC 实现的 servlet 规范 DispatcherServlet 做一定的了解，知道它是怎么处理一个请求的。 DispatcherServlet 主要包含三部分逻辑，映射处理器(HandlerMapping)，映射适配器(HandlerAdapter)，视图处理器(ViewResolver)。DispatcherServlet 通过 HandlerMapping 找到合适的 Handler，再通过 HandlerAdapter 进行适配，最终返回 ModelAndView 经由 ViewResolver 处理返回给前端。 回到主题上，我们想要改造这部分通信模型从而能够实现 RPC 的编程模型有两种办法，一是直接编写一个新的 Servlet，实现 REST over Servlet 的效果，从而对服务端通信逻辑得到一个完整的控制，这样我们可以为服务端添加自定义的运行模型(服务端限流、调用链处理等)。二是仅仅修改一部分 HandlerMapping 的代码，将请求映射变得可以适配 RPC 的编程模型。 鉴于工作量与现实条件，我们选择后一种方法，继续沿用 DispatcherServlet，但改造部分 HandlerMapping 的代码。 首先我们会通过 Scanner 扫描到标注了@RpcClient 注解的接口以及其实现类，我们会将其注册到 HandlerMapping 中，所以首先我们要看 HandlerMapping 中有没有能扩展注册逻辑的地方。 接着我们再考虑处理请求的事儿，我们需要 HandlerMapping 能够做到在没有 Spring Annotation 的情况下也能为不同的参数选择不同的 argumentResolver 参数处理器，这一点在 springMVC 中是通过标注注解来区分的(RequestMapping、RequestBody 等)，所以我们还需要看看 HandlerMapping 中有没有能扩展参数注解逻辑的地方。 带着这两点目的，我们先来看 HandlerMapping 的逻辑。 HandlerMapping 的初始化HandlerMapping 的初始化源码比较长，我们直接一笔略过不是很重要的部分了。首先 RequestMappingHandlerMapping 的父类 AbstractHandlerMethodMapping 类实现了 InitializingBean 接口，在属性初始化完成后会调用 afterPropertiesSet()方法，在该方法中调用 initHandlerMethods()进行 HandlerMethod 初始化。InitHandlerMethods 方法中使用 detectHandlerMethods 方法从 bean 中根据 bean name 查找 handlerMethod，此方法中调用 registerHandlerMethod 来注册正常的 handlerMethod。 123protected void registerHandlerMethod(Object handler, Method method, T mapping) &#123; this.mappingRegistry.register(mapping, handler, method); &#125; 我们发现这个方法是 protected 的，那么第一步我们找到了去哪注册我们的 RPC 方法到 RequestMappingHandlerMapping 中。接口可以看到入参是 handler 方法，但在 handlerMapping 中真正被注册的 handlerMethod 对象，显然这部分逻辑在 mappingRegistry 的 register 方法中。register 方法中我们找到了转换的关键方法： 1HandlerMethod handlerMethod = createHandlerMethod(handler, method); 此方法中调用了 handlerMethod 对象的构造器来构造一个 handlerMethod对象。handlerMethod 的属性中包含一个叫 parameters 的 methodParameter 对象数组。我们知道 handlerMethod 对象对应的是一个实现方法，那么 methodParameter 对象对应的就是入参了。 接着往 methodParameter 对象里看，发现了一个叫 parameterAnnotations 的 Annotation 数组，看样子这就是我们第二个需要关注的地方了。那么总结一下，滤去无需关注的部分，handlerMapping 的初始化整个如下图所示： HandlerAdapter 的请求处理这边 dispatcherServlet 在真正处理请求的时候是用 handlerAdapter 去处理再返回 ModelAndView 对象的，但是所有相关对象都是注册在 handlerMapping 中。 我们直接来看看 RequestMappingHandlerAdapter 的处理逻辑吧，handlerAdapter 在 handle 方法中调用 handleInternal 方法，并调用 invokeHandlerMethod 方法，此方法中使用 createInvocableHandlerMethod 方法将 handlerMethod 对象包装成了一个 servletInvocableHandlerMethod 对象，此对象最终调用 invokeAndHandle 方法完成对应请求逻辑的处理。我们只关注 invokeAndHandle 里面的 invokeForRequest 方法，该方法作为对入参的处理正是我们的目标。最终我们看到了此方法中的 getMethodArgumentValues 方法中的一段对入参注解的处理逻辑: 12345678910if (this.argumentResolvers.supportsParameter(parameter)) &#123; try &#123; args[i] = this.argumentResolvers.resolveArgument(parameter, mavContainer, request, this.dataBinderFactory); &#125; catch (Exception var9) &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(this.getArgumentResolutionErrorMessage(&quot;Error resolving argument&quot;, i), var9); &#125; throw var9; &#125;&#125; 显然，这里使用 supportsParameter 方法来作为判断依据选择 argumentResolver，里层的逻辑就是一个简单的遍历选择真正支持入参的参数处理器。实际上 RequestMappingHandlerAdapte 在初始化时候就注册了一堆参数处理器： 1234567891011121314151617 private List&lt;HandlerMethodReturnValueHandler&gt; getDefaultReturnValueHandlers() &#123; List&lt;HandlerMethodReturnValueHandler&gt; handlers = new ArrayList&lt;HandlerMethodReturnValueHandler&gt;(); // Single-purpose return value types handlers.add(new ModelAndViewMethodReturnValueHandler()); handlers.add(new ModelMethodProcessor()); handlers.add(new ViewMethodReturnValueHandler()); handlers.add(new ResponseBodyEmitterReturnValueHandler(getMessageConverters())); handlers.add(new StreamingResponseBodyReturnValueHandler()); handlers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.contentNegotiationManager, this.requestResponseBodyAdvice)); handlers.add(new HttpHeadersReturnValueHandler()); handlers.add(new CallableMethodReturnValueHandler()); handlers.add(new DeferredResultMethodReturnValueHandler()); handlers.add(new AsyncTaskMethodReturnValueHandler(this.beanFactory));\t//...&#125; 我们调个眼熟的 RequestResponseBodyMethodProcessor 来看看其 supportsParameter 方法: 1234@Overridepublic boolean supportsParameter(MethodParameter parameter) &#123; return parameter.hasParameterAnnotation(RequestBody.class);&#125; 这里直接调用了 MethodParameter 自身的 public 方法 hasParameterAnnotation 方法来判断是否有相应的注解，比如有 RequestBody 注解那么我们就选用 RequestResponseBodyMethodProcessor 来作为其参数处理器。 还是滤去无用逻辑，整个流程如下： 服务端的 RPC 编程模型以上我们了解了 DispatcherServlet 在 REST 编程模型中是部分逻辑，现在我们依据之前讲的改造部分 HandlerMapping 的代码从而使其适配 RPC 编程模型。 RPC 方法注册首先我们需要将方法注册到 handlerMapping，而这点由上述 RequestHandlerMapping 的初始化流程得知直接调用 registerHandlerMethod 方法即可。结合我们的扫描逻辑，大致代码如下： 1234567891011121314151617181920212223242526272829303132333435public class RpcRequestMappingHandlerMapping extends RequestMappingHandlerMapping&#123;\tpublic void registerRpcToMvc(final String prefix) &#123; final AdvancedApiToMvcScanner scanner = new AdvancedApiToMvcScanner(RpcService.class); scanner.setBasePackage(basePackage); Map&lt;Class&lt;?&gt;, Set&lt;MethodTemplate&gt;&gt; mvcMap; //扫描到注解了@RpcService的接口及method元信息 try &#123; mvcMap = scanner.scan(); &#125; catch (final IOException e) &#123; throw new FatalBeanException(&quot;failed to scan&quot;); &#125; for (final Class&lt;?&gt; clazz : mvcMap.keySet()) &#123; final Set&lt;MethodTemplate&gt; methodTemplates = mvcMap.get(clazz); for (final MethodTemplate methodTemplate : methodTemplates) &#123; if (methodTemplate == null) &#123; continue; &#125; final Method method = methodTemplate.getMethod(); Http.HttpMethod httpMethod; String uriTemplate = null; //隐式契约：方法类型和url地址 httpMethod = MvcFuncUtil.judgeMethodType(method); uriTemplate = MvcFuncUtil.genMvcFuncName(clazz, httpMethod.name(), method); final RequestMappingInfo requestMappingInfo = RequestMappingInfo .paths(this.resolveEmbeddedValuesInPatterns(new String[]&#123;uriTemplate&#125;)) .methods(RequestMethod.valueOf(httpMethod.name())) .build(); //注册到spring mvc this.registerHandlerMethod(handler, method, requestMappingInfo); &#125; &#125; &#125;&#125; 我们自定义了注册方法，只需在容器启动时调用即可。 RPC 请求处理以上所说，光完成注册是不够的，我们需要对入参注解做一些处理，例如我们虽然没有写注解@RequestBody User user，我们仍然希望 handlerAdapter 在处理的时候能够以为我们写了，并用 RequestResponseBodyMethodProcessor 参数解析器来进行处理。 我们直接重写 RequestMappingHandlerMapping 的 createHandlerMethod 方法： 1234567891011@Overrideprotected HandlerMethod createHandlerMethod(Object handler, Method method) &#123; HandlerMethod handlerMethod; if (handler instanceof String) &#123; String beanName = (String) handler; handlerMethod = new HandlerMethod(beanName, this.getApplicationContext().getAutowireCapableBeanFactory(), method); &#125; else &#123; handlerMethod = new HandlerMethod(handler, method); &#125; return new RpcHandlerMethod(handlerMethod);&#125; 我们自定义了自己的 HandlerMethod 对象： 12345678910111213141516public class RpcHandlerMethod extends HandlerMethod &#123; protected RpcHandlerMethod(HandlerMethod handlerMethod) &#123; super(handlerMethod); initMethodParameters(); &#125; private void initMethodParameters() &#123; MethodParameter[] methodParameters = super.getMethodParameters(); Annotation[][] parameterAnnotations = null; for (int i = 0; i &lt; methodParameters.length; i++) &#123; SynthesizingMethodParameter methodParameter = (SynthesizingMethodParameter) methodParameters[i]; methodParameters[i] = new RpcMethodParameter(methodParameter); &#125; &#125;&#125; 很容易看到，这里的重点是初始化了自定义的 MethodParameter 对象： 12345678910111213141516171819202122232425262728public class RpcMethodParameter extends SynthesizingMethodParameter &#123; private volatile Annotation[] annotations; protected RpcMethodParameter(SynthesizingMethodParameter original) &#123; super(original); this.annotations = initParameterAnnotations(); &#125; private Annotation[] initParameterAnnotations() &#123; List&lt;Annotation&gt; annotationList = new ArrayList&lt;&gt;(); final Class&lt;?&gt; parameterType = this.getParameterType(); if (MvcFuncUtil.isRequestParamClass(parameterType)) &#123; annotationList.add(MvcFuncUtil.newRequestParam(MvcFuncUtil.genMvcParamName(this.getParameterIndex()))); &#125; else if (MvcFuncUtil.isRequestBodyClass(parameterType)) &#123; annotationList.add(MvcFuncUtil.newRequestBody()); &#125; return annotationList.toArray(new Annotation[]&#123;&#125;); &#125; @Override public Annotation[] getParameterAnnotations() &#123; if (annotations != null &amp;&amp; annotations.length &gt; 0) &#123; return annotations; &#125; return super.getParameterAnnotations(); &#125;&#125; 自定义的 MethodParameter 对象中重写了 getParameterAnnotations 方法，而次方法正是 argumentResolver 用来判断自己是否适合该参数的方法。我们做了些改造使得合适的参数会被合适的参数解析器”误以为”加了对应的注解，从而自己会去进行正常的参数处理逻辑。整个处理流程如下，粉红色部分也正是我们所扩展的点了： RPC 编程模型经过改造之后，我们已经可以实现文章开头所描述的透明 RPC 来开发微服务了，整个运行架构变成了下面这样：","tags":["框架","Spring","SpringMVC","分布式","微服务"],"categories":["框架","Spring","SpringMVC"]},{"title":"图文讲解哈希表","path":"/2023/12/25/图文讲解哈希表/","content":"今天我们来说一种新的数据结构散列（哈希）表，散列是应用非常广泛的数据结构，在我们的刷题过程中，散列表的出场率特别高。所以我们快来一起把散列表的内些事给整明白吧，文章框架如下。 说散列表之前，我们先设想以下场景。 袁厨穿越回了古代，凭借从现代学习的做饭手艺，开了一个袁记菜馆，正值开业初期，店里生意十分火爆，但是顾客结账时就犯难了，由于菜品太多，每当结账时，老板娘总是按照菜单一个一个找价格（遍历查找），每次都要找半天，所以结账的地方总是排起长队，顾客们表示用户体验很差。袁厨一想这不是办法啊，太浪费大家时间了，所以袁厨就先把菜单按照首字母排序（二分查找），然后查找的时候根据首字母查找，这样结账的时候就能大大提高检索效率啦！但是呢？工作日顾客不多，老板娘完全应付的过来，但是每逢节假日，还是会排起长队。那么有没有什么更好的办法呢？对呀！我们把所有的价格都背下来不就可以了吗？每个菜的价格我们都了如指掌，结账的时候我们只需把每道菜的价格相加即可。所以袁厨和老板娘加班加点的进行背诵。下次再结账的时候一说吃了什么菜，我们立马就知道价格啦。自此以后收银台再也没有出现过长队啦，袁记菜馆开着开着一不小心就成了天下第一饭店。 下面我们来看一下袁记菜馆老板娘进化史。 上面的后期结账的过程则模拟了我们的散列表查找，那么在计算机中是如何使用进行查找的呢？ 散列表查找步骤散列表，最有用的基本数据结构之一。是根据关键码的值直接进行访问的数据结构，散列表的实现常常叫做散列（hasing）。散列是一种用于以常数平均时间执行插入、删除和查找的技术，下面我们来看一下散列过程。 我们的整个散列过程主要分为两步 （1）通过散列函数计算记录的散列地址，并按此散列地址存储该记录。就好比麻辣鱼，我们就让它在川菜区，糖醋鱼，我们就让它在鲁菜区。但是我们需要注意的是，无论什么记录我们都需要用同一个散列函数计算地址，然后再存储。 （2)当我们查找时，我们通过同样的散列函数计算记录的散列地址，按此散列地址访问该记录。因为我们存和取的时候用的都是一个散列函数，因此结果肯定相同。 刚才我们在散列过程中提到了散列函数，那么散列函数是什么呢？ 我们假设某个函数为 f，使得 存储位置 &#x3D; f (key) 那样我们就能通过查找关键字不需要比较就可获得需要的记录的存储位置。这种存储技术被称为散列技术。散列技术是在通过记录的存储位置和它的关键字之间建立一个确定的对应关系 f ,使得每个关键字 key 都对应一个存储位置 f(key)。见下图 这里的 f 就是我们所说的散列函数（哈希）函数。我们利用散列技术将记录存储在一块连续的存储空间中，这块连续存储空间就是我们本文的主人公——散列(哈希) 上图为我们描述了用散列函数将关键字映射到散列表，但是大家有没有考虑到这种情况，那就是将关键字映射到同一个槽中的情况，即 f(k4) &#x3D; f(k3) 时。这种情况我们将其称之为冲突，k3 和 k4 则被称之为散列函数 f 的同义词，如果产生这种情况，则会让我们查找错误。幸运的是我们能找到有效的方法解决冲突。 首先我们可以对哈希函数下手，我们可以精心设计哈希函数，让其尽可能少的产生冲突，所以我们创建哈希函数时应遵循以下规则 （1）必须是一致的，假设你输入辣子鸡丁时得到的是在看，那么每次输入辣子鸡丁时，得到的也必须为在看。如果不是这样，散列表将毫无用处。 （2）计算简单，假设我们设计了一个算法，可以保证所有关键字都不会冲突，但是这个算法计算复杂，会耗费很多时间，这样的话就大大降低了查找效率，反而得不偿失。所以咱们散列函数的计算时间不应该超过其他查找技术与关键字的比较时间，不然的话我们干嘛不使用其他查找技术呢? （3）散列地址分布均匀我们刚才说了冲突的带来的问题，所以我们最好的办法就是让散列地址尽量均匀分布在存储空间中，这样即保证空间的有效利用，又减少了处理冲突而消耗的时间。 现在我们已经对散列表，散列函数等知识有所了解啦，那么我们来看几种常用的散列函数构造方法。这些方法的共同点为都是将原来的数字按某种规律变成了另一个数字。所以是很容易理解的。 散列函数构造方法直接定址法如果我们对盈利为0-9的菜品设计哈希表，我们则直接可以根据作为地址，则 f(key) &#x3D; key; 即下面这种情况。 有没有感觉上面的图很熟悉，没错我们经常用的数组其实就是一张哈希表，关键码就是数组的索引下标，然后我们通过下标直接访问数组中的元素。 另外我们假设每道菜的成本为50块，那我们还可以根据盈利+成本来作为地址，那么则 f(key) &#x3D; key + 50。也就是说我们可以根据线性函数值作为散列地址。 f(key) &#x3D; a * key + b a,b均为常数 优点：简单、均匀、无冲突。 应用场景：需要事先知道关键字的分布情况，适合查找表较小且连续的情况 数字分析法该方法也是十分简单的方法，就是分析我们的关键字，取其中一段，或对其位移，叠加，用作地址。比如我们的学号，前 6 位都是一样的，但是后面 3 位都不相同，我们则可以用学号作为键，后面的 3 位做为我们的散列地址。如果我们这样还是容易产生冲突，则可以对抽取数字再进行处理。我们的目的只有一个，提供一个散列函数将关键字合理的分配到散列表的各位置。这里我们提到了一种新的方式抽取，这也是在散列函数中经常用到的手段。 优点：简单、均匀、适用于关键字位数较大的情况 应用场景：关键字位数较大，知道关键字分布情况且关键字的若干位较均匀 折叠法其实这个方法也很简单，也是处理我们的关键字然后用作我们的散列地址，主要思路是将关键字从左到右分割成位数相等的几部分，然后叠加求和，并按散列表表长，取后几位作为散列地址。 比如我们的关键字是123456789，则我们分为三部分 123 ，456 ，789 然后将其相加得 1368 然后我们再取其后三位 368 作为我们的散列地址。 优点：事先不需要知道关键字情况 应用场景：适合关键字位数较多的情况 除法散列法在用来设计散列函数的除法散列法中，通过取 key 除以 p 的余数，将关键字映射到 p 个槽中的某一个上，对于散列表长度为 m 的散列函数公式为 f(k) &#x3D; k mod p (p &lt;&#x3D; m) 例如，如果散列表长度为 12，即 m &#x3D; 12 ，我们的参数 p 也设为12，那 k &#x3D; 100时 f(k) &#x3D; 100 % 12 &#x3D; 4 由于只需要做一次除法操作，所以除法散列法是非常快的。 由上面的公式可以看出，该方法的重点在于 p 的取值，如果 p 值选的不好，就可能会容易产生同义词。见下面这种情况。我们哈希表长度为6，我们选择6为p值，则有可能产生这种情况，所有关键字都得到了0这个地址数。 那我们在选用除法散列法时选取 p 值时应该遵循怎样的规则呢？ m 不应为 2 的幂，因为如果 m &#x3D; 2^p ，则 f(k) 就是 k 的 p 个最低位数字。例 12 % 8 &#x3D; 4 ，12的二进制表示位1100，后三位为100。 若散列表长为 m ,通常 p 为 小于或等于表长（最好接近m）的最小质数或不包含小于 20 质因子的合数。 合数：合数是指在大于1的整数中除了能被1和本身整除外，还能被其他数（0除外）整除的数。 质因子：质因子（或质因数）在数论里是指能整除给定正整数的质数。 注：这里的2，3，5为质因子 还是上面的例子，我们根据规则选择 5 为 p 值，我们再来看。这时我们发现只有 6 和 36 冲突，相对来说就好了很多。 优点：计算效率高，灵活 应用场景：不知道关键字分布情况 乘法散列法构造散列函数的乘法散列法主要包含两个步骤 用关键字 k 乘上常数 A(0 &lt; A &lt; 1)，并提取 k A 的小数部分 用 m 乘以这个值，再向下取整 散列函数为 f (k) &#x3D; ⌊ m(kA mod 1) ⌋ 这里的 kA mod 1 的含义是取 keyA 的小数部分，即 kA - ⌊kA⌋ 。 优点：对 m 的选择不是特别关键一般选择它为 2 的某个幂次（m &#x3D; 2 ^ p ,p为某个整数） 应用场景：不知道关键字情况 平方取中法这个方法就比较简单了，假设关键字是 321，那么他的平方就是 103041，再抽取中间的 3 位就是 030 或 304 用作散列地址。再比如关键字是 1234 那么它的平方就是 1522756 ，抽取中间 3 位就是 227 用作散列地址. 优点：灵活，适用范围广泛 适用场景：不知道关键字分布，而位数又不是很大的情况。 随机数法故名思意，取关键字的随机函数值为它的散列地址。也就是 **f(key) &#x3D; random(key)**。这里的random是随机函数。（具体解析见随机探测法） 适用场景：关键字的长度不等时 上面我们的例子都是通过数字进行举例，那么如果是字符串可不可以作为键呢？当然也是可以的，各种各样的符号我们都可以转换成某种数字来对待，比如我们经常接触的ASCII 码，所以是同样适用的。 以上就是常用的散列函数构造方法，其实他们的中心思想是一致的，将关键字经过加工处理之后变成另外一个数字，而这个数字就是我们的存储位置，是不是有一种间谍传递情报的感觉。 一个好的哈希函数可以帮助我们尽可能少的产生冲突，但是也不能完全避免产生冲突，那么遇到冲突时应该怎么做呢？下面给大家带来几种常用的处理散列冲突的方法。 处理散列冲突的方法我们在使用 hash 函数之后发现关键字 key1 不等于 key2 ，但是 f(key1) &#x3D; f(key2)，即有冲突，那么该怎么办呢？不急我们慢慢往下看。 开放地址法了解开放地址法之前我们先设想以下场景。 袁记菜馆内，铃铃铃，铃铃铃 电话铃响了 大鹏：老袁，给我订个包间，我今天要去带几个客户去你那谈生意。 袁厨：大鹏啊，你常用的那个包间被人订走啦。 大鹏：老袁你这不仗义呀，咋没给我留住呀，那你给我找个空房间吧。 袁厨：好滴老哥 哦，穿越回古代就没有电话啦，那看来穿越的时候得带着几个手机了。 上面的场景其实就是一种处理冲突的方法—–开放地址法 开放地址法就是一旦发生冲突，就去寻找下一个空的散列地址，只要列表足够大，空的散列地址总能找到，并将记录存入，为了使用开放寻址法插入一个元素，需要连续地检查散列表，或称为探查，我们常用的有线性探测，二次探测，随机探测。 线性探测法下面我们先来看一下线性探测，公式： 我们来看一个例子，我们的关键字集合为{12，67，56，16，25，37，22，29，15，47，48，21}，表长为12，我们再用散列函数 f(key) &#x3D; key mod 12。 我们求出每个 key 的 f(key)见下表 我们查看上表发现，前五位的 f(key) 都不相同，即没有冲突，可以直接存入，但是到了第六位 f(37) &#x3D; f(25) &#x3D; 1,那我们就需要利用上面的公式 f(37) &#x3D; f (f(37) + 1 ) mod 12 &#x3D; 2，这其实就是我们的订包间的做法。下面我们看一下将上面的所有数存入哈希表是什么情况吧。 注：蓝色为计算哈希值，红色为存入哈希表 我们把这种解决冲突的开放地址法称为线性探测法。下面我们通过视频来模拟一下线性探测法的存储过程。 &lt;video id&#x3D;”video” controls&#x3D;””src&#x3D;”http://mpvideo.qpic.cn/0bf23udbgaagw4aa2n5tqrpvnxodcpoqmeya.f10002.mp4?dis_k=7098b34ae4d0d564f208a7d543987175&amp;dis_t=1609338638&amp;spec_id=MzU4MDUyMDQyNQ%3D%3D1609338634&amp;vid=wxv_1612509429344567297&amp;format_id=10002“ preload&#x3D;”none”&gt; 另外我们在解决冲突的时候，会遇到 48 和 37 虽然不是同义词，却争夺一个地址的情况，我们称其为堆积。因为堆积使得我们需要不断的处理冲突，插入和查找效率都会大大降低。 通过上面的视频我们应该了解了线性探测的执行过程了，那么我们考虑一下这种情况，若是我们的最后一位不为21，为 34 时会有什么事情发生呢？ 此时他第一次会落在下标为 10 的位置，那么如果继续使用线性探测的话，则需要通过不断取余后得到结果，数据量小还好，要是很大的话那也太慢了吧，但是明明他的前面就有一个空房间呀，如果向前移动只需移动一次即可。不要着急，前辈们已经帮我们想好了解决方法 二次探测法其实理解了我们的上个例子之后，这个一下就能整明白了，根本不用费脑子，这个方法就是更改了一下di的取值 注：这里的是 -1^2 为负值 而不是 （-1)^2 所以对于我们的34来说，当di &#x3D; -1时，就可以找到空位置了。 二次探测法的目的就是为了不让关键字聚集在某一块区域。另外还有一种有趣的方法，位移量采用随机函数计算得到，接着往下看吧. 随机探测法大家看到这是不又有新问题了，刚才我们在散列函数构造规则的第一条中说 （1）必须是一致的，假设你输入辣子鸡丁时得到的是在看，那么每次输入辣子鸡丁时，得到的也必须为在看。如果不是这样，散列表将毫无用处。 咦？怎么又是在看哈哈，那么问题来了，我们使用随机数作为他的偏移量，那么我们查找的时候岂不是查不到了？因为我们 di 是随机生成的呀，这里的随机其实是伪随机数，伪随机数含义为，我们设置随机种子相同，则不断调用随机函数可以生成不会重复的数列，我们在查找时，用同样的随机种子，它每次得到的数列是相同的，那么相同的 di 就能得到相同的散列地址。 随机种子（Random Seed）是计算机专业术语，一种以随机数作为对象的以真随机数（种子）为初始条件的随机数。一般计算机的随机数都是伪随机数，以一个真随机数（种子）作为初始条件，然后用一定的算法不停迭代产生随机数 通过上面的测试是不是一下就秒懂啦，使用相同的随机种子，生成的数列是相同的。所以为什么我们可以使用随机数作为它的偏移量。 下面我们再来看一下其他的函数处理散列冲突的方法 再哈希法这个方法其实也特别简单，利用不同的哈希函数再求得一个哈希地址，直到不出现冲突为止。 f,(key) &#x3D; RH,( key ) (i &#x3D; 1,2,3,4…..k) 这里的RH,就是不同的散列函数，你可以把我们之前说过的那些散列函数都用上，每当发生冲突时就换一个散列函数，相信总有一个能够解决冲突的。这种方法能使关键字不产生聚集，但是代价就是增加了计算时间。是不是很简单啊。 链地址法下面我们再设想以下情景。 袁记菜馆内，铃铃铃，铃铃铃电话铃又响了，那个大鹏又来订房间了。 大鹏：老袁啊，我一会去你那吃个饭，还是上回那个包间 袁厨：大鹏你下回能不能早点说啊，又没人订走了，这回是老王订的 大鹏：老王这个老东西啊，反正也是熟人，你再给我整个桌子，我拼在他后面吧 不好意思啊各位同学，信鸽最近太贵了还没来得及买。上面的情景就是模拟我们的新的处理冲突的方法链地址法。 上面我们都是遇到冲突之后，就换地方。那么我们有没有不换地方的办法呢？那就是我们现在说的链地址法。 还记得我们说过的同义词吗？就是 key 不同 f(key) 相同的情况，我们将这些同义词存储在一个单链表中，这种表叫做同义词子表，散列表中只存储同义词子表的头指针。我们还是用刚才的例子，关键字集合为{12，67，56，16，25，37，22，29，15，47，48，21}，表长为12，我们再用散列函数 f(key) &#x3D; key mod 12。我们用了链地址法之后就再也不存在冲突了，无论有多少冲突，我们只需在同义词子表中添加结点即可。下面我们看下链地址法的存储情况。 链地址法虽然能够不产生冲突，但是也带来了查找时需要遍历单链表的性能消耗，有得必有失嘛。 公共溢出区法下面我们再来看一种新的方法，这回大鹏又要来吃饭了。 袁记菜馆内….. 袁厨：呦，这是什么风把你给刮来了，咋没开你的大奔啊。 大鹏：哎呀妈呀，别那么多废话了，我快饿死了，你快给我找个位置，我要吃点饭。 袁厨：你来的，太不巧了，咱们的店已经满了，你先去旁边的小屋看会电视，等有空了我再叫你。小屋里面还有几个和你一样来晚的，你们一起看吧。 大鹏：电视？看电视？ 上面的情景就是模拟我们的公共溢出区法，这也是很好理解的，你不是冲突吗？那冲突的各位我先给你安排个地方呆着，这样你就有地方住了。我们为所有冲突的关键字建立了一个公共的溢出区来存放。 那么我们怎么进行查找呢？我们首先通过散列函数计算出散列地址后，先于基本表对比，如果不相等再到溢出表去顺序查找。这种解决冲突的方法，对于冲突很少的情况性能还是非常高的。 散列表查找算法(线性探测法)下面我们来看一下散列表查找算法的实现 首先需要定义散列列表的结构以及一些相关常数，其中elem代表散列表数据存储数组，count代表的是当前插入元素个数，size代表哈希表容量，NULLKEY散列表初始值，然后我们如果查找成功就返回索引，如果不存在该元素就返回元素不存在。 我们将哈希表初始化，为数组元素赋初值。 插入操作的具体步骤： （1）通过哈希函数(除法散列法)，将key转化为数组下标 （2）如果该下标中没有元素，则插入，否则说明有冲突，则利用线性探测法处理冲突。详细步骤见注释 查找操作的具体步骤： （1）通过哈希函数（同插入时一样），将key转化成数组下标 （2）通过数组下标找到key值，如果key一致，则查找成功，否则利用线性探测法继续查找。 下面我们来看一下完整代码 散列表性能分析如果没有冲突的话，散列查找是我们查找中效率最高的，时间复杂度为O(1),但是没有冲突的情况是一种理想情况，那么散列查找的平均查找长度取决于哪些方面呢？ 1.散列函数是否均匀我们在上文说到，可以通过设计散列函数减少冲突，但是由于不同的散列函数对一组关键字产生冲突可能性是相同的，因此我们可以不考虑它对平均查找长度的影响。 2.处理冲突的方法相同关键字，相同散列函数，不同处理冲突方式，会使平均查找长度不同，比如我们线性探测有时会堆积，则不如二次探测法好，因为链地址法处理冲突时不会产生任何堆积，因而具有最佳的平均查找性能 3.散列表的装填因子本来想在上文中提到装填因子的，但是后来发现即使没有说明也不影响我们对哈希表的理解，下面我们来看一下装填因子的总结 装填因子 α &#x3D; 填入表中的记录数 &#x2F; 散列表长度 散列因子则代表着散列表的装满程度，表中记录越多，α就越大，产生冲突的概率就越大。我们上面提到的例子中 表的长度为12，填入记录数为6，那么此时的 α &#x3D; 6 &#x2F; 12 &#x3D; 0.5 所以说当我们的 α 比较大时再填入元素那么产生冲突的可能性就非常大了。所以说散列表的平均查找长度取决于装填因子，而不是取决于记录数。所以说我们需要做的就是选择一个合适的装填因子以便将平均查找长度限定在一个范围之内。","tags":["算法","数据结构","哈希表"],"categories":["算法","数据结构"]},{"title":"SQL优化最干货总结","path":"/2023/12/25/SQL优化最干货总结/","content":"前言BATJTMD等大厂的面试难度越来越高，但无论从大厂还是到小公司，一直未变的一个重点就是对SQL优化经验的考察。一提到数据库，先“说一说你对SQL优化的见解吧？”。 SQL优化已经成为衡量程序猿优秀与否的硬性指标，甚至在各大厂招聘岗位职能上都有明码标注，如果是你，在这个问题上能吊打面试官还是会被吊打呢？ 目录 前言 SELECT语句 - 语法顺序： SELECT语句 - 执行顺序： SQL优化策略 一、避免不走索引的场景 二、SELECT语句其他优化 三、增删改 DML 语句优化 四、查询条件优化 五、建表优化 有朋友疑问到，SQL优化真的有这么重要么？如下图所示，SQL优化在提升系统性能中是：（成本最低 &amp;&amp; 优化效果最明显） 的途径。如果你的团队在SQL优化这方面搞得很优秀，对你们整个大型系统可用性方面无疑是一个质的跨越，真的能让你们老板省下不止几沓子钱。 优化成本：硬件&gt;系统配置&gt;数据库表结构&gt;SQL及索引。 优化效果：硬件&lt;系统配置&lt;数据库表结构&lt;SQL及索引。 123456789101112131415String result = &quot;嗯，不错，&quot;; if (&quot;SQL优化经验足&quot;) &#123; if (&quot;熟悉事务锁&quot;) &#123; if (&quot;并发场景处理666&quot;) &#123; if (&quot;会打王者荣耀&quot;) &#123; result += &quot;明天入职&quot; &#125; &#125; &#125;&#125; else &#123; result += &quot;先回去等消息吧&quot;;&#125; Logger.info(&quot;面试官：&quot; + result ); 别看了，上面这是一道送命题。 好了我们言归正传，首先，对于MySQL层优化我一般遵从五个原则： 减少数据访问：设置合理的字段类型，启用压缩，通过索引访问等减少磁盘IO 返回更少的数据：只返回需要的字段和数据分页处理 减少磁盘io及网络io 减少交互次数：批量DML操作，函数存储等减少数据连接次数 减少服务器CPU开销：尽量减少数据库排序操作以及全表查询，减少cpu 内存占用 利用更多资源：使用表分区，可以增加并行操作，更大限度利用cpu资源 总结到SQL优化中，就三点: 最大化利用索引； 尽可能避免全表扫描； 减少无效数据的查询； 理解SQL优化原理 ，首先要搞清楚SQL执行顺序： SELECT语句 - 语法顺序：123456789101. SELECT 2. DISTINCT &lt;select_list&gt;3. FROM &lt;left_table&gt;4. &lt;join_type&gt; JOIN &lt;right_table&gt;5. ON &lt;join_condition&gt;6. WHERE &lt;where_condition&gt;7. GROUP BY &lt;group_by_list&gt;8. HAVING &lt;having_condition&gt;9. ORDER BY &lt;order_by_condition&gt;10.LIMIT &lt;limit_number&gt; SELECT语句 - 执行顺序： FROM&lt;表名&gt; # 选取表，将多个表数据通过笛卡尔积变成一个表。ON&lt;筛选条件&gt; # 对笛卡尔积的虚表进行筛选JOIN &lt;join, left join, right join…&gt;&lt;join表&gt; # 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中WHERE&lt;where条件&gt; # 对上述虚表进行筛选GROUP BY&lt;分组条件&gt; # 分组&lt;SUM()等聚合函数&gt; # 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的HAVING&lt;分组筛选&gt; # 对分组后的结果进行聚合筛选SELECT&lt;返回数据列表&gt; # 返回的单列必须在group by子句中，聚合函数除外DISTINCT# 数据除重ORDER BY&lt;排序条件&gt; # 排序LIMIT&lt;行数限制&gt; SQL优化策略 声明：以下SQL优化策略适用于数据量较大的场景下，如果数据量较小，没必要以此为准，以免画蛇添足。 一、避免不走索引的场景1. 尽量避免在字段开头模糊查询，会导致数据库引擎放弃索引进行全表扫描。如下： 1SELECT * FROM t WHERE username LIKE &#x27;%陈%&#x27; 优化方式：尽量在字段后面使用模糊查询。如下： 1SELECT * FROM t WHERE username LIKE &#x27;陈%&#x27; 如果需求是要在前面使用模糊查询， 使用MySQL内置函数INSTR(str,substr) 来匹配，作用类似于java中的indexOf()，查询字符串出现的角标位置 使用FullText全文索引，用match against 检索 数据量较大的情况，建议引用ElasticSearch、solr，亿级数据量检索速度秒级 当表数据量较少（几千条儿那种），别整花里胡哨的，直接用like ‘%xx%’。 2. 尽量避免使用in 和not in，会导致引擎走全表扫描。如下： 1SELECT * FROM t WHERE id IN (2,3) 优化方式：如果是连续数值，可以用between代替。如下： 1SELECT * FROM t WHERE id BETWEEN 2 AND 3 如果是子查询，可以用exists代替。如下： 1234-- 不走索引select * from A where A.id in (select id from B);-- 走索引select * from A where exists (select * from B where B.id = A.id); 3. 尽量避免使用 or，会导致数据库引擎放弃索引进行全表扫描。如下： 1SELECT * FROM t WHERE id = 1 OR id = 3 优化方式：可以用union代替or。如下： 123SELECT * FROM t WHERE id = 1 UNIONSELECT * FROM t WHERE id = 3 4. 尽量避免进行null值的判断，会导致数据库引擎放弃索引进行全表扫描。如下： 1SELECT * FROM t WHERE score IS NULL 优化方式：可以给字段添加默认值0，对0值进行判断。如下： 1SELECT * FROM t WHERE score = 0 5.尽量避免在where条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描。可以将表达式、函数操作移动到等号右侧。如下： 1234-- 全表扫描SELECT * FROM T WHERE score/10 = 9-- 走索引SELECT * FROM T WHERE score = 10*9 6. 当数据量大时，避免使用where 1&#x3D;1的条件。通常为了方便拼装查询条件，我们会默认使用该条件，数据库引擎会放弃索引进行全表扫描。如下： 1SELECT username, age, sex FROM T WHERE 1=1 优化方式：用代码拼装sql时进行判断，没 where 条件就去掉 where，有where条件就加 and。 7. 查询条件不能用 &lt;&gt; 或者 !&#x3D;使用索引列作为条件进行查询时，需要避免使用&lt;&gt;或者!&#x3D;等判断条件。如确实业务需要，使用到不等于符号，需要在重新评估索引建立，避免在此字段上建立索引，改由查询条件中其他索引字段代替。 8. where条件仅包含复合索引非前置列如下：复合（联合）索引包含key_part1，key_part2，key_part3三列，但SQL语句没有包含索引前置列”key_part1”，按照MySQL联合索引的最左匹配原则，不会走联合索引。 1select col1 from table where key_part2=1 and key_part3=2 9. 隐式类型转换造成不使用索引如下SQL语句由于索引对列类型为varchar，但给定的值为数值，涉及隐式类型转换，造成不能正确走索引。 1select col1 from table where col_varchar=123; 10. order by 条件要与where中条件一致，否则order by不会利用索引进行排序12345-- 不走age索引SELECT * FROM t order by age; -- 走age索引SELECT * FROM t where age &gt; 0 order by age; 对于上面的语句，数据库的处理顺序是： 第一步：根据where条件和统计信息生成执行计划，得到数据。 第二步：将得到的数据排序。当执行处理数据（order by）时，数据库会先查看第一步的执行计划，看order by 的字段是否在执行计划中利用了索引。如果是，则可以利用索引顺序而直接取得已经排好序的数据。如果不是，则重新进行排序操作。 第三步：返回排序后的数据。 当order by 中的字段出现在where条件中时，才会利用索引而不再二次排序，更准确的说，order by 中的字段在执行计划中利用了索引时，不用排序操作。 这个结论不仅对order by有效，对其他需要排序的操作也有效。比如group by 、union 、distinct等。 11. 正确使用hint优化语句MySQL中可以使用hint指定优化器在执行时选择或忽略特定的索引。一般而言，处于版本变更带来的表结构索引变化，更建议避免使用hint，而是通过Analyze table多收集统计信息。但在特定场合下，指定hint可以排除其他索引干扰而指定更优的执行计划。 USE INDEX 在你查询语句中表名的后面，添加 USE INDEX 来提供希望 MySQL 去参考的索引列表，就可以让 MySQL 不再考虑其他可用的索引。例子: SELECT col1 FROM table USE INDEX (mod_time, name)… IGNORE INDEX 如果只是单纯的想让 MySQL 忽略一个或者多个索引，可以使用 IGNORE INDEX 作为 Hint。例子: SELECT col1 FROM table IGNORE INDEX (priority) … FORCE INDEX 为强制 MySQL 使用一个特定的索引，可在查询中使用FORCE INDEX 作为Hint。例子: SELECT col1 FROM table FORCE INDEX (mod_time) … 在查询的时候，数据库系统会自动分析查询语句，并选择一个最合适的索引。但是很多时候，数据库系统的查询优化器并不一定总是能使用最优索引。如果我们知道如何选择索引，可以使用FORCE INDEX强制查询使用指定的索引。 例如： 1SELECT * FROM students FORCE INDEX (idx_class_id) WHERE class_id = 1 ORDER BY id DESC; 二、SELECT语句其他优化**1. 避免出现select **首先，select * 操作在任何类型数据库中都不是一个好的SQL编写习惯。 使用select * 取出全部列，会让优化器无法完成索引覆盖扫描这类优化，会影响优化器对执行计划的选择，也会增加网络带宽消耗，更会带来额外的I&#x2F;O,内存和CPU消耗。 建议提出业务实际需要的列数，将指定列名以取代select *。 2. 避免出现不确定结果的函数特定针对主从复制这类业务场景。由于原理上从库复制的是主库执行的语句，使用如now()、rand()、sysdate()、current_user()等不确定结果的函数很容易导致主库与从库相应的数据不一致。另外不确定值的函数,产生的SQL语句无法利用query cache。 3.多表关联查询时，小表在前，大表在后。在MySQL中，执行 from 后的表关联查询是从左往右执行的（Oracle相反），第一张表会涉及到全表扫描，所以将小表放在前面，先扫小表，扫描快效率较高，在扫描后面的大表，或许只扫描大表的前100行就符合返回条件并return了。 例如：表1有50条数据，表2有30亿条数据；如果全表扫描表2，你品，那就先去吃个饭再说吧是吧。 4. 使用表的别名当在SQL语句中连接多个表时，请使用表的别名并把别名前缀于每个列名上。这样就可以减少解析的时间并减少哪些友列名歧义引起的语法错误。 5. 用where字句替换HAVING字句避免使用HAVING字句，因为HAVING只会在检索出所有记录之后才对结果集进行过滤，而where则是在聚合前刷选记录，如果能通过where字句限制记录的数目，那就能减少这方面的开销。HAVING中的条件一般用于聚合函数的过滤，除此之外，应该将条件写在where字句中。 where和having的区别：where后面不能使用组函数 6.调整Where字句中的连接顺序MySQL采用从左往右，自上而下的顺序解析where子句。根据这个原理，应将过滤数据多的条件往前放，最快速度缩小结果集。 三、增删改 DML 语句优化1. 大批量插入数据如果同时执行大量的插入，建议使用多个值的INSERT语句(方法二)。这比使用分开INSERT语句快（方法一），一般情况下批量插入效率有几倍的差别。 方法一： 12345insert into T values(1,2); insert into T values(1,3); insert into T values(1,4); 方法二： 1Insert into T values(1,2),(1,3),(1,4); 选择后一种方法的原因有三。 减少SQL语句解析的操作，MySQL没有类似Oracle的share pool，采用方法二，只需要解析一次就能进行数据的插入操作； 在特定场景可以减少对DB连接次数 SQL语句较短，可以减少网络传输的IO。 2. 适当使用commit适当使用commit可以释放事务占用的资源而减少消耗，commit后能释放的资源如下： 事务占用的undo数据块； 事务在redo log中记录的数据块； 释放事务施加的，减少锁争用影响性能。特别是在需要使用delete删除大量数据的时候，必须分解删除量并定期commit。 3. 避免重复查询更新的数据针对业务中经常出现的更新行同时又希望获得改行信息的需求，MySQL并不支持PostgreSQL那样的UPDATE RETURNING语法，在MySQL中可以通过变量实现。 例如，更新一行记录的时间戳，同时希望查询当前记录中存放的时间戳是什么，简单方法实现： 123Update t1 set time=now() where col1=1; Select time from t1 where id =1; 使用变量，可以重写为以下方式： 123Update t1 set time=now () where col1=1 and @now: = now (); Select @now; 前后二者都需要两次网络来回，但使用变量避免了再次访问数据表，特别是当t1表数据量较大时，后者比前者快很多。 4.查询优先还是更新（insert、update、delete）优先MySQL 还允许改变语句调度的优先级，它可以使来自多个客户端的查询更好地协作，这样单个客户端就不会由于锁定而等待很长时间。改变优先级还可以确保特定类型的查询被处理得更快。我们首先应该确定应用的类型，判断应用是以查询为主还是以更新为主的，是确保查询效率还是确保更新的效率，决定是查询优先还是更新优先。 下面我们提到的改变调度策略的方法主要是针对只存在表锁的存储引擎，比如 MyISAM 、MEMROY、MERGE，对于Innodb 存储引擎，语句的执行是由获得行锁的顺序决定的。MySQL 的默认的调度策略可用总结如下： 1）写入操作优先于读取操作。 2）对某张数据表的写入操作某一时刻只能发生一次，写入请求按照它们到达的次序来处理。 3）对某张数据表的多个读取操作可以同时地进行。MySQL 提供了几个语句调节符，允许你修改它的调度策略： LOW_PRIORITY关键字应用于DELETE、INSERT、LOAD DATA、REPLACE和UPDATE； HIGH_PRIORITY关键字应用于SELECT和INSERT语句； DELAYED关键字应用于INSERT和REPLACE语句。 如果写入操作是一个 LOW_PRIORITY（低优先级）请求，那么系统就不会认为它的优先级高于读取操作。在这种情况下，如果写入者在等待的时候，第二个读取者到达了，那么就允许第二个读取者插到写入者之前。只有在没有其它的读取者的时候，才允许写入者开始操作。这种调度修改可能存在 LOW_PRIORITY写入操作永远被阻塞的情况。 SELECT 查询的HIGH_PRIORITY（高优先级）关键字也类似。它允许SELECT 插入正在等待的写入操作之前，即使在正常情况下写入操作的优先级更高。另外一种影响是，高优先级的 SELECT 在正常的 SELECT 语句之前执行，因为这些语句会被写入操作阻塞。如果希望所有支持LOW_PRIORITY 选项的语句都默认地按照低优先级来处理，那么 请使用–low-priority-updates 选项来启动服务器。通过使用 INSERTHIGH_PRIORITY 来把 INSERT 语句提高到正常的写入优先级，可以消除该选项对单个INSERT语句的影响。 四、查询条件优化1. 对于复杂的查询，可以使用中间临时表 暂存数据2. 优化group by语句默认情况下，MySQL 会对GROUP BY分组的所有值进行排序，如 “GROUP BY col1，col2，….;” 查询的方法如同在查询中指定 “ORDER BY col1，col2，…;” 如果显式包括一个包含相同的列的 ORDER BY子句，MySQL 可以毫不减速地对它进行优化，尽管仍然进行排序。 因此，如果查询包括 GROUP BY 但你并不想对分组的值进行排序，你可以指定 ORDER BY NULL禁止排序。例如： 1SELECT col1, col2, COUNT(*) FROM table GROUP BY col1, col2 ORDER BY NULL ; 3. 优化join语句MySQL中可以通过子查询来使用 SELECT 语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的 SQL 操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询可以被更有效率的连接(JOIN)..替代。 例子：假设要将所有没有订单记录的用户取出来，可以用下面这个查询完成： 1SELECT col1 FROM customerinfo WHERE CustomerID NOT in (SELECT CustomerID FROM salesinfo ) 如果使用连接(JOIN).. 来完成这个查询工作，速度将会有所提升。尤其是当 salesinfo表中对 CustomerID 建有索引的话，性能将会更好，查询如下： 123SELECT col1 FROM customerinfo LEFT JOIN salesinfoON customerinfo.CustomerID=salesinfo.CustomerID WHERE salesinfo.CustomerID IS NULL 连接(JOIN).. 之所以更有效率一些，是因为 MySQL 不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。 4. 优化union查询MySQL通过创建并填充临时表的方式来执行union查询。除非确实要消除重复的行，否则建议使用union all。原因在于如果没有all这个关键词，MySQL会给临时表加上distinct选项，这会导致对整个临时表的数据做唯一性校验，这样做的消耗相当高。 高效： 12345SELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 UNION ALL SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= &#x27;TEST&#x27;; 低效： 12345SELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 UNION SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= &#x27;TEST&#x27;; 5.拆分复杂SQL为多个小SQL，避免大事务 简单的SQL容易使用到MySQL的QUERY CACHE； 减少锁表时间特别是使用MyISAM存储引擎的表； 可以使用多核CPU。 6. 使用truncate代替delete当删除全表中记录时，使用delete语句的操作会被记录到undo块中，删除记录也记录binlog，当确认需要删除全表时，会产生很大量的binlog并占用大量的undo数据块，此时既没有很好的效率也占用了大量的资源。 使用truncate替代，不会记录可恢复的信息，数据不能被恢复。也因此使用truncate操作有其极少的资源占用与极快的时间。另外，使用truncate可以回收表的水位，使自增字段值归零。 7. 使用合理的分页方式以提高分页效率使用合理的分页方式以提高分页效率 针对展现等分页需求，合适的分页方式能够提高分页的效率。 案例1： 12select * from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15; 上述例子通过一次性根据过滤条件取出所有字段进行排序返回。数据访问开销&#x3D;索引IO+索引全部记录结果对应的表数据IO。因此，该种写法越翻到后面执行效率越差，时间越长，尤其表数据量很大的时候。 适用场景：当中间结果集很小（10000行以下）或者查询条件复杂（指涉及多个不同查询字段或者多表连接）时适用。 案例2： 123select t.* from (select id from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15) a, t where a.id = t.id; 上述例子必须满足t表主键是id列，且有覆盖索引secondary key:(thread_id, deleted, gmt_create)。通过先根据过滤条件利用覆盖索引取出主键id进行排序，再进行join操作取出其他字段。数据访问开销&#x3D;索引IO+索引分页后结果（例子中是15行）对应的表数据IO。因此，该写法每次翻页消耗的资源和时间都基本相同，就像翻第一页一样。 适用场景：当查询和排序字段（即where子句和order by子句涉及的字段）有对应覆盖索引时，且中间结果集很大的情况时适用。 五、建表优化1. 在表中建立索引，优先考虑where、order by使用到的字段。2. 尽量使用数字型字段（如性别，男：1 女：2），若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。 这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 3. 查询数据量大的表 会造成查询缓慢。主要的原因是扫描行数过多。这个时候可以通过程序，分段分页进行查询，循环遍历，将结果合并处理进行展示。要查询100000到100050的数据，如下： 12SELECT * FROM (SELECT ROW_NUMBER() OVER(ORDER BY ID ASC) AS rowid,* FROM infoTab)t WHERE t.rowid &gt; 100000 AND t.rowid &lt;= 100050 4. 用varchar&#x2F;nvarchar 代替 char&#x2F;nchar尽可能的使用 varchar&#x2F;nvarchar 代替 char&#x2F;nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。","tags":["MySQL","DataBase","SQL优化"],"categories":["DataBase"]},{"title":"TCP/IP简述","path":"/2023/12/25/TCP-IP简述/","content":"一、TCP&#x2F;IP模型TCP&#x2F;IP协议模型（Transmission Control Protocol&#x2F;Internet Protocol），包含了一系列构成互联网基础的网络协议，是Internet的核心协议。 基于TCP&#x2F;IP的参考模型将协议分成四个层次，它们分别是链路层、网络层、传输层和应用层。下图表示TCP&#x2F;IP模型与OSI模型各层的对照关系。 TCP&#x2F;IP协议族按照层次由上到下，层层包装。最上面的是应用层，这里面有http，ftp 等等我们熟悉的协议。而第二层则是传输层，著名的TCP和UDP协议就在这个层次。第三层是网络层，IP协议就在这里，它负责对数据加上IP地址和其他的数据以确定传输的目标。第四层是数据链路层，这个层次为待传送的数据加入一个以太网协议头，并进行CRC编码，为最后的数据传输做准备。 上图清楚地表示了TCP&#x2F;IP协议中每个层的作用，而TCP&#x2F;IP协议通信的过程其实就对应着数据入栈与出栈的过程。入栈的过程，数据发送方每层不断地封装首部与尾部，添加一些传输的信息，确保能传输到目的地。出栈的过程，数据接收方每层不断地拆除首部与尾部，得到最终传输的数据。 上图以HTTP协议为例，具体说明。 二、数据链路层物理层负责0、1比特流与物理设备电压高低、光的闪灭之间的互换。数据链路层负责将0、1序列划分为数据帧从一个节点传输到临近的另一个节点,这些节点是通过MAC来唯一标识的(MAC,物理地址，一个主机会有一个MAC地址)。 封装成帧: 把网络层数据报加头和尾，封装成帧,帧头中包括源MAC地址和目的MAC地址。 透明传输: 零比特填充、转义字符。 可靠传输: 在出错率很低的链路上很少用，但是无线链路WLAN会保证可靠传输。 差错检测(CRC): 接收者检测错误,如果发现差错，丢弃该帧。 三、网络层1、IP协议IP协议是TCP&#x2F;IP协议的核心，所有的TCP，UDP，IMCP，IGMP的数据都以IP数据格式传输。要注意的是，IP不是可靠的协议，这是说，IP协议没有提供一种数据未传达以后的处理机制，这被认为是上层协议：TCP或UDP要做的事情。 1.1 IP地址在数据链路层中我们一般通过MAC地址来识别不同的节点，而在IP层我们也要有一个类似的地址标识，这就是IP地址。 32位IP地址分为网络位和地址位，这样做可以减少路由器中路由表记录的数目，有了网络地址，就可以限定拥有相同网络地址的终端都在同一个范围内，那么路由表只需要维护一条这个网络地址的方向，就可以找到相应的这些终端了。 A类IP地址: 0.0.0.0127.0.0.0B类IP地址:128.0.0.1191.255.0.0C类IP地址:192.168.0.0~239.255.255.0 1.2 IP协议头 这里只介绍:八位的TTL字段。这个字段规定该数据包在穿过多少个路由之后才会被抛弃。某个IP数据包每穿过一个路由器，该数据包的TTL数值就会减少1，当该数据包的TTL成为零，它就会被自动抛弃。 这个字段的最大值也就是255，也就是说一个协议包也就在路由器里面穿行255次就会被抛弃了，根据系统的不同，这个数字也不一样，一般是32或者是64。 2、ARP及RARP协议ARP 是根据IP地址获取MAC地址的一种协议。 ARP（地址解析）协议是一种解析协议，本来主机是完全不知道这个IP对应的是哪个主机的哪个接口，当主机要发送一个IP包的时候，会首先查一下自己的ARP高速缓存（就是一个IP-MAC地址对应表缓存）。 如果查询的IP－MAC值对不存在，那么主机就向网络发送一个ARP协议广播包，这个广播包里面就有待查询的IP地址，而直接收到这份广播的包的所有主机都会查询自己的IP地址，如果收到广播包的某一个主机发现自己符合条件，那么就准备好一个包含自己的MAC地址的ARP包传送给发送ARP广播的主机。 而广播主机拿到ARP包后会更新自己的ARP缓存（就是存放IP-MAC对应表的地方）。发送广播的主机就会用新的ARP缓存数据准备好数据链路层的的数据包发送工作。 RARP协议的工作与此相反，不做赘述。 3、ICMP协议IP协议并不是一个可靠的协议，它不保证数据被送达，那么，自然的，保证数据送达的工作应该由其他的模块来完成。其中一个重要的模块就是ICMP(网络控制报文)协议。ICMP不是高层协议，而是IP层的协议。 当传送IP数据包发生错误。比如主机不可达，路由不可达等等，ICMP协议将会把错误信息封包，然后传送回给主机。给主机一个处理错误的机会，这也就是为什么说建立在IP层以上的协议是可能做到安全的原因。 四、pingping可以说是ICMP的最著名的应用，是TCP&#x2F;IP协议的一部分。利用“ping”命令可以检查网络是否连通，可以很好地帮助我们分析和判定网络故障。 例如：当我们某一个网站上不去的时候。通常会ping一下这个网站。ping会回显出一些有用的信息。一般的信息如下: ping这个单词源自声纳定位，而这个程序的作用也确实如此，它利用ICMP协议包来侦测另一个主机是否可达。原理是用类型码为0的ICMP发请求，受到请求的主机则用类型码为8的ICMP回应。 五、TracerouteTraceroute是用来侦测主机到目的主机之间所经路由情况的重要工具，也是最便利的工具。 Traceroute的原理是非常非常的有意思，它收到到目的主机的IP后，首先给目的主机发送一个TTL&#x3D;1的UDP数据包，而经过的第一个路由器收到这个数据包以后，就自动把TTL减1，而TTL变为0以后，路由器就把这个包给抛弃了，并同时产生 一个主机不可达的ICMP数据报给主机。主机收到这个数据报以后再发一个TTL&#x3D;2的UDP数据报给目的主机，然后刺激第二个路由器给主机发ICMP数据 报。如此往复直到到达目的主机。这样，traceroute就拿到了所有的路由器IP。 六、TCP&#x2F;UDPTCP&#x2F;UDP都是是传输层协议，但是两者具有不同的特性，同时也具有不同的应用场景，下面以图表的形式对比分析。 面向报文 面向报文的传输方式是应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则IP层需要分片，降低效率。若太短，会是IP太小。 面向字节流 面向字节流的话，虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序看成是一连串的无结构的字节流。TCP有一个缓冲，当应用程序传送的数据块太长，TCP就可以把它划分短一些再传送。 关于拥塞控制，流量控制，是TCP的重点，后面讲解。 TCP和UDP协议的一些应用 什么时候应该使用TCP？ 当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。 什么时候应该使用UDP？ 当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。 七、DNSDNS（Domain Name System，域名系统），因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。DNS协议运行在UDP协议之上，使用端口号53。 八、TCP连接的建立与终止1、三次握手TCP是面向连接的，无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。在TCP&#x2F;IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的是同步连接双方的序列号和确认号并交换 TCP窗口大小信息。 第一次握手：建立连接。客户端发送连接请求报文段，将SYN位置为1，Sequence Number为x；然后，客户端进入SYN_SEND状态，等待服务器的确认； 第二次握手：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。 为什么要三次握手？ 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。 具体例子：“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。 于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 2、四次挥手当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，肯定是要断开TCP连接的啊。那对于TCP的断开连接，这里就有了神秘的“四次分手”。 第一次分手：主机1（可以使客户端，也可以是服务器端），设置Sequence Number，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了； 第二次分手：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，Acknowledgment Number为Sequence Number加1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求； 第三次分手：主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态； 第四次分手：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。 为什么要四次分手？ TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。 为什么要等待2MSL？ MSL：报文段最大生存时间，它是任何报文段被丢弃前在网络内的最长时间。原因有二： 保证TCP协议的全双工连接能够可靠关闭 保证这次连接的重复数据段从网络中消失 第一点：如果主机1直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致主机2没有收到主机1最后回复的ACK。那么主机2就会在超时之后继续发送FIN，此时由于主机1已经CLOSED了，就找不到与重发的FIN对应的连接。所以，主机1不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。 第二点：如果主机1直接CLOSED，然后又再向主机2发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达主机2，由于新连接和老连接的端口号是一样的，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。 九、TCP流量控制如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 利用滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制。 设A向B发送数据。在连接建立时，B告诉了A：“我的接收窗口是 rwnd &#x3D; 400 ”(这里的 rwnd 表示 receiver window) 。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。请注意，TCP的窗口单位是字节，不是报文段。假设每一个报文段为100字节长，而数据报文段序号的初始值设为1。大写ACK表示首部中的确认位ACK，小写ack表示确认字段的值ack。 从图中可以看出，B进行了三次流量控制。第一次把窗口减少到 rwnd &#x3D; 300 ，第二次又减到了 rwnd &#x3D; 100 ，最后减到 rwnd &#x3D; 0 ，即不允许发送方再发送数据了。这种使发送方暂停发送的状态将持续到主机B重新发出一个新的窗口值为止。B向A发送的三个报文段都设置了 ACK &#x3D; 1 ，只有在ACK&#x3D;1时确认号字段才有意义。 TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器。 十、TCP拥塞控制发送方维持一个拥塞窗口 cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口。 发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。 慢开始算法： 当主机开始发送数据时，如果立即所大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。因此，较好的方法是 先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。 通常在刚刚开始发送报文段时，先把拥塞窗口 cwnd 设置为一个最大报文段MSS的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口 cwnd ，可以使分组注入到网络的速率更加合理。 每经过一个传输轮次，拥塞窗口 cwnd 就加倍。一个传输轮次所经历的时间其实就是往返时间RTT。不过“传输轮次”更加强调：把拥塞窗口cwnd所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认。 另，慢开始的“慢”并不是指cwnd的增长速率慢，而是指在TCP开始发送报文段时先设置cwnd&#x3D;1，使得发送方在开始时只发送一个报文段（目的是试探一下网络的拥塞情况），然后再逐渐增大cwnd。 为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量。慢开始门限ssthresh的用法如下： 当 cwnd &lt; ssthresh 时，使用上述的慢开始算法。 当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 当 cwnd &#x3D; ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。拥塞避免 拥塞避免 让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送 方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。 这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生 拥塞的路由器有足够时间把队列中积压的分组处理完毕。 如下图，用具体数值说明了上述拥塞控制的过程。现在发送窗口的大小和拥塞窗口一样大。 2、快重传和快恢复快重传快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认。 接收方收到了M1和M2后都分别发出了确认。现在假定接收方没有收到M3但接着收到了M4。 显然，接收方不能确认M4，因为M4是收到的失序报文段。根据 可靠传输原理，接收方可以什么都不做，也可以在适当时机发送一次对M2的确认。 但按照快重传算法的规定，接收方应及时发送对M2的重复确认，这样做可以让 发送方及早知道报文段M3没有到达接收方。发送方接着发送了M5和M6。接收方收到这两个报文后，也还要再次发出对M2的重复确认。这样，发送方共收到了 接收方的四个对M2的确认，其中后三个都是重复确认。 快重传算法还规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段M3，而不必 继续等待M3设置的重传计时器到期。 由于发送方尽早重传未被确认的报文段，因此采用快重传后可以使整个网络吞吐量提高约20%。 快恢复与快重传配合使用的还有快恢复算法，其过程有以下两个要点： 当发送方连续收到三个重复确认，就执行“乘法减小”算法，把慢开始门限ssthresh减半。 与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为 慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。","tags":["计算机网络","TCP/IP"],"categories":["计算机网络"]},{"title":"Java 泛型 T，E，K，V，?，傻傻分不清？","path":"/2023/12/24/Java-泛型-T，E，K，V，-，傻傻分不清？/","content":"前言Java 泛型（generics）是 JDK 5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许开发者在编译时检测到非法的类型。 泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。 泛型带来的好处在没有泛型的情况的下，通过对类型 Object 的引用来实现参数的“任意化”，“任意化”带来的缺点是要做显式的强制类型转换，而这种转换是要求开发者对实际参数类型可以预知的情况下进行的。对于强制类型转换错误的情况，编译器可能不提示错误，在运行的时候才出现异常，这是本身就是一个安全隐患。 那么泛型的好处就是在编译的时候能够检查类型安全，并且所有的强制转换都是自动和隐式的。 12345678910111213141516171819202122232425262728293031323334353637public class GlmapperGeneric&lt;T&gt; &#123; private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125; public static void main(String[] args) &#123; // do nothing &#125; /** * 不指定类型 */ public void noSpecifyType() &#123; GlmapperGeneric glmapperGeneric = new GlmapperGeneric(); glmapperGeneric.set(&quot;test&quot;); // 需要强制类型转换 String test = (String) glmapperGeneric.get(); System.out.println(test); &#125; /** * 指定类型 */ public void specifyType() &#123; GlmapperGeneric&lt;String&gt; glmapperGeneric = new GlmapperGeneric(); glmapperGeneric.set(&quot;test&quot;); // 不需要强制类型转换 String test = glmapperGeneric.get(); System.out.println(test); &#125;&#125; 上面这段代码中的 specifyType 方法中 省去了强制转换，可以在编译时候检查类型安全，可以用在类，方法，接口上。 泛型中通配符我们在定义泛型类，泛型方法，泛型接口的时候经常会碰见很多不同的通配符，比如 T，E，K，V 等等，这些通配符又都是什么意思呢？ 常用的 T，E，K，V，？本质上这些个都是通配符，没啥区别，只不过是编码时的一种约定俗成的东西。比如上述代码中的 T ，我们可以换成 A-Z 之间的任何一个 字母都可以，并不会影响程序的正常运行，但是如果换成其他的字母代替 T ，在可读性上可能会弱一些。通常情况下，T，E，K，V，？是这样约定的： ？表示不确定的 java 类型 T (type) 表示具体的一个java类型 K V (key value) 分别代表java键值中的Key Value E (element) 代表Element ？无界通配符先从一个小例子看起 。 我有一个父类 Animal 和几个子类，如狗、猫等，现在我需要一个动物的列表，我的第一个想法是像这样的： 1List&lt;Animal&gt; listAnimals 但是老板的想法确实这样的： 1List&lt;? extends Animal&gt; listAnimals 为什么要使用通配符而不是简单的泛型呢？通配符其实在声明局部变量时是没有什么意义的，但是当你为一个方法声明一个参数时，它是非常重要的。 123456789101112131415161718192021static int countLegs (List&lt;? extends Animal &gt; animals ) &#123; int retVal = 0; for ( Animal animal : animals ) &#123; retVal += animal.countLegs(); &#125; return retVal;&#125;static int countLegs1 (List&lt; Animal &gt; animals ) &#123; int retVal = 0; for ( Animal animal : animals ) &#123; retVal += animal.countLegs(); &#125; return retVal;&#125;public static void main(String[] args) &#123; List&lt;Dog&gt; dogs = new ArrayList&lt;&gt;(); // 不会报错 countLegs( dogs ); // 报错 countLegs1(dogs);&#125; 当调用 countLegs1 时，就会飘红，提示的错误信息如下： 所以，对于不确定或者不关心实际要操作的类型，可以使用无限制通配符（尖括号里一个问号，即 &lt;?&gt; ），表示可以持有任何类型。像 countLegs 方法中，限定了上届，但是不关心具体类型是什么，所以对于传入的 Animal 的所有子类都可以支持，并且不会报错。而 countLegs1 就不行。 上界通配符 &lt; ? extends E&gt; 上界：用 extends 关键字声明，表示参数化的类型可能是所指定的类型，或者是此类型的子类。 在类型参数中使用 extends 表示这个泛型中的参数必须是 E 或者 E 的子类，这样有两个好处： 如果传入的类型不是 E 或者 E 的子类，编译不成功 泛型中可以使用 E 的方法，要不然还得强转成 E 才能使用 12345private &lt;K extends A, E extends B&gt; E test(K arg1, E arg2) &#123; E result = arg2; arg2.compareTo(arg1); //..... return result;&#125; 类型参数列表中如果有多个类型参数上限，用逗号分开 下界通配符 &lt; ? super E&gt; 下界: 用 super 进行声明，表示参数化的类型可能是所指定的类型，或者是此类型的父类型，直至 Object 在类型参数中使用 super 表示这个泛型中的参数必须是 E 或者 E 的父类。 1234567891011121314private &lt;T&gt; void test(List&lt;? super T&gt; dst, List&lt;T&gt; src) &#123; for (T t : src) &#123; dst.add(t); &#125;&#125;public static void main(String[] args) &#123; List&lt;Dog&gt; dogs = new ArrayList&lt;&gt;(); List&lt;Animal&gt; animals = new ArrayList&lt;&gt;(); new Test3().test(animals,dogs);&#125;// Dog 是 Animal 的子类class Dog extends Animal &#123;&#125; dst 类型 “大于等于” src 的类型，这里的“大于等于”是指 dst 表示的范围比 src 要大，因此装得下 dst 的容器也就能装 src 。 ？和 T 的区别 ？和 T 都表示不确定的类型，区别在于我们可以对 T 进行操作，但是对 ？不行，比如如下这种 ： 1234// 可以T t = operate();// 不可以？car = operate(); 简单总结下： T 是一个 确定的 类型，通常用于泛型类和泛型方法的定义，？是一个 不确定 的类型，通常用于泛型方法的调用代码和形参，不能用于定义类和泛型方法。 区别1：通过 T 来 确保 泛型参数的一致性12345// 通过 T 来 确保 泛型参数的一致性public &lt;T extends Number&gt; voidtest(List&lt;T&gt; dest, List&lt;T&gt; src)//通配符是 不确定的，所以这个方法不能保证两个 List 具有相同的元素类型public void test(List&lt;? extends Number&gt; dest, List&lt;? extends Number&gt; src) 像下面的代码中，约定的 T 是 Number 的子类才可以，但是申明时是用的 String ，所以就会飘红报错。 不能保证两个 List 具有相同的元素类型的情况 1234GlmapperGeneric&lt;String&gt; glmapperGeneric = new GlmapperGeneric&lt;&gt;();List&lt;String&gt; dest = new ArrayList&lt;&gt;();List&lt;Number&gt; src = new ArrayList&lt;&gt;();glmapperGeneric.testNon(dest,src); 上面的代码在编译器并不会报错，但是当进入到 testNon 方法内部操作时（比如赋值），对于 dest 和 src 而言，就还是需要进行类型转换。 区别2：类型参数可以多重限定而通配符不行 使用 &amp; 符号设定多重边界（Multi Bounds)，指定泛型类型 T 必须是 MultiLimitInterfaceA 和 MultiLimitInterfaceB 的共有子类型，此时变量 t 就具有了所有限定的方法和属性。对于通配符来说，因为它不是一个确定的类型，所以不能进行多重限定。 区别3：通配符可以使用超类限定而类型参数不行类型参数 T 只具有 一种 类型限定方式： 1T extends A 但是通配符 ? 可以进行 两种限定： 1? extends A? super A ** Class 和 Class&lt;?&gt; 区别**前面介绍了 ？和 T 的区别，那么对于，Class 和 &lt;Class 又有什么区别呢？Class 和 Class 最常见的是在反射场景下的使用，这里以用一段发射的代码来说明下。 123// 通过反射的方式生成 multiLimit // 对象，这里比较明显的是，我们需要使用强制类型转换MultiLimit multiLimit = (MultiLimit)Class.forName(&quot;com.glmapper.bridge.boot.generic.MultiLimit&quot;).newInstance(); 对于上述代码，在运行期，如果反射的类型不是 MultiLimit 类，那么一定会报 java.lang.ClassCastException 错误。 对于这种情况，则可以使用下面的代码来代替，使得在在编译期就能直接 检查到类型的问题： Class 在实例化的时候，T 要替换成具体类。Class&lt;?&gt; 它是个通配泛型，? 可以代表任何类型，所以主要用于声明时的限制情况。比如，我们可以这样做申明： 1234// 可以public Class&lt;?&gt; clazz;// 不可以，因为 T 需要指定类型public Class&lt;T&gt; clazzT; 所以当不知道定声明什么类型的 Class 的时候可以定义一 个Class&lt;?&gt;。 那如果也想 public Class clazzT; 这样的话，就必须让当前的类也指定 T ， 123public class Test3&lt;T&gt; &#123; public Class&lt;?&gt; clazz; // 不会报错 public Class&lt;T&gt; clazzT; 小结本文零碎整理了下 JAVA 泛型中的一些点，不是很全，仅供参考。","tags":["Java","泛型"],"categories":["Java"]},{"title":"Docker从入门到干活，看这一篇足矣","path":"/2023/12/24/Docker从入门到干活，看这一篇足矣/","content":"1. 容器简介1.1. 什么是 Linux 容器Linux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。 容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。 更加详细地来说，请您假定您在开发一个应用。您使用的是一台笔记本电脑，而且您的开发环境具有特定的配置。其他开发人员身处的环境配置可能稍有不同。您正在开发的应用依赖于您当前的配置，还要依赖于某些特定文件。 与此同时，您的企业还拥有标准化的测试和生产环境，且具有自身的配置和一系列支持文件。 您希望尽可能多在本地模拟这些环境，而不产生重新创建服务器环境的开销。 因此，您要如何确保应用能够在这些环境中运行和通过质量检测，并且在部署过程中不出现令人头疼的问题，也无需重新编写代码和进行故障修复？答案就是使用容器。 容器可以确保您的应用拥有必需的配置和文件，使得这些应用能够在从开发到测试、再到生产的整个流程中顺利运行，而不出现任何不良问题。这样可以避免危机，做到皆大欢喜。 虽然这只是简化的示例，但在需要很高的可移植性、可配置性和隔离的情况下，我们可以利用 Linux 容器通过很多方式解决难题。 无论基础架构是在企业内部还是在云端，或者混合使用两者，容器都能满足您的需求。 1.2. 容器不就是虚拟化吗是，但也不竟然。我们用一种简单方式来思考一下： 虚拟化使得许多操作系统可同时在单个系统上运行。 容器则可共享同一个操作系统内核，将应用进程与系统其他部分隔离开。 图 - 普通虚拟化技术和Docker的对比 这意味着什么？首先，让多个操作系统在单个虚拟机监控程序上运行以实现虚拟化，并不能达成和使用容器同等的轻量级效果。 事实上，在仅拥有容量有限的有限资源时，您需要能够可以进行密集部署的轻量级应用。 Linux 容器可从单个操作系统运行，在所有容器中共享该操作系统，因此应用和服务能够保持轻量级，并行快速运行。 1.3. 容器发展简史 我们现在称为容器技术的概念最初出现在 2000 年，当时称为 FreeBSD jail，这种技术可将 FreeBSD 系统分区为多个子系统（也称为 Jail）。 Jail 是作为安全环境而开发的，系统管理员可与企业内部或外部的多个用户共享这些 Jail。 Jail 的目的是让进程在经过修改的 chroot 环境中创建，而不会脱离和影响整个系统 — 在 chroot 环境中，对文件系统、网络和用户的访问都实现了虚拟化。 尽管 Jail 在实施方面存在局限性，但最终人们找到了脱离这种隔离环境的方法。 但这个概念非常有吸引力。 2001 年，通过 Jacques Gélinas 的 VServer 项目，隔离环境的实施进入了 Linux 领域。 正如 Gélinas 所说，这项工作的目的是“在高度独立且安全的单一环境中运行多个通用 Linux 服务器 [sic]。” 在完成了这项针对 Linux 中多个受控制用户空间的基础性工作后，Linux 容器开始逐渐成形并最终发展成了现在的模样。 2. 什么是 Docker？“Docker” 一词指代多种事物，包括开源社区项目、开源项目使用的工具、主导支持此类项目的公司 Docker Inc. 以及该公司官方支持的工具。技术产品和公司使用同一名称，的确让人有点困惑。 🎍 IT 软件中所说的 “Docker” ，是指容器化技术，用于支持创建和使用 Linux 容器。 🎍 开源 Docker 社区致力于改进这类技术，并免费提供给所有用户，使之获益。 🎍 Docker Inc. 公司凭借 Docker 社区产品起家，它主要负责提升社区版本的安全性，并将改进后的版本与更广泛的技术社区分享。此外，它还专门对这些技术产品进行完善和安全固化，以服务于企业客户。 借助 Docker ，您可将容器当做重量轻、模块化的虚拟机使用。同时，您还将获得高度的灵活性，从而实现对容器的高效创建、部署及复制，并能将其从一个环境顺利迁移至另一个环境。 2.1. Docker 如何工作？Docker 技术使用 Linux 内核和内核功能（例如 Cgroups 和 namespaces）来分隔进程，以便各进程相互独立运行。 这种独立性正是采用容器的目的所在；它可以独立运行多种进程、多个应用程序，更加充分地发挥基础设施的作用，同时保持各个独立系统的安全性。 容器工具（包括 Docker）可提供基于镜像的部署模式。这使得它能够轻松跨多种环境，与其依赖程序共享应用或服务组。Docker 还可在这一容器环境中自动部署应用程序（或者合并多种流程，以构建单个应用程序）。 此外，由于这些工具基于 Linux 容器构建，使得 Docker 既易于使用，又别具一格 —— 它可为用户提供前所未有的高度应用程访问权限、快速部署以及版本控制和分发能力。 2.2. Docker 技术是否与传统的 Linux 容器相同？否。Docker 技术最初是基于 LXC 技术构建（大多数人都会将这一技术与“传统的” Linux 容器联系在一起），但后来它逐渐摆脱了对这种技术的依赖。 就轻量级 虚拟化 这一功能来看，LXC 非常有用，但它无法提供出色的开发人员或用户体验。除了运行容器之外，Docker 技术还具备其他多项功能，包括简化用于构建容器、传输镜像以及控制镜像版本的流程。 传统的 Linux 容器使用 init 系统来管理多种进程。这意味着，所有应用程序都作为一个整体运行。与此相反，Docker 技术鼓励应用程序各自独立运行其进程，并提供相应工具以实现这一功能。这种精细化运作模式自有其优势。 2.3. docker的目标docker的主要目标是”Build,Ship and Run any App,Angwhere”,构建，运输，处处运行 构建：做一个docker镜像 运输：docker pull 运行：启动一个容器 每一个容器，他都有自己的文件系统rootfs. 3. 安装Docker环境说明 123456789# 需要两台几点进行安装[root@docker01 ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@docker01 ~]# uname -r 3.10.0-327.el7.x86_64[root@docker01 ~]# hostname -I10.0.0.100 172.16.1.100 [root@docker02 ~]# hostname -I10.0.0.101 172.16.1.101 在两个节点上都进行操作 123wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.reposed -i &#x27;s#download.docker.com#mirrors.ustc.edu.cn/docker-ce#g&#x27; /etc/yum.repos.d/docker-ce.repoyum install docker-ce -y 修改在docker01配置： 1234567# 修改启动文件，监听远程端口vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://10.0.0.100:2375systemctl daemon-reloadsystemctl enable docker.service systemctl restart docker.service# ps -ef检查进行，是否启动 在docker02测试 123456789[root@docker02 ~]# docker -H 10.0.0.100 infoContainers: 0 Running: 0 Paused: 0 Stopped: 0Images: 0Server Version: 17.12.0-ceStorage Driver: devicemapper··· 3.1. Docker基础命令操作查看docker相关信息 1234567891011121314151617[root@docker01 ~]# docker version Client: Version: 17.12.0-ce API version: 1.35 Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:10:14 2017 OS/Arch: linux/amd64Server: Engine: Version: 17.12.0-ce API version: 1.35 (minimum version 1.12) Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:12:46 2017 OS/Arch: linux/amd64 Experimental: false 配置docker镜像加速 1234vi /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125; 3.2. 启动第一个容器12345678910[root@docker01 ~]# docker run -d -p 80:80 nginxUnable to find image &#x27;nginx:latest&#x27; locallylatest: Pulling from library/nginxe7bb522d92ff: Pull complete 6edc05228666: Pull complete cd866a17e81f: Pull complete Digest: sha256:285b49d42c703fdf257d1e2422765c4ba9d3e37768d6ea83d7fe2043dad6e63dStatus: Downloaded newer image for nginx:latest8d8f81da12b5c10af6ba1a5d07f4abc041cb95b01f3d632c3d638922800b0b4d# 容器启动后，在浏览器进行访问测试 参数说明 3.3. Docker镜像生命周期 4. Docker镜像相关操作4.1. 搜索官方仓库镜像1234[root@docker01 ~]# docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 3992 [OK] ansible/centos7-ansible Ansible on Centos7 105 [OK] 列表说明 4.2. 获取镜像根据镜像名称拉取镜像 1234[root@docker01 ~]# docker pull centosUsing default tag: latestlatest: Pulling from library/centosaf4b0a2388c6: Downloading 34.65MB/73.67MB 查看当前主机镜像列表 1234[root@docker01 ~]# docker image list REPOSITORY TAG IMAGE ID CREATED SIZEcentos latest ff426288ea90 3 weeks ago 207MBnginx latest 3f8a4339aadd 5 weeks ago 108MB 拉第三方镜像方法 1docker pull index.tenxcloud.com/tenxcloud/httpd 4.3. 导出镜像123456[root@docker01 ~]# docker image list REPOSITORY TAG IMAGE ID CREATED SIZEcentos latest ff426288ea90 3 weeks ago 207MBnginx latest 3f8a4339aadd 5 weeks ago 108MB# 导出[root@docker01 ~]# docker image save centos &gt; docker-centos.tar.gz 4.4. 删除镜像1234[root@docker01 ~]# docker image rm centos:latest[root@docker01 ~]# docker image list REPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 3f8a4339aadd 5 weeks ago 108MB 4.5. 导入镜像1234567[root@docker01 ~]# docker image load -i docker-centos.tar.gz e15afa4858b6: Loading layer 215.8MB/215.8MBLoaded image: centos:latest[root@docker01 ~]# docker image list REPOSITORY TAG IMAGE ID CREATED SIZEcentos latest ff426288ea90 3 weeks ago 207MBnginx latest 3f8a4339aadd 5 weeks ago 108MB 4.6. 查看镜像的详细信息1[root@docker01 ~]# docker image inspect centos 5. 容器的日常管理5.1. 容器的起&#x2F;停最简单的运行一个容器 1[root@docker01 ~]# docker run nginx 创建容器，两步走（不常用） 1234[root@docker01 ~]# docker create centos:latest /bin/bashbb7f32368ecf0492adb59e20032ab2e6cf6a563a0e6751e58930ee5f7aaef204[root@docker01 ~]# docker start stupefied_nobelstupefied_nobel 快速启动容器方法 1[root@docker01 ~]# docker run centos:latest /usr/bin/sleep 20; 容器内的第一个进程必须一直处于运行的状态，否则这个容器，就会处于退出状态！ 查看正在运行的容器 12345[root@docker01 ~]# docker container ls 或[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8708e93fd767 nginx &quot;nginx -g &#x27;daemon of…&quot; 6 seconds ago Up 4 seconds 80/tcp keen_lewin 查看你容器详细信息&#x2F;ip 1[root@docker01 ~]# docker container inspect 容器名称/id 查看你所有容器（包括未运行的） 12345[root@docker01 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8708e93fd767 nginx &quot;nginx -g &#x27;daemon of…&quot; 4 minutes ago Exited (0) 59 seconds ago keen_lewinf9f3e6af7508 nginx &quot;nginx -g &#x27;daemon of…&quot; 5 minutes ago Exited (0) 5 minutes ago optimistic_haibt8d8f81da12b5 nginx &quot;nginx -g &#x27;daemon of…&quot; 3 hours ago Exited (0) 3 hours ago lucid_bohr 停止容器 123[root@docker01 ~]# docker stop 容器名称/id 或[root@docker01 ~]# docker container kill 容器名称/id 5.2. 进入容器方法启动时进去方法 123[root@docker01 ~]# docker run -it #参数：-it 可交互终端[root@docker01 ~]# docker run -it nginx:latest /bin/bashroot@79241093859e:/# 退出&#x2F;离开容器 1ctrl+p &amp; ctrl+q 启动后进入容器的方法 启动一个docker 12345[root@docker01 ~]# docker run -it centos:latest [root@1bf0f43c4d2f /]# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 15:47 pts/0 00:00:00 /bin/bashroot 13 1 0 15:47 pts/0 00:00:00 ps -ef attach进入容器，使用pts&#x2F;0 ，会让所用通过此方法进如放入用户看到同样的操作。 12345[root@docker01 ~]# docker attach 1bf0f43c4d2f[root@1bf0f43c4d2f /]# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 15:47 pts/0 00:00:00 /bin/bashroot 14 1 0 15:49 pts/0 00:00:00 ps -ef 自命名启动一个容器 –name 12345[root@docker01 ~]# docker attach 1bf0f43c4d2f[root@1bf0f43c4d2f /]# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 15:47 pts/0 00:00:00 /bin/bashroot 14 1 0 15:49 pts/0 00:00:00 ps -ef exec 进入容器方法（推荐使用） 1234567[root@docker01 ~]# docker exec -it clsn1 /bin/bash [root@b20fa75b4b40 /]# 重新分配一个终端[root@b20fa75b4b40 /]# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 16:11 pts/0 00:00:00 /bin/bashroot 13 0 0 16:14 pts/1 00:00:00 /bin/bashroot 26 13 0 16:14 pts/1 00:00:00 ps -ef 5.3. 删除所有容器12[root@docker01 ~]# docker rm -f `docker ps -a -q`# -f 强制删除 5.4. 启动时进行端口映射-p参数端口映射 12[root@docker01 ~]# docker run -d -p 8888:80 nginx:latest 287bec5c60263166c03e1fc5b0b8262fe76507be3dfae4ce5cd2ee2d1e8a89a9 不同指定映射方法 随机映射 1docker run -P （大P）# 需要镜像支持 6. Docker 数据卷的管理6.1. 挂载时创建卷挂载卷 12[root@docker01 ~]# docker run -d -p 80:80 -v /data:/usr/share/nginx/html nginx:latest079786c1e297b5c5031e7a841160c74e91d4ad06516505043c60dbb78a259d09 容器内站点目录: &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 在宿主机写入数据，查看 123[root@docker01 ~]# echo &quot;http://www.nmtui.com&quot; &gt;/data/index.html[root@docker01 ~]# curl 10.0.0.100http://www.nmtui.com 设置共享卷，使用同一个卷启动一个新的容器 1234[root@docker01 ~]# docker run -d -p 8080:80 -v /data:/usr/share/nginx/html nginx:latest 351f0bd78d273604bd0971b186979aa0f3cbf45247274493d2490527babb4e42[root@docker01 ~]# curl 10.0.0.100:8080http://www.nmtui.com 查看卷列表 12[root@docker01 ~]# docker volume lsDRIVER VOLUME NAME 6.2. 创建卷后挂载创建一个卷 12345[root@docker01 ~]# docker volume create f3b95f7bd17da220e63d4e70850b8d7fb3e20f8ad02043423a39fdd072b83521[root@docker01 ~]# docker volume ls DRIVER VOLUME NAMElocal f3b95f7bd17da220e63d4e70850b8d7fb3e20f8ad02043423a39fdd072b83521 指定卷名 1234[root@docker01 ~]# docker volume ls DRIVER VOLUME NAMElocal clsnlocal f3b95f7bd17da220e63d4e70850b8d7fb3e20f8ad02043423a39fdd072b83521 查看卷路径 123456789101112[root@docker01 ~]# docker volume inspect clsn [ &#123; &quot;CreatedAt&quot;: &quot;2018-02-01T00:39:25+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/clsn/_data&quot;, &quot;Name&quot;: &quot;clsn&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;] 使用卷创建 123456[root@docker01 ~]# docker run -d -p 9000:80 -v clsn:/usr/share/nginx/html nginx:latest 1434559cff996162da7ce71820ed8f5937fb7c02113bbc84e965845c219d3503# 宿主机测试[root@docker01 ~]# echo &#x27;blog.nmtui.com&#x27; &gt;/var/lib/docker/volumes/clsn/_data/index.html [root@docker01 ~]# curl 10.0.0.100:9000blog.nmtui.com 设置卷 12[root@docker01 ~]# docker run -d -P --volumes-from 079786c1e297 nginx:latest b54b9c9930b417ab3257c6e4a8280b54fae57043c0b76b9dc60b4788e92369fb 查看使用的端口 123456789101112[root@docker01 ~]# netstat -lntup Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1400/sshd tcp 0 0 10.0.0.100:2375 0.0.0.0:* LISTEN 26218/dockerd tcp6 0 0 :::9000 :::* LISTEN 32015/docker-proxy tcp6 0 0 :::8080 :::* LISTEN 31853/docker-proxy tcp6 0 0 :::80 :::* LISTEN 31752/docker-proxy tcp6 0 0 :::22 :::* LISTEN 1400/sshd tcp6 0 0 :::32769 :::* LISTEN 32300/docker-proxy [root@docker01 ~]# curl 10.0.0.100:32769http://www.nmtui.com 6.3. 手动将容器保存为镜像本次是基于docker官方centos 6.8 镜像创建 官方镜像列表： https://hub.docker.com/explore/ 启动一个centos6.8的镜像 123456[root@docker01 ~]# docker pull centos:6.8[root@docker01 ~]# docker run -it -p 1022:22 centos:6.8 /bin/bash# 在容器种安装sshd服务，并修改系统密码[root@582051b2b92b ~]# yum install openssh-server -y [root@582051b2b92b ~]# echo &quot;root:123456&quot; |chpasswd[root@582051b2b92b ~]# /etc/init.d/sshd start 启动完成后镜像ssh连接测试 将容器提交为镜像 1[root@docker01 ~]# docker commit brave_mcclintock centos6-ssh 使用新的镜像启动容器 12[root@docker01 ~]# docker run -d -p 1122:22 centos6-ssh:latest /usr/sbin/sshd -D 5b8161fda2a9f2c39c196c67e2eb9274977e7723fe51c4f08a0190217ae93094 在容器安装httpd服务 1[root@5b8161fda2a9 /]# yum install httpd -y 编写启动脚本脚本 123456[root@5b8161fda2a9 /]# cat init.sh #!/bin/bash /etc/init.d/httpd start /usr/sbin/sshd -D[root@5b8161fda2a9 /]# chmod +x init.sh # 注意执行权限 再次提交为新的镜像 12[root@docker01 ~]# docker commit 5b8161fda2a9 centos6-httpd sha256:705d67a786cac040800b8485cf046fd57b1828b805c515377fc3e9cea3a481c1 启动镜像，做好端口映射。并在浏览器中测试访问 12[root@docker01 ~]# docker run -d -p 1222:22 -p 80:80 centos6-httpd /init.sh 46fa6a06644e31701dc019fb3a8c3b6ef008d4c2c10d46662a97664f838d8c2c 7. Dockerfile自动构建docker镜像官方构建dockerffile文件参考 https://github.com/CentOS/CentOS-Dockerfiles 7.1. Dockerfile指令集dockerfile主要组成部分： 基础镜像信息 FROM centos:6.8 制作镜像操作指令RUN yum insatll openssh-server -y 容器启动时执行指令 CMD [“&#x2F;bin&#x2F;bash”] dockerfile常用指令： FROM 这个镜像的妈妈是谁？（指定基础镜像） MAINTAINER 告诉别人，谁负责养它？（指定维护者信息，可以没有） RUN 你想让它干啥（在命令前面加上RUN即可） ADD 给它点创业资金（COPY文件，会自动解压） WORKDIR 我是cd,今天刚化了妆（设置当前工作目录） VOLUME 给它一个存放行李的地方（设置卷，挂载主机目录） EXPOSE 它要打开的门是啥（指定对外的端口） CMD 奔跑吧，兄弟！（指定容器启动后的要干的事情） dockerfile其他指令： COPY 复制文件 ENV 环境变量 ENTRYPOINT 容器启动后执行的命令 7.2. 创建一个Dockerfile创建第一个Dockerfile文件 123456789# 创建目录[root@docker01 base]# cd /opt/base# 创建Dcokerfile文件，注意大小写[root@docker01 base]# vim DockerfileFROM centos:6.8RUN yum install openssh-server -y RUN echo &quot;root:123456&quot; |chpasswdRUN /etc/init.d/sshd start CMD [&quot;/usr/sbin/sshd&quot;,&quot;-D&quot;] 使用自构建的镜像启动 12[root@docker01 base]# docker image build -t centos6.8-ssh . -t 为镜像标签打标签 . 表示当前路径 使用自构建的镜像启动 12[root@docker01 base]# docker run -d -p 2022:22 centos6.8-ssh-b dc3027d3c15dac881e8e2aeff80724216f3ac725f142daa66484f7cb5d074e7a 7.3. 使用Dcokerfile安装kodexplorerDockerfile文件内容 12345678FROM centos:6.8RUN yum install wget unzip php php-gd php-mbstring -y &amp;&amp; yum clean all# 设置工作目录，之后的操作都在这个目录中WORKDIR /var/www/html/RUN wget -c http://static.kodcloud.com/update/download/kodexplorer4.25.zipRUN unzip kodexplorer4.25.zip &amp;&amp; rm -f kodexplorer4.25.zipRUN chown -R apache.apache .CMD [&quot;/usr/sbin/apachectl&quot;,&quot;-D&quot;,&quot;FOREGROUND&quot;] 更多的Dockerfile可以参考官方方法。 8. Docker中的镜像分层参考文档： http://www.maiziedu.com/wiki/cloud/dockerimage/ Docker 支持通过扩展现有镜像，创建新的镜像。实际上，Docker Hub 中 99% 的镜像都是通过在 base 镜像中安装和配置需要的软件构建出来的。 从上图可以看到，新镜像是从 base 镜像一层一层叠加生成的。每安装一个软件，就在现有镜像的基础上增加一层。 8.1. Docker 镜像为什么分层镜像分层最大的一个好处就是共享资源。 比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 如果多个容器共享一份基础镜像，当某个容器修改了基础镜像的内容，比如 &#x2F;etc 下的文件，这时其他容器的 &#x2F;etc 是不会被修改的，修改只会被限制在单个容器内。这就是容器 Copy-on-Write 特性。 8.2. 可写的容器层当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。 8.3. 容器层的细节说明镜像层数量可能会很多，所有镜像层会联合在一起组成一个统一的文件系统。如果不同层中有一个相同路径的文件，比如 &#x2F;a，上层的 &#x2F;a 会覆盖下层的 &#x2F;a，也就是说用户只能访问到上层中的文件 &#x2F;a。在容器层中，用户看到的是一个叠加之后的文件系统。 文件操作的 只有当需要修改时才复制一份数据，这种特性被称作 Copy-on-Write。可见，容器层保存的是镜像变化的部分，不会对镜像本身进行任何修改。 这样就解释了我们前面提出的问题：容器层记录对镜像的修改，所有镜像层都是只读的，不会被容器修改，所以镜像可以被多个容器共享。 9. 使用docker运行zabbix-server9.1. 容器间的互联在运行zabbix之前务必要了解容器间互联的方法 123456# 创建一个nginx容器docker run -d -p 80:80 nginx# 创建容器，做link，并进入容器中docker run -it --link quirky_brown:web01 centos-ssh /bin/bash# 在容器中访问nginx容器可以ping通ping web01 命令执行过程 12345678910111213141516171819# 启动apache容器[root@docker01 ~]# docker run -d httpd:2.4 3f1f7fc554720424327286bd2b04aeab1b084a3fb011a785b0deab6a34e56955^[[A[root@docker01 docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3f1f7fc55472 httpd:2.4 &quot;httpd-foreground&quot; 6 seconds ago Up 5 seconds 80/tcp determined_clarke# 拉取一个busybox 镜像[root@docker01 ~]# docker pull busybox # 启动容器[root@docker01 ~]# docker run -it --link determined_clarke:web busybox:latest /bin/sh / # # 使用新的容器访问最初的web容器/ # ping web PING web (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.058 ms^C--- web ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.058/0.058/0.058 ms 9.2. 启动zabbix容器1、启动一个mysql的容器 1234567docker run --name mysql-server -t \\ -e MYSQL_DATABASE=&quot;zabbix&quot; \\ -e MYSQL_USER=&quot;zabbix&quot; \\ -e MYSQL_PASSWORD=&quot;zabbix_pwd&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;root_pwd&quot; \\ -d mysql:5.7 \\ --character-set-server=utf8 --collation-server=utf8_bin 2、启动java-gateway容器监控java服务 12docker run --name zabbix-java-gateway -t \\ -d zabbix/zabbix-java-gateway:latest 3、启动zabbix-mysql容器使用link连接mysql与java-gateway。 1234567891011docker run --name zabbix-server-mysql -t \\ -e DB_SERVER_HOST=&quot;mysql-server&quot; \\ -e MYSQL_DATABASE=&quot;zabbix&quot; \\ -e MYSQL_USER=&quot;zabbix&quot; \\ -e MYSQL_PASSWORD=&quot;zabbix_pwd&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;root_pwd&quot; \\ -e ZBX_JAVAGATEWAY=&quot;zabbix-java-gateway&quot; \\ --link mysql-server:mysql \\ --link zabbix-java-gateway:zabbix-java-gateway \\ -p 10051:10051 \\ -d zabbix/zabbix-server-mysql:latest 4、启动zabbix web显示，使用link连接zabbix-mysql与mysql。 12345678910docker run --name zabbix-web-nginx-mysql -t \\ -e DB_SERVER_HOST=&quot;mysql-server&quot; \\ -e MYSQL_DATABASE=&quot;zabbix&quot; \\ -e MYSQL_USER=&quot;zabbix&quot; \\ -e MYSQL_PASSWORD=&quot;zabbix_pwd&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;root_pwd&quot; \\ --link mysql-server:mysql \\ --link zabbix-server-mysql:zabbix-server \\ -p 80:80 \\ -d zabbix/zabbix-web-nginx-mysql:latest 9.3. 关于zabbix API关于zabbix API可以参考官方文档： https://www.zabbix.com/documentation/3.4/zh/manual/api 1、获取token方法 123456789101112# 获取token[root@docker02 ~]# curl -s -X POST -H &#x27;Content-Type:application/json&#x27; -d &#x27;&#123;&quot;jsonrpc&quot;: &quot;2.0&quot;,&quot;method&quot;: &quot;user.login&quot;,&quot;params&quot;: &#123;&quot;user&quot;: &quot;Admin&quot;,&quot;password&quot;: &quot;zabbix&quot;&#125;,&quot;id&quot;: 1&#125;&#x27; http://10.0.0.100/api_jsonrpc.php&#123;&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:&quot;d3be707f9e866ec5d0d1c242292cbebd&quot;,&quot;id&quot;:1&#125; 10. docker 仓库（registry）10.1. 创建一个普通仓库1、创建仓库 1docker run -d -p 5000:5000 --restart=always --name registry -v /opt/myregistry:/var/lib/registry registry 2、修改配置文件，使之支持http 12345[root@docker01 ~]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], &quot;insecure-registries&quot;: [&quot;10.0.0.100:5000&quot;]&#125; 重启docker让修改生效 1[root@docker01 ~]# systemctl restart docker.service 3、修改镜像标签 123456[root@docker01 ~]# docker tag busybox:latest 10.0.0.100:5000/clsn/busybox:1.0[root@docker01 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcentos6-ssh latest 3c2b1e57a0f5 18 hours ago 393MBhttpd 2.4 2e202f453940 6 days ago 179MB10.0.0.100:5000/clsn/busybox 1.0 5b0d59026729 8 days ago 1.15MB 4、将新打标签的镜像上传镜像到仓库 1[root@docker01 ~]# docker push 10.0.0.100:5000/clsn/busybox 10.2. 带basic认证的仓库1、安装加密工具 1[root@docker01 clsn]# yum install httpd-tools -y 2、设置认证密码 12mkdir /opt/registry-var/auth/ -phtpasswd -Bbn clsn 123456 &gt; /opt/registry-var/auth/htpasswd 3、启动容器，在启动时传入认证参数 1docker run -d -p 5000:5000 -v /opt/registry-var/auth/:/auth/ -e &quot;REGISTRY_AUTH=htpasswd&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd registry 4、使用验证用户测试 12345678910111213141516171819202122232425# 登陆用户[root@docker01 ~]# docker login 10.0.0.100:5000 Username: clsn Password: 123456Login Succeeded# 推送镜像到仓库[root@docker01 ~]# docker push 10.0.0.100:5000/clsn/busybox The push refers to repository [10.0.0.100:5000/clsn/busybox]4febd3792a1f: Pushed 1.0: digest: sha256:4cee1979ba0bf7db9fc5d28fb7b798ca69ae95a47c5fecf46327720df4ff352d size: 527#认证文件的保存位置[root@docker01 ~]# cat .docker/config.json &#123; &quot;auths&quot;: &#123; &quot;10.0.0.100:5000&quot;: &#123; &quot;auth&quot;: &quot;Y2xzbjoxMjM0NTY=&quot; &#125;, &quot;https://index.docker.io/v1/&quot;: &#123; &quot;auth&quot;: &quot;Y2xzbjpIenNAMTk5Ng==&quot; &#125; &#125;, &quot;HttpHeaders&quot;: &#123; &quot;User-Agent&quot;: &quot;Docker-Client/17.12.0-ce (linux)&quot; &#125;&#125; 至此，一个简单的docker镜像仓库搭建完成 11. docker-compose编排工具11.1. 安装docker-compose1234# 下载pip软件yum install -y python2-pip# 下载 docker-composepip install docker-compose 国内开启pip 下载加速： http://mirrors.aliyun.com/help/pypi 1234567mkdir ~/.pip/cat &gt; ~/.pip/pip.conf &lt;&lt;&#x27;EOF&#x27;[global]index-url = https://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.comEOF 11.2. 编排启动镜像1、创建文件目录 12[root@docker01 ~]# mkdir /opt/my_wordpress/[root@docker01 ~]# cd /opt/my_wordpress/ 2、编写编排文件 1234567891011121314151617181920212223242526[root@docker01 my_wordpress]# vim docker-compose.ymlversion: &#x27;3&#x27;services: db: image: mysql:5.7 volumes: - /data/db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest volumes: - /data/web_data:/var/www/html ports: - &quot;8000:80&quot; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress 3、启动 123[root@docker01 my_wordpress]# docker-compose up #启动方法：docker-compose up #后台启动方法：docker-compose up -d 4、浏览器上访问http://10.0.0.100:8000 进行wordpress的安装即可 11.3. haproxy代理后端docker容器1、修改编排脚本 1234567891011121314151617181920212223242526[root@docker01 my_wordpress]# cat docker-compose.yml version: &#x27;3&#x27;services: db: image: mysql:5.7 volumes: - /data/db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest volumes: - /data/web_data:/var/www/html ports: - &quot;80&quot; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress 2、同时启动两台wordpress 1234[root@docker01 my_wordpress]# docker-compose scale wordpress=2 WARNING: The scale command is deprecated. Use the up command with the --scale flag instead.Starting mywordpress_wordpress_1 ... doneCreating mywordpress_wordpress_2 ... done 3、安装haproxy 1[root@docker01 ~]# yum install haproxy -y 4、修改haproxy配置文件 关于配置文件的详细说明，参考： https://www.cnblogs.com/MacoLee/p/5853413.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@docker01 ~]#cp /etc/haproxy/haproxy.cfg&#123;,.bak&#125;[root@docker01 ~]# vim /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats level admin #支持命令行控制defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000listen stats mode http bind 0.0.0.0:8888 stats enable stats uri /haproxy-status stats auth admin:123456frontend frontend_www_example_com bind 10.0.0.100:8000 mode http option httplog log global default_backend backend_www_example_combackend backend_www_example_com option forwardfor header X-REAL-IP option httpchk HEAD / HTTP/1.0 balance roundrobin server web-node1 10.0.0.100:32768 check inter 2000 rise 30 fall 15 server web-node2 10.0.0.100:32769 check inter 2000 rise 30 fall 15 5、启动haproxy 12systemctl start haproxysystemctl enable haproxy 6、使用浏览器访问hapeoxy监听的8000端口可以看到负载的情况 7、使用浏览器访问 http://10.0.0.100:8888/haproxy-status 可以看到后端节点的监控状况， 11.4. 安装socat 直接操作socket控制haproxy1、安装软件 1yum install socat.x86_64 -y 2、查看帮助 1[root@docker01 web_data]# echo &quot;help&quot;|socat stdio /var/lib/haproxy/stats 3、下线后端节点 1echo &quot;disable server backend_www_example_com/web-node2&quot;|socat stdio /var/lib/haproxy/stats 4、上线后端节点 1echo &quot;enable server backend_www_example_com/web-node3&quot;|socat stdio /var/lib/haproxy/stats 5、编写php测试页，放到&#x2F;data&#x2F;web_data下，在浏览器中访问可以查看当前的节点 123456789101112[root@docker01 web_data]# vim check.php&lt;html&gt; &lt;head&gt; &lt;title&gt;PHP测试&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;?php echo &#x27;&lt;p&gt;Hello World &lt;/p&gt;&#x27;; ?&gt; &lt;?php echo &quot;访问的服务器地址是:&quot;.&quot;&lt;fontcolor=red&gt;&quot;.$_SERVER[&#x27;SERVER_ADDR&#x27;].&quot;&lt;/font&gt;&quot;.&quot;&lt;br&gt;&quot;; echo&quot;访问的服务器域名是:&quot;.&quot;&lt;fontcolor=red&gt;&quot;.$_SERVER[&#x27;SERVER_NAME&#x27;].&quot;&lt;/font&gt;&quot;.&quot;&lt;br&gt;&quot;; ?&gt; &lt;/body&gt;&lt;/html&gt; 12. 重启docker服务，容器全部退出的解决办法12.1. 在启动是指定自动重启1docker run --restart=always 12.1. 修改docker默认配置文件12# 添加上下面这行&quot;live-restore&quot;: true docker server配置文件 &#x2F;etc&#x2F;docker&#x2F;daemon.json 参考 1234567[root@docker02 ~]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], &quot;graph&quot;: &quot;/opt/mydocker&quot;, # 修改数据的存放目录到/opt/mydocker/，原/var/lib/docker/ &quot;insecure-registries&quot;: [&quot;10.0.0.100:5000&quot;], &quot;live-restore&quot;: true&#125; 重启生效，只对在此之后启动的容器生效 1[root@docker01 ~]# systemctl restart docker.service 13. Docker网络类型 13.1. docker的网络类型 Bridge默认docker网络隔离基于网络命名空间，在物理机上创建docker容器时会为每一个docker容器分配网络命名空间，并且把容器IP桥接到物理机的虚拟网桥上。 13.2. 不为容器配置网络功能此模式下创建容器是不会为容器配置任何网络参数的，如：容器网卡、IP、通信路由等，全部需要自己去配置。 123456[root@docker01 ~]# docker run -it --network none busybox:latest /bin/sh / # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 13.3. 与其他容器共享网络配置(Container）此模式和host模式很类似，只是此模式创建容器共享的是其他容器的IP和端口而不是物理机，此模式容器自身是不会配置网络和端口，创建此模式容器进去后，你会发现里边的IP是你所指定的那个容器IP并且端口也是共享的，而且其它还是互相隔离的，如进程等。 12345678910[root@docker01 ~]# docker run -it --network container:mywordpress_db_1 busybox:latest /bin/sh / # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever105: eth0@if106: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff inet 172.18.0.3/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever 13.4. 使用宿主机网络此模式创建的容器没有自己独立的网络命名空间，是和物理机共享一个Network Namespace，并且共享物理机的所有端口与IP，并且这个模式认为是不安全的。 1[root@docker01 ~]# docker run -it --network host busybox:latest /bin/shshell 13.5. 查看网络列表123456[root@docker01 ~]# docker network list NETWORK ID NAME DRIVER SCOPEb15e8a720d3b bridge bridge local345d65b4c2a0 host host localbc5e2a32bb55 mywordpress_default bridge localebf76eea91bb none null local 用PIPEWORK为docker容器配置独立IP 参考文档： blog.csdn.net&#x2F;design321&#x2F;article&#x2F;details&#x2F;48264825 官方网站： github.com&#x2F;jpetazzo&#x2F;pipework 宿主环境：centos7.2 1、安装pipework 1234wget https://github.com/jpetazzo/pipework/archive/master.zipunzip master.zip cp pipework-master/pipework /usr/local/bin/chmod +x /usr/local/bin/pipework 2、配置桥接网卡 安装桥接工具 1yum install bridge-utils.x86_64 -y 修改网卡配置，实现桥接 1234567891011121314151617181920# 修改eth0配置，让br0实现桥接[root@docker01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 TYPE=EthernetBOOTPROTO=staticNAME=eth0DEVICE=eth0ONBOOT=yesBRIDGE=br0[root@docker01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-br0 TYPE=BridgeBOOTPROTO=staticNAME=br0DEVICE=br0ONBOOT=yesIPADDR=10.0.0.100NETMASK=255.255.255.0GATEWAY=10.0.0.254DNS1=223.5.5.5# 重启网络[root@docker01 ~]# /etc/init.d/network restart 3、运行一个容器镜像测试： 1pipework br0 $(docker run -d -it -p 6880:80 --name httpd_pw httpd) 10.0.0.220/24@10.0.0.254 在其他主机上测试端口及连通性 12345[root@docker01 ~]# curl 10.0.0.220&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;[root@docker01 ~]# ping 10.0.0.220 -c 1PING 10.0.0.220 (10.0.0.220) 56(84) bytes of data.64 bytes from 10.0.0.220: icmp_seq=1 ttl=64 time=0.043 ms 4、再运行一个容器，设置网路类型为none： 1pipework br0 $(docker run -d -it --net=none --name test httpd:2.4) 10.0.0.221/24@10.0.0.254 进行访问测试 12[root@docker01 ~]# curl 10.0.0.221&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 5、重启容器后需要再次指定： 12pipework br0 testduliip 172.16.146.113/24@172.16.146.1pipework br0 testduliip01 172.16.146.112/24@172.16.146.1 Dcoker跨主机通信之overlay可以参考： cnblogs.com&#x2F;CloudMan6&#x2F;p&#x2F;7270551.html 13.6. Docker跨主机通信之macvlan创建网络 12[root@docker01 ~]# docker network create --driver macvlan --subnet 10.1.0.0/24 --gateway 10.1.0.254 -o parent=eth0 macvlan_133a1f41dcc074f91b5bd45e7dfedabfb2b8ec82db16542f05213839a119b62ca 设置网卡为混杂模式 1ip link set eth0 promisc on 创建使用macvlan网络容器 1[root@docker02 ~]# docker run -it --network macvlan_1 --ip=10.1.0.222 busybox /b 14. docker企业级镜像仓库harbor容器管理 123[root@docker01 harbor]# pwd/opt/harbor[root@docker01 harbor]# docker-compose stop 1、安装docker、docker-compose 下载 harbor 12cd /opt &amp;&amp; https://storage.googleapis.com/harbor-releases/harbor-offline-installer-v1.3.0.tgztar xf harbor-offline-installer-v1.3.0.tgz 2、修改主机及web界面密码 12345[root@docker01 harbor]# vim harbor.cfg ··· hostname = 10.0.0.100 harbor_admin_password = Harbor12345 ··· 3、执行安装脚本 1[root@docker01 harbor]# ./install.sh 浏览器访问 http://10.0.0.11 添加一个项目 4、镜像推送到仓库的指定项目 1234567891011[root@docker02 ~]# docker tag centos:6.8 10.0.0.100/clsn/centos6.8:1.0[root@docker02 ~]# [root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 5b0d59026729 8 days ago 1.15MB10.0.0.100/clsn/centos6.8 1.0 6704d778b3ba 2 months ago 195MBcentos 6.8 6704d778b3ba 2 months ago 195MB[root@docker02 ~]# docker login 10.0.0.100Username: adminPassword: Login Succeeded 5、推送镜像 123[root@docker02 ~]# docker push 10.0.0.100/clsn/centos6.8 The push refers to repository [10.0.0.100/clsn/centos6.8]e00c9229b481: Pushing 13.53MB/194.5MB 6、在web界面里查看 14.1. 使用容器的建议 不要以拆分方式进行应用程序发布 不要创建大型镜像 不要在单个容器中运行多个进程 不要再镜像内保存凭证，不要依赖IP地址 以非root用户运行进程 不要使用“最新”标签 不要利用运行中的容器创建镜像 不要使用单层镜像 不要将数据存放在容器内 14.2. 关于Docker容器的监控容器的基本信息 包括容器的数量、ID、名称、镜像、启动命令、端口等信息 容器的运行状态 统计各状态的容器的数量，包括运行中、暂停、停止及异常退出 容器的用量信息 统计容器的CPU使用率、内存使用量、块设备I&#x2F;O使用量、网络使用情况等资源的使用情况 参考文献 redhat.com&#x2F;zh&#x2F;topics&#x2F;containers&#x2F;whats-a-linux-container redhat.com&#x2F;zh&#x2F;topics&#x2F;containers&#x2F;what-is-docker blog.51cto.com&#x2F;dihaifeng&#x2F;1713512 cnblogs.com&#x2F;Bourbon-tian&#x2F;p&#x2F;6867796.html cnblogs.com&#x2F;CloudMan6&#x2F;p&#x2F;6806193.html","tags":["开发工具","Docker"],"categories":["开发工具"]},{"title":"MySQL大表优化方案","path":"/2023/12/24/MySQL大表优化方案/","content":"当MySQL单表记录数过大时，增删改查性能都会急剧下降，可以参考以下步骤来优化： 单表优化除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量： 字段 尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED VARCHAR的长度只分配真正需要的空间 使用枚举或整数代替字符串类型 尽量使用TIMESTAMP而非DATETIME， 单表不要有太多字段，建议在20以内 避免使用NULL字段，很难查询优化且占用额外索引空间 用整型来存IP 索引 索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描 值分布很稀少的字段不适合建索引，例如”性别”这种只有两三个值的字段 字符字段只建前缀索引 字符字段最好不要做主键 不用外键，由程序保证约束 尽量不用UNIQUE，由程序保证约束 使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引 查询SQL 可通过开启慢查询日志来找出较慢的SQL 不做列运算：SELECT id WHERE age + 1 &#x3D; 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边 sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库 不用SELECT * OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内 不用函数和触发器，在应用程序实现 避免%xxx式查询 少用JOIN 使用同类型进行比较，比如用’123’和’123’比，123和123比 尽量避免在WHERE子句中使用!&#x3D;或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描 对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5 列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大 引擎目前广泛使用的是MyISAM和InnoDB两种引擎： MyISAMMyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是： 不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁 不支持事务 不支持外键 不支持崩溃后的安全恢复 在表有读取查询的同时，支持往表中插入新纪录 支持BLOB和TEXT的前500个字符索引，支持全文索引 支持延迟更新索引，极大提升写入性能 对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用 InnoDBInnoDB在MySQL 5.5后成为默认索引，它的特点是： 支持行锁，采用MVCC来支持高并发 支持事务 支持外键 支持崩溃后的安全恢复 不支持全文索引 总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表 系统调优参数可以使用下面几个工具来做基准测试： sysbench：一个模块化，跨平台以及多线程的性能测试工具 iibench-mysql：基于 Java 的 MySQL&#x2F;Percona&#x2F;MariaDB 索引进行插入性能测试工具 tpcc-mysql：Percona开发的TPC-C测试工具 具体的调优参数内容较多，具体可参考官方文档，这里介绍一些比较重要的参数： back_log：back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。可以从默认的50升至500 wait_timeout：数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时 max_user_connection: 最大连接数，默认为0无上限，最好设一个合理上限 thread_concurrency：并发线程数，设为CPU核数的两倍 skip_name_resolve：禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问 key_buffer_size：索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询show status like ‘key_read%’，保证key_reads &#x2F; key_read_requests在0.1%以下最好 innodb_buffer_pool_size：缓存数据块和索引块，对InnoDB表性能影响最大。通过查询show status like ‘Innodb_buffer_pool_read%’，保证 (Innodb_buffer_pool_read_requests – Innodb_buffer_pool_reads) &#x2F; Innodb_buffer_pool_read_requests越高越好 innodb_additional_mem_pool_size：InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小 innodb_log_buffer_size：InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB query_cache_size：缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失。根据命中率(Qcache_hits&#x2F;(Qcache_hits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大.可以通过命令show status like ‘Qcache_%’查看目前系统Query catch使用大小 read_buffer_size：MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能 sort_buffer_size：MySql执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sort_buffer_size变量的大小 read_rnd_buffer_size：MySql的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 record_buffer：每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值 thread_cache_size：保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的 table_cache：类似于thread_cache_size，但用来缓存表文件，对InnoDB效果不大，主要用于MyISAM 升级硬件Scale up，这个不多说了，根据MySQL是CPU密集型还是I&#x2F;O密集型，通过提升CPU和内存、使用SSD，都能显著提升MySQL性能 读写分离也是目前常用的优化，从库读主库写，一般不要采用双主或多主引入很多复杂性，尽量采用文中的其他方案来提高性能。同时目前很多拆分的解决方案同时也兼顾考虑了读写分离 缓存缓存可以发生在这些层次： MySQL内部：在系统调优参数介绍了相关设置 数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象Persistence Object 应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对象是数据传输对象Data Transfer Object Web层：针对web页面做缓存 浏览器客户端：用户端的缓存 可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式： 直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring Cache的工作方式。这种实现非常简单，同步好，但效率一般。 回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。 表分区MySQL在5.1版引入的分区是一种简单的水平拆分，用户需要在建表的时候加上分区参数，对应用是透明的无需修改代码 对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成，实现分区的代码实际上是通过对一组底层表的对象封装，但对SQL层来说是一个完全封装底层的黑盒子。MySQL实现分区的方式也意味着索引也是按照分区的子表定义，没有全局索引 用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，可以通过EXPLAIN PARTITIONS来查看某条SQL语句会落在那些分区上，从而进行SQL优化，如下图5条记录落在两个分区上： 12345678mysql&gt; explain partitions select count(1) from user_partition where id in (1,2,3,4,5);+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+| 1 | SIMPLE | user_partition | p1,p4 | range | PRIMARY | PRIMARY | 8 | NULL | 5 | Using where; Using index |+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+1 row in set (0.00 sec) 分区的好处是： 可以让单表存储更多的数据 分区表的数据更容易维护，可以通过清楚整个分区批量删除大量数据，也可以增加新的分区来支持新插入的数据。另外，还可以对一个独立分区进行优化、检查、修复等操作 部分查询能够从查询条件确定只落在少数分区上，速度会很快 分区表的数据还可以分布在不同的物理设备上，从而搞笑利用多个硬件设备 可以使用分区表赖避免某些特殊瓶颈，例如InnoDB单个索引的互斥访问、ext3文件系统的inode锁竞争 可以备份和恢复单个分区 分区的限制和缺点： 一个表最多只能有1024个分区 如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来 分区表无法使用外键约束 NULL值会使分区过滤无效 所有分区必须使用相同的存储引擎 分区的类型： RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区 LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择 HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式 KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值 分区适合的场景有： 最适合的场景数据的时间序列性比较强，则可以按时间来分区，如下所示： 1234567891011121314CREATE TABLE members ( firstname VARCHAR(25) NOT NULL, lastname VARCHAR(25) NOT NULL, username VARCHAR(16) NOT NULL, email VARCHAR(35), joined DATE NOT NULL)PARTITION BY RANGE( YEAR(joined) ) ( PARTITION p0 VALUES LESS THAN (1960), PARTITION p1 VALUES LESS THAN (1970), PARTITION p2 VALUES LESS THAN (1980), PARTITION p3 VALUES LESS THAN (1990), PARTITION p4 VALUES LESS THAN MAXVALUE); 查询时加上时间范围条件效率会非常高，同时对于不需要的历史数据能很容的批量删除。 如果数据有明显的热点，而且除了这部分数据，其他数据很少被访问到，那么可以将热点数据单独放在一个分区，让这个分区的数据能够有机会都缓存在内存中，查询时只访问一个很小的分区表，能够有效使用索引和缓存 另外MySQL有一种早期的简单的分区实现 - 合并表（merge table），限制较多且缺乏优化，不建议使用，应该用新的分区机制来替代 垂直拆分垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库。垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联 比如原始的用户表是： 垂直拆分后是： 垂直拆分的优点是： 可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I&#x2F;O次数(每次查询时读取的Block 就少) 可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起 数据维护简单 缺点是： 主键出现冗余，需要管理冗余列 会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力 依然存在单表数据量过大的问题（需要水平拆分） 事务处理复杂 水平拆分概述水平拆分是通过某种策略将数据分片来存储，分库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量。 前面的表分区本质上也是一种特殊的库内分表 库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决 前面垂直拆分的用户表如果进行水平拆分，结果是： 实际情况中往往会是垂直拆分和水平拆分的结合，即将Users_A_M和Users_N_Z再拆成Users和UserExtras，这样一共四张表 水平拆分的优点是: 不存在单库大数据和高并发的性能瓶颈 应用端改造较少 提高了系统的稳定性和负载能力 缺点是： 分片事务一致性难以解决 跨节点Join性能差，逻辑复杂 数据多次扩展难度跟维护量极大 分片原则 能不分就不分，参考单表优化 分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量 分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容 尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题 查询条件尽量优化，尽量避免Select * 的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。 通过数据冗余和表分区赖降低跨库Join的可能 这里特别强调一下分片规则的选择问题，如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片，因为具有时效性的数据，我们往往关注其近期的数据，查询条件中往往带有时间字段进行过滤，比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。 总体上来说，分片的选择是取决于最频繁的查询SQL的条件，因为不带任何Where语句的查询SQL，会遍历所有的分片，性能相对最差，因此这种SQL越多，对系统的影响越大，所以我们要尽量避免这种SQL的产生。 解决方案由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。 客户端架构通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现 这是一个客户端架构的例子： 可以看到分片的实现是和应用服务器在一起的，通过修改Spring JDBC层来实现 客户端架构的优点是： 应用直连数据库，降低外围系统依赖所带来的宕机风险 集成成本低，无需额外运维的组件 缺点是： 限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心 将分片逻辑的压力放在应用服务器上，造成额外风险 代理架构通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件 这是一个代理架构的例子： 代理组件为了分流和防止单点，一般以集群形式存在，同时可能需要Zookeeper之类的服务组件来管理 代理架构的优点是： 能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强 对于应用服务器透明且没有增加任何额外负载 缺点是： 需部署和运维独立的代理中间件，成本高 应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险 各方案比较 出品方 架构模型 支持数据库 分库 分表 读写分离 外部依赖 是否开源 实现语言 支持语言 最后更新 Github星数 MySQL Fabric MySQL官方 代理架构 MySQL 有 有 有 无 是 python 无限制 4个月前 35 Cobar 阿里巴巴 代理架构 MySQL 有 无 无 无 是 Java 无限制 两年前 1287 Cobar Client 阿里巴巴 客户端架构 MySQL 有 无 无 无 是 Java Java 三年前 344 TDDL 淘宝 客户端架构 无限制 有 有 有 Diamond 只开源部分 Java Java 未知 519 Atlas 奇虎360 代理架构 MySQL 有 有 有 无 是 C 无限制 10个月前 1941 Heisenberg 百度熊照 代理架构 MySQL 有 有 有 无 是 Java 无限制 2个月前 197 TribeDB 个人 代理架构 MySQL 有 有 有 无 是 NodeJS 无限制 3个月前 126 ShardingJDBC 当当 客户端架构 MySQL 有 有 有 无 是 Java Java 当天 1144 Shark 个人 客户端架构 MySQL 有 有 无 无 是 Java Java 两天前 84 KingShard 个人 代理架构 MySQL 有 有 有 无 是 Golang 无限制 两天前 1836 OneProxy 平民软件 代理架构 MySQL 有 有 有 无 否 未知 无限制 未知 未知 MyCat 社区 代理架构 MySQL 有 有 有 无 是 Java 无限制 两天前 1270 Vitess Youtube 代理架构 MySQL 有 有 有 无 是 Golang 无限制 当天 3636 Mixer 个人 代理架构 MySQL 有 有 无 无 是 Golang 无限制 9个月前 472 JetPants Tumblr 客户端架构 MySQL 有 有 无 无 是 Ruby Ruby 10个月前 957 HibernateShard Hibernate 客户端架构 无限制 有 有 无 无 是 Java Java 4年前 57 MybatisShard MakerSoft 客户端架构 无限制 有 有 无 无 是 Java Java 11个月前 119 Gizzard Twitter 代理架构 无限制 有 有 无 无 是 Java 无限制 3年前 2087 如此多的方案，如何进行选择？可以按以下思路来考虑： 确定是使用代理架构还是客户端架构。中小型规模或是比较简单的场景倾向于选择客户端架构，复杂场景或大规模系统倾向选择代理架构 具体功能是否满足，比如需要跨节点ORDER BY，那么支持该功能的优先考虑 不考虑一年内没有更新的产品，说明开发停滞，甚至无人维护和技术支持 最好按大公司-&gt;社区-&gt;小公司-&gt;个人这样的出品方顺序来选择 选择口碑较好的，比如github星数、使用者数量质量和使用者反馈 开源的优先，往往项目有特殊需求可能需要改动源代码 按照上述思路，推荐以下选择： 客户端架构：ShardingJDBC 代理架构：MyCat或者Atlas 兼容MySQL且可水平扩展的数据库目前也有一些开源数据库兼容MySQL协议，如： TiDB Cubrid 但其工业品质和MySQL尚有差距，且需要较大的运维投入，如果想将原始的MySQL迁移到可水平扩展的新数据库中，可以考虑一些云数据库： 阿里云PetaData 阿里云OceanBase 腾讯云DCDB NoSQL在MySQL上做Sharding是一种戴着镣铐的跳舞，事实上很多大表本身对MySQL这种RDBMS的需求并不大，并不要求ACID，可以考虑将这些表迁移到NoSQL，彻底解决水平扩展问题，例如： 日志类、监控类、统计类数据 非结构化或弱结构化数据 对事务要求不强，且无太多关联操作的数据","tags":["MySQL","DataBase","SQL优化"],"categories":["DataBase"]},{"title":"MyBatis 的执行流程","path":"/2023/12/24/MyBatis-的执行流程！/","content":"概要在MyBatis中，利用编程式进行数据查询，主要就是下面几行代码： 123SqlSession session = sqlSessionFactory.openSession();UserMapper userMapper = session.getMapper(UserMapper.class);List&lt;LwUser&gt; userList = userMapper.listUserByUserName(&quot;孤狼1号&quot;); 第一行是获取一个SqlSession对象在上一篇文章分析过了，第二行就是获取UserMapper接口，第三行一行代码就实现了整个查询语句的流程，接下来我们就来仔细分析一下第二和第三步。 获取Mapper接口(getMapper)第二步是通过SqlSession对象是获取一个Mapper接口，这个流程还是相对简单的，下面就是我们调用session.getMapper方法之后的运行时序图： 1、在调用getMapper之后，会去Configuration对象中获取Mapper对象，因为在项目启动的时候就会把Mapper接口加载并解析存储到Configuration对象 2、通过Configuration对象中的MapperRegistry对象属性，继续调用getMapper方法 3、根据type类型，从MapperRegistry对象中的knownMappers获取到当前类型对应的代理工厂类，然后通过代理工厂类生成对应Mapper的代理类 4、最终获取到我们接口对应的代理类MapperProxy对象 而MapperProxy可以看到实现了InvocationHandler，使用的就是JDK动态代理。 至此获取Mapper流程结束了，那么就有一个问题了MapperRegistry对象内的HashMap属性knownMappers中的数据是什么时候存进去的呢？ Mapper接口和映射文件是何时关联的Mapper接口及其映射文件是在加载mybatis-config配置文件的时候存储进去的，下面就是时序图： 1、首先我们会手动调用SqlSessionFactoryBuilder方法中的build()方法： 2、然后会构造一个XMLConfigBuilder对象，并调用其parse方法： 3、然后会继续调用自己的parseConfiguration来解析配置文件，这里面就会分别去解析全局配置文件的顶级节点，其他的我们先不看，我们直接看最后解析mappers节点 4、继续调用自己的mapperElement来解析mappers文件（这个方法比较长，为了方便截图完整，所以把字体缩小了1号），可以看到，这里面分了四种方式来解析mappers节点的配置，对应了4种mapper配置方式，而其中红框内的两种方式是直接配置的xml映射文件，蓝框内的两种方式是解析直接配置Mapper接口的方式，从这里也可以说明，不论配置哪种方式，最终MyBatis都会将xml映射文件和Mapper接口进行关联。 5、我们先看第2种和第3中（直接配置xml映射文件的解析方式），会构建一个XMLMapperBuilder对象并调用其parse方法。 当然，这个还是会被解析的，后面执行查询的时候会再次通过不断遍历去全部解析完毕，不过有一点需要注意的是，互相引用这种是会导致解析失败报错的，所以在开发过程中我们应该避免循环依赖的产生。 6、解析完映射文件之后，调用自身方法bindMapperForNamespace，开始绑定Mapper接口和映射文件： 7、调用Configuration对象的addMapper 8、调用Configuration对象的属性MapperRegistry内的addMapper方法，这个方法就是正式将Mapper接口添加到knownMappers，所以上面getMapper可以直接获取： 到这里我们就完成了Mapper接口和xml映射文件的绑定 9、注意上面红框里面的代码，又调用了一次parse方法，这个parse方法主要是解析注解，比如下面的语句： 12@Select(&quot;select * from lw_user&quot;)List&lt;LwUser&gt; listAllUser(); 所以这个方法里面会去解析@Select等注解，需要注意的是，parse方法里面会同时再解析一次xml映射文件，因为上面我们提到了mappers节点有4种配置方式，其中两种配置的是Mapper接口，而配置Mapper接口会直接先调用addMapper接口，并没有解析映射文件，所以进入注解解析方法parse之中会需要再尝试解析一次XML映射文件。 解析完成之后，还会对Mapper接口中的方法进行解析，并将每个方法的全限定类名作为key存入存入Configuration中的mappedStatements属性。 需要指出的是，这里存储的时候，同一个value会存储2次，**一个全限定名作为key，另一个就是只用方法名(sql语句的id)来作为key**： 所以最终mappedStatements会是下面的情况： 事实上如果我们通过接口的方式来编程的话，最后来getStatement的时候，都是根据全限定名来取的，所以即使有重名对我们也没有影响，而之所以要这么做的原因其实还是为了兼容早期版本的用法，那就是不通过接口，而是直接通过方法名的方式来进行查询： 1session.selectList(&quot;com.lonelyWolf.mybatis.mapper.UserMapper.listAllUser&quot;); 这里如果shortName没有重复的话，是可以直接通过简写来查询的： 1session.selectList(&quot;listAllUser&quot;); 但是通过简写来查询一旦shortName重复了就会抛出以下异常： 这里的异常其实就是StrickMap的get方法抛出来的： sql执行流程分析上面我们讲到了，获取到的Mapper接口实际上被包装成为了代理对象，所以我们执行查询语句肯定是执行的代理对象方法，接下来我们就以Mapper接口的代理对象MapperProxy来分析一下查询流程。 整个sql执行流程可以分为两大步骤： 一、寻找sql 二、执行sql语句 寻找sql首先还是来看一下寻找sql语句的时序图： 1、了解代理模式的应该都知道，调用被代理对象的方法之后实际上执行的就是代理对象的invoke方法 2、因为我们这里并没有调用Object类中的方法，所以肯定走的else。else中会继续调用MapperProxy内部类MapperMethodInvoker中的方法cachedInvoker，这里面会有一个判断，判断一下我们是不是default方法，因为Jdk1.8中接口中可以新增default方法，而default方法是并不是一个抽象方法，所以也需要特殊处理（刚开始会从缓存里面取，缓存相关知识我们这里先不讲，后面会单独写一篇来分析一下缓存)）。 3、接下来，是构造一个MapperMethod对象,这个对象封装了Mapper接口中对应的方法信息以及对应的sql语句信息： 这里面就会把要执行的sql语句，请求参数，方法返回值全部解析封装成MapperMethod对象，然后后面就可以开始准备执行sql语句了 执行sql语句还是先来看一下执行Sql语句的时序图： 1、我们继续上面的流程进入execute方法： 2、这里面会根据语句类型以及返回值类型来决定如何执行，本人这里返回的是一个集合，故而我们进入executeForMany方法： 3、这里面首先会将前面存好的参数进行一次转换，然后绕了这么一圈，回到了起点SqlSession对象，继续调用selectList方法： 3、接下来又讲流程委派给了Execute去执行query方法，最终又会去调用queryFromDatabase方法： 4、到这里之后，终于要进入正题了，一般带了这种do开头的方法就是真正做事的，Spring中很多地方也是采用的这种命名方式： 注意，前面我们的sql语句还是占位符的方式，并没有将参数设置进去，所以这里在return上面一行调用prepareStatement方法创建Statement对象的时候会去设置参数，替换占位符。参数如何设置我们先跳过，等把流程执行完了我们在单独分析参数映射和结果集映射。 5、继续进入PreparedStatementHandler对象的query方法，可以看到，这一步就是调用了jdbc操作对象PreparedStatement中的execute方法，最后一步就是转换结果集然后返回。 到这里，整个SQL语句执行流程分析就结束了，中途有一些参数的存储以及转换并没有深入进去，因为参数的转换并不是核心，只要清楚整个数据的流转流程，我们自己也可以有自己的实现方式，只要存起来最后我们能重新解析读出来就行。 参数映射现在我们来看一下上面在执行查询之前参数是如何进行设置的，我们先进入prepareStatement方法： 我们发现，最终是调用了StatementHandler中的parameterize进行参数设置，接下来这里为了节省篇幅，我们不会一步步点进去，直接进入设置参数的方法： 上面的BaseTypeHandler是一个抽象类，setNonNullParameter并没有实现，都是交给子类去实现，而每一个子类就是对应了数据库的一种类型。下图中就是默认的一个子类StringTypeHandler，里面没什么其他逻辑，就是设置参数。 可以看到String里面调用了jdbc中的setString方法，而如果是int也会调用setInt方法。看到这些子类如果大家之前阅读过我前面讲的MyBatis参数配置，应该就很明显可以知道，这些子类就是系统默认提供的一些typeHandler。而这些默认的typeHandler会默认被注册并和Java对象进行绑定： 正是因为MyBatis中默认提供了常用数据类型的映射，所以我们写Sql的时候才可以省略参数映射关系，可以直接采用下面的方式，系统可以根据我们参数的类型，自动选择合适的typeHander进行映射： 1select user_id,user_name from lw_user where user_name=#&#123;userName&#125; 上面这条语句实际上和下面这条是等价的： 1select user_id,user_name from lw_user where user_name=#&#123;userName,jdbcType=VARCHAR&#125; 或者说我们可以直接指定typeHandler： 1select user_id,user_name from lw_user where user_name = #&#123;userName,jdbcType=VARCHAR,typeHandler=org.apache.ibatis.type.IntegerTypeHandler&#125; 这里因为我们配置了typeHandler，所以会优先以配置的typeHandler为主不会再去读取默认的映射，如果类型不匹配就会直接报错了： 看到这里很多人应该就知道了，如果我们自己自定义一个typeHandler，然后就可以配置成我们自己的自定义类。所以接下来就让我们看看如何自定义一个typeHandler 自定义typeHandler自定义typeHandler需要实现BaseTypeHandler接口，BaseTypeHandler有4个方法，包括结果集映射，为了节省篇幅，代码没有写上来： 1234567891011121314151617package com.lonelyWolf.mybatis.typeHandler;import org.apache.ibatis.type.BaseTypeHandler;import org.apache.ibatis.type.JdbcType;import java.sql.CallableStatement;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;public class MyTypeHandler extends BaseTypeHandler&lt;String&gt; &#123; @Override public void setNonNullParameter(PreparedStatement preparedStatement, int index, String param, JdbcType jdbcType) throws SQLException &#123; System.out.println(&quot;自定义typeHandler生效了&quot;); preparedStatement.setString(index,param); &#125; 然后我们改写一下上面的查询语句： 1select user_id,user_name from lw_user where user_name=#&#123;userName,jdbcType=VARCHAR,typeHandler=com.lonelyWolf.mybatis.typeHandler.MyTypeHandler&#125; 然后执行，可以看到，自定义的typeHandler生效了： 结果集映射接下来让我们看看结果集的映射，回到上面执行sql流程的最后一个方法： 1resultSetHandler.handleResultSets(ps) 结果集映射里面的逻辑相对来说还是挺复杂的，因为要考虑到非常多的情况，这里我们就不会去深究每一个细节，直接进入到正式解析结果集的代码，下面的5个代码片段就是一个简单的但是完整的解析流程： 从上面的代码片段我们也可以看到，实际上解析结果集还是很复杂的，就如我们上一篇介绍的复杂查询一样，一个查询可以不断嵌套其他查询，还有延迟加载等等一些复杂的特性的处理，所以逻辑分支是有很多，但是不管怎么处理，最后的核心还是上面的一套流程，最终还是会调用typeHandler来获取查询到的结果。 是的，你没猜错，这个就是上面我们映射参数的typeHandler，因为typeHandler里面不只是一个设置参数方法，还有获取结果集方法(上面设置参数的时候省略了)。 自定义typeHandler结果集所以说我们还是用上面那个MyTypeHandler 例子来重写一下取值方法(省略了设置参数方法)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.lonelyWolf.mybatis.typeHandler;import org.apache.ibatis.type.BaseTypeHandler;import org.apache.ibatis.type.JdbcType;import java.sql.CallableStatement;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;public class MyTypeHandler extends BaseTypeHandler&lt;String&gt; &#123; /** * 设置参数 */ @Override public void setNonNullParameter(PreparedStatement preparedStatement, int index, String param, JdbcType jdbcType) throws SQLException &#123; System.out.println(&quot;设置参数-&gt;自定义typeHandler生效了&quot;); preparedStatement.setString(index,param); &#125; /** * 根据列名获取结果 */ @Override public String getNullableResult(ResultSet resultSet, String columnName) throws SQLException &#123; System.out.println(&quot;根据columnName获取结果-&gt;自定义typeHandler生效了&quot;); return resultSet.getString(columnName); &#125; /** * 根据列的下标来获取结果 */ @Override public String getNullableResult(ResultSet resultSet, int columnIndex) throws SQLException &#123; System.out.println(&quot;根据columnIndex获取结果-&gt;自定义typeHandler生效了&quot;); return resultSet.getString(columnIndex); &#125; /** * 处理存储过程的结果集 */ @Override public String getNullableResult(CallableStatement callableStatement, int columnIndex) throws SQLException &#123; return callableStatement.getString(columnIndex); &#125;&#125; 改写Mapper映射文件配置： 12345678&lt;resultMap id=&quot;MyUserResultMap&quot; type=&quot;lwUser&quot;&gt; &lt;result column=&quot;user_id&quot; property=&quot;userId&quot; jdbcType=&quot;VARCHAR&quot; typeHandler=&quot;com.lonelyWolf.mybatis.typeHandler.MyTypeHandler&quot; /&gt; &lt;result column=&quot;user_name&quot; property=&quot;userName&quot; jdbcType=&quot;VARCHAR&quot; /&gt;&lt;/resultMap&gt;&lt;select id=&quot;listUserByUserName&quot; parameterType=&quot;String&quot; resultMap=&quot;MyUserResultMap&quot;&gt; select user_id,user_name from lw_user where user_name=#&#123;userName,jdbcType=VARCHAR,typeHandler=com.lonelyWolf.mybatis.typeHandler.MyTypeHandler&#125;&lt;/select&gt; 执行之后输出如下： 因为我们属性上面只配置了一个属性，所以只输出了一次。 工作流程图上面介绍了代码的流转，可能绕来绕去有点晕，所以我们来画一个主要的对象之间流程图来更加清晰的展示一下MyBatis主要工作流程： 从上面的工作流程图上我们可以看到，SqlSession下面还有4大对象，这4大对象也很重要，后面学习拦截器的时候就是针对这4大对象进行的拦截，关于这4大对象的具体详情，我们下一篇文章再展开分析。 总结本文主要分析了MyBatis的SQL执行流程。在分析流程的过程中，我们也举例论证了如何自定义typeHandler来实现自定义的参数映射和结果集映射，不过MyBatis中提供的默认映射其实可以满足大部分的需求，如果我们对某些属性需要特殊处理，那么就可以采用自定义的typeHandler来实现，相信如果本文如果读懂了，以下几点大家应该至少会有一个清晰的认识： 1、Mapper接口和映射文件是如何进行绑定的 2、MyBatis中SQL语句的执行流程 3、自定义MyBatis中的参数设置处理器typeHandler 4、自定义MyBatis中结果集处理器typeHandler 当然，其中很多细节并没有提到，而看源码我们也并不需要追求每一行代码都能看懂，就比如我们一个稍微复杂一点的业务系统，即使我们是项目开发者如果某一个模块不是本人负责的，恐怕也很难搞清楚每一行代码的含义。所以对于MyBatis及其他框架的源码中也是一样，首先应该从大局入手，掌握整体流程和设计思想，然后如果对某些实现细节感兴趣，再深入进行了解。","tags":["MyBatis","框架"],"categories":["框架"]},{"title":"用“状态模式”代替if-else","path":"/2023/12/24/用“状态模式”代替if-else/","content":"简介 状态模式是行为型设计模式的一种。其设计理念是当对象的内部状态发生改变时，随之改变其行为。状态和行为之间是一一对应的。 该模式主要用于，对象的行为依赖于它的状态，并且其行为是随着状态的改变而切换时。 状态模式UML类图类图讲解 State：抽象状态接口（也可以定义成抽象类），该接口封装了所有状态所对应的行为。ConcreteStateA&#x2F;B：具体状态类，该类实现了抽象状态接口，会根据自身对应的状态来实现接口中定义的方法，还有另一个功能是指明如何过渡到下一个状态。Context：环境（上下文）角色，该类负责状态的切换，还持有一个State实例，代表当前环境所处状态。 案例讲解案例：通过状态模式来实现自助售卖机的功能。 状态接口12345678public interface State &#123; // 挑选商品 void choose(); // 付款 boolean payment(); // 分发商品 void dispenseCommodity();&#125; 挑选商品状态类123456789101112131415161718192021222324252627282930public class ChooseGoods implements State &#123; VendingMachine machine; public ChooseGoods(VendingMachine machine) &#123; this.machine = machine; &#125; @Override public void choose() &#123; if (machine.getCount() &gt; 0) &#123; System.out.println(&quot;商品挑选成功，请及时付款！&quot;); machine.setState(machine.getPaymentState()); &#125; else &#123; System.out.println(&quot;很遗憾，商品售罄了！&quot;); machine.setState(machine.getEmptyState()); &#125; &#125; @Override public boolean payment() &#123; System.out.println(&quot;请先挑选商品！&quot;); return false; &#125; @Override public void dispenseCommodity() &#123; System.out.println(&quot;请先挑选商品！&quot;); &#125;&#125; 付款状态类12345678910111213141516171819202122232425262728293031public class PaymentState implements State &#123; VendingMachine machine; public PaymentState(VendingMachine machine) &#123; this.machine = machine; &#125; @Override public void choose() &#123; System.out.println(&quot;商品已选购完成请勿重复挑选&quot;); &#125; @Override public boolean payment() &#123; Random random = new Random(); int num = random.nextInt(10); if(num % 2 == 0)&#123; System.out.println(&quot;付款成功！&quot;); machine.setState(machine.getDispenseCommodityState()); return true; &#125; System.out.println(&quot;付款失败，请重新支付！&quot;); return false; &#125; @Override public void dispenseCommodity() &#123; System.out.println(&quot;请先完成支付！&quot;); &#125;&#125; 商品售罄状态类123456789101112131415161718192021222324public class EmptyState implements State &#123; VendingMachine machine; public EmptyState(VendingMachine machine) &#123; this.machine = machine; &#125; @Override public void choose() &#123; System.out.println(&quot;对不起商品已售罄！&quot;); &#125; @Override public boolean payment() &#123; System.out.println(&quot;对不起商品已售罄！&quot;); return false; &#125; @Override public void dispenseCommodity() &#123; System.out.println(&quot;对不起商品已售罄！&quot;); &#125;&#125; 分发商品状态类12345678910111213141516171819202122232425public class DispenseCommodityState implements State &#123; VendingMachine machine; public DispenseCommodityState(VendingMachine machine) &#123; this.machine = machine; &#125; @Override public void choose() &#123; System.out.println(&quot;请及时取走您的商品！&quot;); &#125; @Override public boolean payment() &#123; System.out.println(&quot;请及时取走您的商品！&quot;); return false; &#125; @Override public void dispenseCommodity() &#123; System.out.println(&quot;请及时取走您的商品！&quot;); machine.setState(machine.getChooseGoods()); &#125;&#125; 自动售货机 &#x3D;&gt; Context角色123456789101112131415161718192021222324252627282930313233public class VendingMachine &#123; // 表示当前状态 private State state = null; // 商品数量 private int count = 0; private State chooseGoods = new ChooseGoods(this); private State paymentState = new PaymentState(this); private State dispenseCommodityState = new DispenseCommodityState(this); private State emptyState = new EmptyState(this); public VendingMachine(int count) &#123; this.count = count; this.state = this.getChooseGoods(); &#125; // 购买商品 public void purchase() &#123; // 挑选商品 state.choose(); // 支付成功 if (state.payment()) &#123; // 分发商品 state.dispenseCommodity(); &#125; &#125; // 获取商品后将商品减一 public int getCount() &#123; return count--; &#125; // get和set方法 ... &#125; 客户端测试类12345678910public class Client &#123; public static void main(String[] args) &#123; VendingMachine machine = new VendingMachine(1); for (int i = 1; i &lt; 4; i++) &#123; System.out.println(&quot;第&quot; + i + &quot;次购买。&quot;); machine.purchase(); &#125; &#125;&#125; 执行结果总结1、状态模式将每个状态所对应的行为封装到一个类中，大大提高了代码的可读性。并且通过这样的设计还可以消除多余的if-else语句，方便代码的维护。 2、状态模式符合“开闭原则”，容易增加和删除状态。 3、任何事情都有利弊，状态模式也不例外。其最显著的问题是，每个状态都要对应一个类，当状态过多时会产生大量的类，从而加大维护成本。 4、应用场景：当一个需求有很多状态，并且状态之间会进行转换，不同状态还对应不同的行为时就可以考虑使用“状态模式”。","tags":["设计模式"],"categories":["设计模式"]},{"title":"Google 开源的 Guava 工具库","path":"/2023/12/24/Google-开源的-Guava-工具库/","content":"目前Google Guava在实际应用中非常广泛，本篇博客将以博主对Guava使用的认识以及在项目中的经验来给大家分享！正如标题所言，学习使用Google Guava可以让你快乐编程，写出优雅的JAVA代码！ 以面向对象思想处理字符串:Joiner&#x2F;Splitter&#x2F;CharMatcher JDK提供的String还不够好么？ 也许还不够友好，至少让我们用起来还不够爽，还得操心！ 举个栗子，比如String提供的split方法，我们得关心空字符串吧，还得考虑返回的结果中存在null元素吧，只提供了前后trim的方法（如果我想对中间元素进行trim呢）。 那么，看下面的代码示例，guava让你不必在操心这些： 123456789101112131415// 连接器private static final Joiner joiner = Joiner.on(&quot;,&quot;).skipNulls();// 分割器private static final Splitter splitter = Splitter.on(&quot;,&quot;).trimResults().omitEmptyStrings();public static void main(String[] args) &#123; // 把集合/数组中的元素 join 在一起 String join = joiner.join(Lists.newArrayList(&quot;a&quot;, null, &quot;b&quot;)); System.out.println(&quot;join=&quot; + join); for(String tmp : splitter.split(&quot;a, ,b,,&quot;)) &#123; System.out.println(&quot;|&quot; + tmp + &quot;|&quot;); &#125;&#125; Joiner&#x2F;Splitter Joiner是连接器，Splitter是分割器，通常我们会把它们定义为static final，利用on生成对象后在应用到String进行处理，这是可以复用的。要知道apache commons StringUtils提供的都是static method。 更加重要的是，guava提供的Joiner&#x2F;Splitter是经过充分测试，它的稳定性和效率要比apache高出不少，这个你可以自行测试下~ 发现没有我们想对String做什么操作，就是生成自己定制化的Joiner&#x2F;Splitter，多么直白，简单，流畅的API！ 对于Joiner，常用的方法是 跳过NULL元素：skipNulls() &#x2F; 对于NULL元素使用其他替代：useForNull(String) 对于Splitter，常用的方法是：trimResults()&#x2F;omitEmptyStrings()。注意拆分的方式，有字符串，还有正则，还有固定长度分割（太贴心了！） 其实除了Joiner&#x2F;Splitter外，guava还提供了字符串匹配器：CharMatcher 123456789101112private static final CharMatcher charMatcherDigit = CharMatcher.DIGIT;private static final Charmatcher charMatcherAny = CharMatcher.ANY;public static void main(String[] args) &#123; // 只保留匹配的字符，其他移除 System.out.println(charMatcherDigit.retainFrom(&quot;abc2def134f~&quot;)); // 移除匹配的字符 System.out.println(charMatcherDigit.removeFrom(&quot;yes,i love you 1314&quot;)); System.out.println(charMatcherAny.inRange(&#x27;a&#x27;, &#x27;f&#x27;).or(charMatcherAny.is(&#x27;a&#x27;)).replaceFrom(&quot;abcdefg&quot;,&quot;*&quot;));&#125; CharMatcher CharMatcher，将字符的匹配和处理解耦，并提供丰富的方法供你使用！ 对基本类型进行支持 guava对JDK提供的原生类型操作进行了扩展，使得功能更加强大！ 1234567891011121314151617// 快速完成到集合的转换List&lt;Integer&gt; list = Ints.asList(1, 3, 5, 7, 9);System.out.println(Ints.join(&quot;,&quot;, 1, 3, 1, 4));// 原生类型数据快速合并int[] newIntArray = Ints.concat(new int[]&#123;1, 2&#125;, new int[]&#123;2, 3, 4&#125;);System.out.println(newIntArray.length);// 最大/最小System.out.println(Ints.max(newIntArray) + &quot;,&quot; + Ints.min(newIntArray));// 是否包含System.out.println(Ints.contains(newArray, 6));// 集合到数组的转换int[] someArray = Ints.toArray(list); Ints guava提供了 Bytes&#x2F;Shorts&#x2F;Ints&#x2F;Iongs&#x2F;Floats&#x2F;Doubles&#x2F;Chars&#x2F;Booleans 这些基本数据类型的扩展支持，只有你想不到的，没有它没有的！ 对JDK集合的有效补充灰色地带:Multiset JDK的集合，提供了有序且可以重复的List，无序且不可以重复的Set。那这里其实对于集合涉及到了2个概念，一个order，一个dups。那么List vs Set，and then some ? Multiset Multiset是什么，我想上面的图，你应该了解它的概念了。Multiset就是无序的，但是可以重复的集合，它就是游离在List&#x2F;Set之间的“灰色地带”！（至于有序的，不允许重复的集合嘛，guava还没有提供，当然在未来应该会提供UniqueList，我猜的，哈哈） 来看一个Multiset的示例： 12345678910Multiset&lt;String&gt; multiset = HashMultiset.create();multiset.add(&quot;a&quot;);multiset.add(&quot;a&quot;);multiset.add(&quot;b&quot;);multiset.add(&quot;c&quot;);multiset.add(&quot;b&quot;);System.out.println(multiset.size());System.out.println(multiset.count(&quot;a&quot;)); Multiset Code Multiset自带一个有用的功能，就是可以跟踪每个对象的数量。 Immutable vs unmodifiable来我们先看一个unmodifiable的例子： 1234567891011121314// List 的不可变设置List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;a&quot;);list.add(&quot;b&quot;);// 这种视图，不够安全，不是真正意义上的快照，怎么能随着而变化呢？List&lt;String&gt; readOnlyList = Collections.unmodifiableList(list);// readOnlyList.add(&quot;c&quot;);// 抛异常：java.lang.UnsupportedOperationExceptionlist.acc(&quot;c&quot;);System.out.println(reaOnlyList.size()); // 3 unmodifiable 你看到JDK提供的unmodifiable的缺陷了吗？ 实际上，Collections.unmodifiableXxx所返回的集合和源集合是同一个对象，只不过可以对集合做出改变的API都被override，会抛出UnsupportedOperationException。 也即是说我们改变源集合，导致不可变视图（unmodifiable View）也会发生变化，oh my god! 当然，在不使用guava的情况下，我们是怎么避免上面的问题的呢？ 1234567// List 的不可变性设置List&lt;String&gt; list = new ArrayList&lt;~&gt;();list.add(&quot;a&quot;);list.add(&quot;b&quot;);// new Object ; CopyList&lt;String&gt; readOnList = Collections.unmodifiableList(new ArrayList&lt;String&gt;(list)); defensive copies 上面揭示了一个概念：Defensive Copies，保护性拷贝。 OK，unmodifiable看上去没有问题呢，但是guava依然觉得可以改进，于是提出了Immutable的概念，来看： 12345678910// guava 是如何做的呢？List&lt;String&gt; immutable = ImmutabeList.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);// immutable.add(&quot;d&quot;);// 抛异常：java.lang.UnsupportedOperationExceptionList&lt;String&gt; immutable2 = ImmutableList.copyOf(list);list.add(&quot;d&quot;);// 视图不随着源而改变 guava 只读设置安全可靠 简单易用System.out.println(&quot;list size:&quot; + list.size() + &quot; immutable2.size:&quot; + immutables.size()); Immutable 就一个copyOf，你不会忘记，如此cheap~ 用Google官方的说法是：we’re using just one class,just say exactly what we mean，很了不起吗（不仅仅是个概念，Immutable在COPY阶段还考虑了线程的并发性等，很智能的！），O(∩_∩)O哈哈~ guava提供了很多Immutable集合，比如 ImmutableList&#x2F;ImmutableSet&#x2F;ImmutableSortedSet&#x2F;ImmutableMap&#x2F;…… 看一个ImmutableMap的例子： 123ImmutableMap&lt;String, String&gt; immutableMap = ImmutableMap.of(&quot;name&quot;, &quot;hubert&quot;, &quot;sex&quot;, &quot;man&quot;);immutableMap.put(&quot;wife&quot;, &quot;no...&quot;); // UnsupportedOperationException ImmutableMap 可不可以一对多：Multimap JDK提供给我们的Map是一个键，一个值，一对一的，那么在实际开发中，显然存在一个KEY多个VALUE的情况（比如一个分类下的书本），我们往往这样表达：Map&lt;k,List&lt;v&gt;&gt;，好像有点臃肿！臃肿也就算了，更加不爽的事，我们还得判断KEY是否存在来决定是否new 一个LIST出来，有点麻烦！更加麻烦的事情还在后头，比如遍历，比如删除，so hard…… 来看guava如何替你解决这个大麻烦的： 1234567Multimap&lt;String, String&gt; multiMap = ArrayListMultimap.create();multiMap.put(&quot;hubert&quot;, &quot;man&quot;);multiMap.put(&quot;hubert&quot;, &quot;yes&quot;);multiMap.put(&quot;lucy&quot;, &quot;woman&quot;);System.out.println(multiMap.get(&quot;hubert&quot;)); //collection Multimap 友情提示下，guava所有的集合都有create方法，这样的好处在于简单，而且我们不必在重复泛型信息了。 get()&#x2F;keys()&#x2F;keySet()&#x2F;values()&#x2F;entries()&#x2F;asMap()都是非常有用的返回view collection的方法。 Multimap的实现类有： ArrayListMultimap&#x2F;HashMultimap&#x2F;LinkedHashMultimap&#x2F;TreeMultimap&#x2F;ImmutableMultimap&#x2F;…… 可不可以双向：BiMap JDK提供的MAP让我们可以find value by key，那么能不能通过find key by value呢，能不能KEY和VALUE都是唯一的呢。这是一个双向的概念，即forward+backward。 在实际场景中有这样的需求吗？比如通过用户ID找到mail，也需要通过mail找回用户名。没有guava的时候，我们需要create forward map AND create backward map，and now just let guava do that for you. 12345678910111213BiMap&lt;String, String&gt; biMap = HashBiMap.create();biMap.put(&quot;name&quot;, &quot;hubert&quot;);// java.lang.IllegaArgumentException: value already present: hubert// value 重复会报错biMap.put(&quot;nick&quot;, &quot;hubert&quot;);// 强制覆盖 name:hubertbiMap.forcePut(&quot;nick&quot;, &quot;hubert&quot;);biMap.put(&quot;123&quot;, &quot;hubertwongcn@163.com&quot;);System.out.println(biMap.inverse().get(&quot;hubertwongcn@163.com&quot;)); // 123 BiMap biMap &#x2F; biMap.inverse() &#x2F; biMap.inverse().inverse() 它们是什么关系呢？ 你可以稍微看一下BiMap的源码实现，实际上，当你创建BiMap的时候，在内部维护了2个map，一个forward map，一个backward map，并且设置了它们之间的关系。 因此，biMap.inverse() !&#x3D; biMap ；biMap.inverse().inverse() &#x3D;&#x3D; biMap 可不可以多个KEY：Table 我们知道数据库除了主键外，还提供了复合索引，而且实际中这样的多级关系查找也是比较多的，当然我们可以利用嵌套的Map来实现：Map&lt;k1,Map&lt;k2,v2&gt;&gt;。为了让我们的代码看起来不那么丑陋，guava为我们提供了Table。 1234567Table&lt;String, String, Integer&gt; table = HashBaseTable.create();table.put(&quot;张三&quot;, &quot;计算机&quot;, 80);table.put(&quot;张三&quot;, &quot;数学&quot;, 90);table.put(&quot;张三&quot;, &quot;语文&quot;, 70);table.put(&quot;李四&quot;, &quot;计算机&quot;, 70);table.put(&quot;李四&quot;, &quot;数学&quot;, 60);table.put(&quot;李四&quot;, &quot;语文&quot;, 100); Table Table涉及到3个概念：rowKey,columnKey,value，并提供了多种视图以及操作方法让你更加轻松的处理多个KEY的场景。 函数式编程：Functions12345678910111213141516171819202122List&lt;String&gt; list = Lists.newArrayList(&quot;hello world&quot;, &quot;yes&quot;, &quot;hubert&quot;);Function&lt;String, String&gt; f1 = new Function&lt;String, String&gt;() &#123; @Override public String apply(String s) &#123; return s.length() &lt;= 5 ? s : s.substring(0, 5); &#125;&#125;;Function&lt;String, String&gt; f2 = new Function&lt;String, String&gt;() &#123; @Override public String apply(String s) &#123; return s.toUpperCase(); &#125;&#125;;Function&lt;String, String&gt; f3 = Functions.compose(f1, f2);Collection&lt;String&gt; collection = Collections2.transform(list, f3);for(String s : collection) &#123; System.out.println(s);&#125; Functions 上面的代码是为了完成将List集合中的元素，先截取5个长度，然后转成大写。 函数式编程的好处在于在集合遍历操作中提供自定义Function的操作，比如transform转换。我们再也不需要一遍遍的遍历集合，显著的简化了代码！ 12345678910Iterables.transform(Iterable, Function);Iterators.transform(Iterator, Function);Collections2.transform(Collection, Function);Lists.transform(List, Function);Maps.transformValues(Map, Function);Multimaps.transformValues(Multimap, Function);Multimaps.transformValues(ListMultimap, Funtion);Tables.transformValues(Table, Function);Maps.transformEntries(Map, EntryTransformer);// ... 对集合的transform操作可以通过Function完成 断言：Predicate12345678910111213List&lt;String&gt; list = Lists.newArrayList(&quot;moom&quot;, &quot;dad&quot;, &quot;refer&quot;, &quot;yes&quot;);Collection&lt;String&gt; collection = Collections2.filter(list, new Predicate&lt;String&gt;)) &#123; @Override public boolean apply(String s) &#123; // 业务逻辑 return new StringBuilder(s).reverse().toString().equals(s); &#125;&#125;;for(String s : collection) &#123; System.out.println(s);&#125; Predicate最常用的功能就是运用在集合的过滤当中！ 12345678Iterables.filter(Iterable, Predicate);Iterators.filter(Iterator, Predicate);Collectios2.filter(Collection, Predicate);Sets.filter(Set, Predicate);Sets.filter(SortedSet, Predicate);Maps.filterKeys(Map, Predicate);Multimaps.filterKeys(Multimap, Predicate);// ... filter 需要注意的是Lists并没有提供filter方法，不过你可以使用Collections2.filter完成！ check null and other：Optional、Preconditions在guava中，对于null的处理手段是快速失败，你可以看看guava的源码，很多方法的第一行就是：Preconditions.checkNotNull(elements); 要知道null是模糊的概念，是成功呢，还是失败呢，还是别的什么含义呢？ 12345678910111213public static void test(String name, int age, Map&lt;String, String&gt; extInfo) &#123; Preconditions.checkNotNull(name, &quot;name must be given!&quot;); Preconditions.checkArgument(age &gt;= 18, &quot;the game you can not play it, your age is under 18!&quot;); Map&lt;String, String&gt; defaulExtInfo = Maps.newHashMap(); defaultExtInfo.put(&quot;sex&quot;, &quot;man&quot;); extInfo = Optional.fromNullable(extInfo).or(defaultExtInfo); for(Map.Entry&lt;String, Stirng&gt; entry : extInfo.entrySet())) &#123; System.out.println(entry.getKey() + &quot;:&quot; + entry.getValue()); &#125;&#125; Preconditions&#x2F;Optional Cache is king 对于大多数互联网项目而言，缓存的重要性，不言而喻！ 如果我们的应用系统，并不想使用一些第三方缓存组件（如redis），我们仅仅想在本地有一个功能足够强大的缓存，很可惜JDK提供的那些SET&#x2F;MAP还不行！ 12345678910111213141516171819202122232425// 定义缓存的实现private static final CacheLoader&lt;Long, User&gt; userCacheLoader = new CacheLoader&lt;Long, User&gt;() &#123; @Override public User load(Long along) throws Exception &#123; // 模拟从数据库/Redis/缓存中加载数据 User user = new User(); user.setId(along); user.setName(Thread.currentThread().getName() + &quot;-&quot; new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;).format(new Date()) + &quot;-&quot; + along); System.out.println(&quot;load:&quot; + user); return user; &#125;&#125;;// 定义缓存的策略，提供对外访问缓存private static final LoadingCache&lt;Long, User&gt; userCacheData = CacheBuilder.newBuilder() .expireAfterAccess(2, TimeUnit.SECONDS) .expireAfterWrite(2, TimeUnit.SECONDS) .refreshAfterWrite(3, TimeUnit.SECONS) .maximumSize(10000L) .bulid(userCacheLoader); CacheLoader 首先，这是一个本地缓存，guava提供的cache是一个简洁、高效，易于维护的。为什么这么说呢？因为并没有一个单独的线程用于刷新 OR 清理cache，对于cache的操作，都是通过访问&#x2F;读写带来的，也就是说在读写中完成缓存的刷新操作！ 其次，我们看到了，我们非常通俗的告诉cache，我们的缓存策略是什么，SO EASY！在如此简单的背后，是guava帮助我们做了很多事情，比如线程安全。 让异步回调更加简单 JDK中提供了Future&#x2F;FutureTask&#x2F;Callable来对异步回调进行支持，但是还是看上去挺复杂的，能不能更加简单呢？比如注册一个监听回调。 12345678910111213141516171819202122232425262728// JDK 所提供的线程池ExecutorService es = Executors.newFixedThreadPool(3);// 经过guava封装的带有监听回调功能的线程池ListeningExecutorService listeningExecutorService = MoreExecutors.listeningDecorator(es);ListenableFuture listenableFuture = listeningExecutorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; if (new Random().nextInt(3) == 2) &#123; throw new NullPointerException(); &#125; return 1; &#125;&#125;);FutureCallback futureCallback = new FutureCallback&lt;Integer&gt; &#123; @Override public void onSuccess(final Integer o) &#123; System.out.println(&quot;------&quot; + o); &#125; @Override public void onFailure(final Throwable throwable) &#123; System.out.println(&quot;======&quot; + throwable.getMessage()); &#125;&#125;;Futures.addCallback(listenableFuture, futureCallback); 异步回调 我们可以通过guava对JDK提供的线程池进行装饰，让其具有异步回调监听功能，然后在设置监听器即可！ Summary到这里，这篇文章也只介绍了guava的冰山一角，其实还有很多内容： guava package 比如反射、注解、网络、并发、IO等等","tags":["工具","开源","Google"],"categories":["工具"]}]